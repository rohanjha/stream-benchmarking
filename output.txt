2018-04-26 01:59:34 WARN  Utils:66 - Your hostname, rjha-VirtualBox resolves to a loopback address: 127.0.1.1; using 10.0.2.15 instead (on interface enp0s3)
2018-04-26 01:59:34 WARN  Utils:66 - Set SPARK_LOCAL_IP if you need to bind to another address
2018-04-26 01:59:35 WARN  NativeCodeLoader:62 - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2018-04-26 01:59:36 INFO  SparkContext:54 - Running Spark version 2.3.0
2018-04-26 01:59:36 INFO  SparkContext:54 - Submitted application: PythonStreamingNetworkWordCount
2018-04-26 01:59:36 INFO  SecurityManager:54 - Changing view acls to: rjha
2018-04-26 01:59:36 INFO  SecurityManager:54 - Changing modify acls to: rjha
2018-04-26 01:59:36 INFO  SecurityManager:54 - Changing view acls groups to: 
2018-04-26 01:59:36 INFO  SecurityManager:54 - Changing modify acls groups to: 
2018-04-26 01:59:36 INFO  SecurityManager:54 - SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(rjha); groups with view permissions: Set(); users  with modify permissions: Set(rjha); groups with modify permissions: Set()
2018-04-26 01:59:37 INFO  Utils:54 - Successfully started service 'sparkDriver' on port 36787.
2018-04-26 01:59:37 INFO  SparkEnv:54 - Registering MapOutputTracker
2018-04-26 01:59:37 INFO  SparkEnv:54 - Registering BlockManagerMaster
2018-04-26 01:59:37 INFO  BlockManagerMasterEndpoint:54 - Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
2018-04-26 01:59:37 INFO  BlockManagerMasterEndpoint:54 - BlockManagerMasterEndpoint up
2018-04-26 01:59:37 INFO  DiskBlockManager:54 - Created local directory at /tmp/blockmgr-ab2a6f34-ff52-4914-917a-cc0f28f6080c
2018-04-26 01:59:37 INFO  MemoryStore:54 - MemoryStore started with capacity 413.9 MB
2018-04-26 01:59:37 INFO  SparkEnv:54 - Registering OutputCommitCoordinator
2018-04-26 01:59:37 INFO  log:192 - Logging initialized @5107ms
2018-04-26 01:59:37 INFO  Server:346 - jetty-9.3.z-SNAPSHOT
2018-04-26 01:59:37 INFO  Server:414 - Started @5213ms
2018-04-26 01:59:37 INFO  AbstractConnector:278 - Started ServerConnector@65c4d5c6{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2018-04-26 01:59:37 INFO  Utils:54 - Successfully started service 'SparkUI' on port 4040.
2018-04-26 01:59:37 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@598de314{/jobs,null,AVAILABLE,@Spark}
2018-04-26 01:59:37 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@71328d75{/jobs/json,null,AVAILABLE,@Spark}
2018-04-26 01:59:37 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@7bcc43c2{/jobs/job,null,AVAILABLE,@Spark}
2018-04-26 01:59:37 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@3b1c5499{/jobs/job/json,null,AVAILABLE,@Spark}
2018-04-26 01:59:37 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@5ea03faa{/stages,null,AVAILABLE,@Spark}
2018-04-26 01:59:37 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@476bb5ef{/stages/json,null,AVAILABLE,@Spark}
2018-04-26 01:59:37 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@262317fd{/stages/stage,null,AVAILABLE,@Spark}
2018-04-26 01:59:37 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@3a1e173b{/stages/stage/json,null,AVAILABLE,@Spark}
2018-04-26 01:59:37 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@43c637a{/stages/pool,null,AVAILABLE,@Spark}
2018-04-26 01:59:37 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@6a295f06{/stages/pool/json,null,AVAILABLE,@Spark}
2018-04-26 01:59:37 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@4a19d040{/storage,null,AVAILABLE,@Spark}
2018-04-26 01:59:37 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@f4b1f7{/storage/json,null,AVAILABLE,@Spark}
2018-04-26 01:59:37 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@67342520{/storage/rdd,null,AVAILABLE,@Spark}
2018-04-26 01:59:37 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@474587af{/storage/rdd/json,null,AVAILABLE,@Spark}
2018-04-26 01:59:37 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@435ac4d7{/environment,null,AVAILABLE,@Spark}
2018-04-26 01:59:37 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@6684ae42{/environment/json,null,AVAILABLE,@Spark}
2018-04-26 01:59:37 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@20df9a97{/executors,null,AVAILABLE,@Spark}
2018-04-26 01:59:37 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@4bcc19dc{/executors/json,null,AVAILABLE,@Spark}
2018-04-26 01:59:37 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@5ea5100e{/executors/threadDump,null,AVAILABLE,@Spark}
2018-04-26 01:59:37 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@1d105326{/executors/threadDump/json,null,AVAILABLE,@Spark}
2018-04-26 01:59:37 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@41c77931{/static,null,AVAILABLE,@Spark}
2018-04-26 01:59:37 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@7b13b7db{/,null,AVAILABLE,@Spark}
2018-04-26 01:59:37 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@668fe269{/api,null,AVAILABLE,@Spark}
2018-04-26 01:59:37 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@282dc8c4{/jobs/job/kill,null,AVAILABLE,@Spark}
2018-04-26 01:59:37 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@4a7f2fc2{/stages/stage/kill,null,AVAILABLE,@Spark}
2018-04-26 01:59:37 INFO  SparkUI:54 - Bound SparkUI to 0.0.0.0, and started at http://10.0.2.15:4040
2018-04-26 01:59:38 INFO  SparkContext:54 - Added file file:/home/rjha/stream-benchmarking-scratch/streaming_test.py at file:/home/rjha/stream-benchmarking-scratch/streaming_test.py with timestamp 1524722378476
2018-04-26 01:59:38 INFO  Utils:54 - Copying /home/rjha/stream-benchmarking-scratch/streaming_test.py to /tmp/spark-d125ef3c-c1e8-4c0b-99d6-f5ed7731f62b/userFiles-e15f5724-119d-4a69-8b03-6e7787714e87/streaming_test.py
2018-04-26 01:59:38 INFO  Executor:54 - Starting executor ID driver on host localhost
2018-04-26 01:59:38 INFO  Utils:54 - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 33681.
2018-04-26 01:59:38 INFO  NettyBlockTransferService:54 - Server created on 10.0.2.15:33681
2018-04-26 01:59:38 INFO  BlockManager:54 - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
2018-04-26 01:59:38 INFO  BlockManagerMaster:54 - Registering BlockManager BlockManagerId(driver, 10.0.2.15, 33681, None)
2018-04-26 01:59:38 INFO  BlockManagerMasterEndpoint:54 - Registering block manager 10.0.2.15:33681 with 413.9 MB RAM, BlockManagerId(driver, 10.0.2.15, 33681, None)
2018-04-26 01:59:38 INFO  BlockManagerMaster:54 - Registered BlockManager BlockManagerId(driver, 10.0.2.15, 33681, None)
2018-04-26 01:59:38 INFO  BlockManager:54 - Initialized BlockManager: BlockManagerId(driver, 10.0.2.15, 33681, None)
2018-04-26 01:59:39 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@2776447a{/metrics/json,null,AVAILABLE,@Spark}
2018-04-26 01:59:40 INFO  ReceiverTracker:54 - Starting 1 receivers
2018-04-26 01:59:40 INFO  ReceiverTracker:54 - ReceiverTracker started
2018-04-26 01:59:40 INFO  SocketInputDStream:54 - Slide time = 1000 ms
2018-04-26 01:59:40 INFO  SocketInputDStream:54 - Storage level = Serialized 1x Replicated
2018-04-26 01:59:40 INFO  SocketInputDStream:54 - Checkpoint interval = null
2018-04-26 01:59:40 INFO  SocketInputDStream:54 - Remember interval = 1000 ms
2018-04-26 01:59:40 INFO  SocketInputDStream:54 - Initialized and validated org.apache.spark.streaming.dstream.SocketInputDStream@7c8073f5
2018-04-26 01:59:40 INFO  PythonTransformedDStream:54 - Slide time = 1000 ms
2018-04-26 01:59:40 INFO  PythonTransformedDStream:54 - Storage level = Serialized 1x Replicated
2018-04-26 01:59:40 INFO  PythonTransformedDStream:54 - Checkpoint interval = null
2018-04-26 01:59:40 INFO  PythonTransformedDStream:54 - Remember interval = 1000 ms
2018-04-26 01:59:40 INFO  PythonTransformedDStream:54 - Initialized and validated org.apache.spark.streaming.api.python.PythonTransformedDStream@838804b
2018-04-26 01:59:40 INFO  ForEachDStream:54 - Slide time = 1000 ms
2018-04-26 01:59:40 INFO  ForEachDStream:54 - Storage level = Serialized 1x Replicated
2018-04-26 01:59:40 INFO  ForEachDStream:54 - Checkpoint interval = null
2018-04-26 01:59:40 INFO  ForEachDStream:54 - Remember interval = 1000 ms
2018-04-26 01:59:40 INFO  ForEachDStream:54 - Initialized and validated org.apache.spark.streaming.dstream.ForEachDStream@57546450
2018-04-26 01:59:40 INFO  SocketInputDStream:54 - Slide time = 1000 ms
2018-04-26 01:59:40 INFO  SocketInputDStream:54 - Storage level = Serialized 1x Replicated
2018-04-26 01:59:40 INFO  SocketInputDStream:54 - Checkpoint interval = null
2018-04-26 01:59:40 INFO  SocketInputDStream:54 - Remember interval = 1000 ms
2018-04-26 01:59:40 INFO  SocketInputDStream:54 - Initialized and validated org.apache.spark.streaming.dstream.SocketInputDStream@7c8073f5
2018-04-26 01:59:40 INFO  PythonTransformedDStream:54 - Slide time = 1000 ms
2018-04-26 01:59:40 INFO  PythonTransformedDStream:54 - Storage level = Serialized 1x Replicated
2018-04-26 01:59:40 INFO  PythonTransformedDStream:54 - Checkpoint interval = null
2018-04-26 01:59:40 INFO  PythonTransformedDStream:54 - Remember interval = 1000 ms
2018-04-26 01:59:40 INFO  PythonTransformedDStream:54 - Initialized and validated org.apache.spark.streaming.api.python.PythonTransformedDStream@838804b
2018-04-26 01:59:40 INFO  ForEachDStream:54 - Slide time = 1000 ms
2018-04-26 01:59:40 INFO  ForEachDStream:54 - Storage level = Serialized 1x Replicated
2018-04-26 01:59:40 INFO  ForEachDStream:54 - Checkpoint interval = null
2018-04-26 01:59:40 INFO  ForEachDStream:54 - Remember interval = 1000 ms
2018-04-26 01:59:40 INFO  ForEachDStream:54 - Initialized and validated org.apache.spark.streaming.dstream.ForEachDStream@4d3a2eca
2018-04-26 01:59:40 INFO  ReceiverTracker:54 - Receiver 0 started
2018-04-26 01:59:40 INFO  RecurringTimer:54 - Started timer for JobGenerator at time 1524722381000
2018-04-26 01:59:40 INFO  JobGenerator:54 - Started JobGenerator at 1524722381000 ms
2018-04-26 01:59:40 INFO  JobScheduler:54 - Started JobScheduler
2018-04-26 01:59:40 INFO  DAGScheduler:54 - Got job 0 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions
2018-04-26 01:59:40 INFO  DAGScheduler:54 - Final stage: ResultStage 0 (start at NativeMethodAccessorImpl.java:0)
2018-04-26 01:59:40 INFO  DAGScheduler:54 - Parents of final stage: List()
2018-04-26 01:59:40 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@59831beb{/streaming,null,AVAILABLE,@Spark}
2018-04-26 01:59:40 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@21ae98c{/streaming/json,null,AVAILABLE,@Spark}
2018-04-26 01:59:40 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@72bcd4e4{/streaming/batch,null,AVAILABLE,@Spark}
2018-04-26 01:59:40 INFO  DAGScheduler:54 - Missing parents: List()
2018-04-26 01:59:40 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@237f49c4{/streaming/batch/json,null,AVAILABLE,@Spark}
2018-04-26 01:59:40 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@1edf4cc7{/static/streaming,null,AVAILABLE,@Spark}
2018-04-26 01:59:40 INFO  StreamingContext:54 - StreamingContext started
2018-04-26 01:59:40 INFO  DAGScheduler:54 - Submitting ResultStage 0 (Receiver 0 ParallelCollectionRDD[0] at makeRDD at ReceiverTracker.scala:613), which has no missing parents
2018-04-26 01:59:40 INFO  MemoryStore:54 - Block broadcast_0 stored as values in memory (estimated size 68.7 KB, free 413.9 MB)
2018-04-26 01:59:40 INFO  MemoryStore:54 - Block broadcast_0_piece0 stored as bytes in memory (estimated size 23.9 KB, free 413.8 MB)
2018-04-26 01:59:40 INFO  BlockManagerInfo:54 - Added broadcast_0_piece0 in memory on 10.0.2.15:33681 (size: 23.9 KB, free: 413.9 MB)
2018-04-26 01:59:40 INFO  SparkContext:54 - Created broadcast 0 from broadcast at DAGScheduler.scala:1039
2018-04-26 01:59:40 INFO  DAGScheduler:54 - Submitting 1 missing tasks from ResultStage 0 (Receiver 0 ParallelCollectionRDD[0] at makeRDD at ReceiverTracker.scala:613) (first 15 tasks are for partitions Vector(0))
2018-04-26 01:59:40 INFO  TaskSchedulerImpl:54 - Adding task set 0.0 with 1 tasks
2018-04-26 01:59:40 INFO  TaskSetManager:54 - Starting task 0.0 in stage 0.0 (TID 0, localhost, executor driver, partition 0, PROCESS_LOCAL, 8442 bytes)
2018-04-26 01:59:41 INFO  Executor:54 - Running task 0.0 in stage 0.0 (TID 0)
2018-04-26 01:59:41 INFO  Executor:54 - Fetching file:/home/rjha/stream-benchmarking-scratch/streaming_test.py with timestamp 1524722378476
2018-04-26 01:59:41 INFO  JobScheduler:54 - Added jobs for time 1524722381000 ms
2018-04-26 01:59:41 INFO  JobScheduler:54 - Starting job streaming job 1524722381000 ms.0 from job set of time 1524722381000 ms
2018-04-26 01:59:41 INFO  Utils:54 - /home/rjha/stream-benchmarking-scratch/streaming_test.py has been previously copied to /tmp/spark-d125ef3c-c1e8-4c0b-99d6-f5ed7731f62b/userFiles-e15f5724-119d-4a69-8b03-6e7787714e87/streaming_test.py
2018-04-26 01:59:41 INFO  SparkContext:54 - Starting job: call at /home/rjha/spark-2.3.0-bin-hadoop2.7/python/lib/py4j-0.10.6-src.zip/py4j/java_gateway.py:2257
2018-04-26 01:59:41 INFO  DAGScheduler:54 - Job 1 finished: call at /home/rjha/spark-2.3.0-bin-hadoop2.7/python/lib/py4j-0.10.6-src.zip/py4j/java_gateway.py:2257, took 0.000572 s
2018-04-26 01:59:41 INFO  JobScheduler:54 - Finished job streaming job 1524722381000 ms.0 from job set of time 1524722381000 ms
2018-04-26 01:59:41 INFO  JobScheduler:54 - Starting job streaming job 1524722381000 ms.1 from job set of time 1524722381000 ms
2018-04-26 01:59:41 INFO  SparkContext:54 - Starting job: call at /home/rjha/spark-2.3.0-bin-hadoop2.7/python/lib/py4j-0.10.6-src.zip/py4j/java_gateway.py:2257
2018-04-26 01:59:41 INFO  DAGScheduler:54 - Job 2 finished: call at /home/rjha/spark-2.3.0-bin-hadoop2.7/python/lib/py4j-0.10.6-src.zip/py4j/java_gateway.py:2257, took 0.000105 s
2018-04-26 01:59:41 INFO  JobScheduler:54 - Finished job streaming job 1524722381000 ms.1 from job set of time 1524722381000 ms
2018-04-26 01:59:41 INFO  JobScheduler:54 - Total delay: 0.446 s for time 1524722381000 ms (execution: 0.263 s)
2018-04-26 01:59:41 INFO  ReceivedBlockTracker:54 - Deleting batches: 
2018-04-26 01:59:41 INFO  InputInfoTracker:54 - remove old batch metadata: 
2018-04-26 01:59:41 INFO  RecurringTimer:54 - Started timer for BlockGenerator at time 1524722381600
2018-04-26 01:59:41 INFO  BlockGenerator:54 - Started BlockGenerator
2018-04-26 01:59:41 INFO  BlockGenerator:54 - Started block pushing thread
2018-04-26 01:59:41 INFO  ReceiverTracker:54 - Registered receiver for stream 0 from 10.0.2.15:36787
2018-04-26 01:59:41 INFO  ReceiverSupervisorImpl:54 - Starting receiver 0
2018-04-26 01:59:41 INFO  SocketReceiver:54 - Connecting to localhost:9998
2018-04-26 01:59:41 INFO  SocketReceiver:54 - Connected to localhost:9998
2018-04-26 01:59:41 INFO  ReceiverSupervisorImpl:54 - Called receiver 0 onStart
2018-04-26 01:59:41 INFO  ReceiverSupervisorImpl:54 - Waiting for receiver to be stopped
2018-04-26 01:59:41 INFO  MemoryStore:54 - Block input-0-1524722381600 stored as bytes in memory (estimated size 198.0 B, free 413.8 MB)
2018-04-26 01:59:41 INFO  BlockManagerInfo:54 - Added input-0-1524722381600 in memory on 10.0.2.15:33681 (size: 198.0 B, free: 413.9 MB)
2018-04-26 01:59:41 WARN  RandomBlockReplicationPolicy:66 - Expecting 1 replicas with only 0 peer/s.
2018-04-26 01:59:41 WARN  BlockManager:66 - Block input-0-1524722381600 replicated to only 0 peer(s) instead of 1 peers
2018-04-26 01:59:41 INFO  BlockGenerator:54 - Pushed block input-0-1524722381600
2018-04-26 01:59:42 INFO  MemoryStore:54 - Block input-0-1524722381800 stored as bytes in memory (estimated size 371.0 B, free 413.8 MB)
2018-04-26 01:59:42 INFO  BlockManagerInfo:54 - Added input-0-1524722381800 in memory on 10.0.2.15:33681 (size: 371.0 B, free: 413.9 MB)
2018-04-26 01:59:42 WARN  RandomBlockReplicationPolicy:66 - Expecting 1 replicas with only 0 peer/s.
2018-04-26 01:59:42 WARN  BlockManager:66 - Block input-0-1524722381800 replicated to only 0 peer(s) instead of 1 peers
2018-04-26 01:59:42 INFO  BlockGenerator:54 - Pushed block input-0-1524722381800
2018-04-26 01:59:42 INFO  JobScheduler:54 - Starting job streaming job 1524722382000 ms.0 from job set of time 1524722382000 ms
2018-04-26 01:59:42 INFO  JobScheduler:54 - Added jobs for time 1524722382000 ms
2018-04-26 01:59:42 INFO  SparkContext:54 - Starting job: call at /home/rjha/spark-2.3.0-bin-hadoop2.7/python/lib/py4j-0.10.6-src.zip/py4j/java_gateway.py:2257
2018-04-26 01:59:42 INFO  DAGScheduler:54 - Got job 3 (call at /home/rjha/spark-2.3.0-bin-hadoop2.7/python/lib/py4j-0.10.6-src.zip/py4j/java_gateway.py:2257) with 1 output partitions
2018-04-26 01:59:42 INFO  DAGScheduler:54 - Final stage: ResultStage 1 (call at /home/rjha/spark-2.3.0-bin-hadoop2.7/python/lib/py4j-0.10.6-src.zip/py4j/java_gateway.py:2257)
2018-04-26 01:59:42 INFO  DAGScheduler:54 - Parents of final stage: List()
2018-04-26 01:59:42 INFO  DAGScheduler:54 - Missing parents: List()
2018-04-26 01:59:42 INFO  DAGScheduler:54 - Submitting ResultStage 1 (PythonRDD[7] at call at /home/rjha/spark-2.3.0-bin-hadoop2.7/python/lib/py4j-0.10.6-src.zip/py4j/java_gateway.py:2257), which has no missing parents
2018-04-26 01:59:42 INFO  MemoryStore:54 - Block broadcast_1 stored as values in memory (estimated size 7.5 KB, free 413.8 MB)
2018-04-26 01:59:42 INFO  MemoryStore:54 - Block broadcast_1_piece0 stored as bytes in memory (estimated size 4.4 KB, free 413.8 MB)
2018-04-26 01:59:42 INFO  BlockManagerInfo:54 - Added broadcast_1_piece0 in memory on 10.0.2.15:33681 (size: 4.4 KB, free: 413.9 MB)
2018-04-26 01:59:42 INFO  SparkContext:54 - Created broadcast 1 from broadcast at DAGScheduler.scala:1039
2018-04-26 01:59:42 INFO  DAGScheduler:54 - Submitting 1 missing tasks from ResultStage 1 (PythonRDD[7] at call at /home/rjha/spark-2.3.0-bin-hadoop2.7/python/lib/py4j-0.10.6-src.zip/py4j/java_gateway.py:2257) (first 15 tasks are for partitions Vector(0))
2018-04-26 01:59:42 INFO  TaskSchedulerImpl:54 - Adding task set 1.0 with 1 tasks
2018-04-26 01:59:42 INFO  TaskSetManager:54 - Starting task 0.0 in stage 1.0 (TID 1, localhost, executor driver, partition 0, ANY, 7772 bytes)
2018-04-26 01:59:42 INFO  Executor:54 - Running task 0.0 in stage 1.0 (TID 1)
2018-04-26 01:59:42 INFO  MemoryStore:54 - Block input-0-1524722382000 stored as bytes in memory (estimated size 389.0 B, free 413.8 MB)
2018-04-26 01:59:42 INFO  BlockManagerInfo:54 - Added input-0-1524722382000 in memory on 10.0.2.15:33681 (size: 389.0 B, free: 413.9 MB)
2018-04-26 01:59:42 WARN  RandomBlockReplicationPolicy:66 - Expecting 1 replicas with only 0 peer/s.
2018-04-26 01:59:42 WARN  BlockManager:66 - Block input-0-1524722382000 replicated to only 0 peer(s) instead of 1 peers
2018-04-26 01:59:42 INFO  BlockGenerator:54 - Pushed block input-0-1524722382000
2018-04-26 01:59:42 INFO  BlockManager:54 - Found block input-0-1524722381600 locally
2018-04-26 01:59:42 INFO  MemoryStore:54 - Block input-0-1524722382200 stored as bytes in memory (estimated size 386.0 B, free 413.8 MB)
2018-04-26 01:59:42 INFO  BlockManagerInfo:54 - Added input-0-1524722382200 in memory on 10.0.2.15:33681 (size: 386.0 B, free: 413.9 MB)
2018-04-26 01:59:42 WARN  RandomBlockReplicationPolicy:66 - Expecting 1 replicas with only 0 peer/s.
2018-04-26 01:59:42 WARN  BlockManager:66 - Block input-0-1524722382200 replicated to only 0 peer(s) instead of 1 peers
2018-04-26 01:59:42 INFO  BlockGenerator:54 - Pushed block input-0-1524722382200
2018-04-26 01:59:42 INFO  MemoryStore:54 - Block input-0-1524722382400 stored as bytes in memory (estimated size 387.0 B, free 413.8 MB)
2018-04-26 01:59:42 INFO  BlockManagerInfo:54 - Added input-0-1524722382400 in memory on 10.0.2.15:33681 (size: 387.0 B, free: 413.9 MB)
2018-04-26 01:59:42 WARN  RandomBlockReplicationPolicy:66 - Expecting 1 replicas with only 0 peer/s.
2018-04-26 01:59:42 WARN  BlockManager:66 - Block input-0-1524722382400 replicated to only 0 peer(s) instead of 1 peers
2018-04-26 01:59:42 INFO  BlockGenerator:54 - Pushed block input-0-1524722382400
2018-04-26 01:59:42 INFO  PythonRunner:54 - Times: total = 531, boot = 476, init = 55, finish = 0
2018-04-26 01:59:42 INFO  MemoryStore:54 - Block input-0-1524722382600 stored as bytes in memory (estimated size 369.0 B, free 413.8 MB)
2018-04-26 01:59:42 INFO  BlockManagerInfo:54 - Added input-0-1524722382600 in memory on 10.0.2.15:33681 (size: 369.0 B, free: 413.9 MB)
2018-04-26 01:59:42 WARN  RandomBlockReplicationPolicy:66 - Expecting 1 replicas with only 0 peer/s.
2018-04-26 01:59:42 WARN  BlockManager:66 - Block input-0-1524722382600 replicated to only 0 peer(s) instead of 1 peers
2018-04-26 01:59:42 INFO  BlockGenerator:54 - Pushed block input-0-1524722382600
2018-04-26 01:59:42 INFO  PythonRunner:54 - Times: total = 109, boot = 11, init = 36, finish = 62
2018-04-26 01:59:43 INFO  MemoryStore:54 - Block input-0-1524722382800 stored as bytes in memory (estimated size 406.0 B, free 413.8 MB)
2018-04-26 01:59:43 INFO  Executor:54 - Finished task 0.0 in stage 1.0 (TID 1). 1267 bytes result sent to driver
2018-04-26 01:59:43 INFO  TaskSetManager:54 - Finished task 0.0 in stage 1.0 (TID 1) in 927 ms on localhost (executor driver) (1/1)
2018-04-26 01:59:43 INFO  TaskSchedulerImpl:54 - Removed TaskSet 1.0, whose tasks have all completed, from pool 
2018-04-26 01:59:43 INFO  BlockManagerInfo:54 - Added input-0-1524722382800 in memory on 10.0.2.15:33681 (size: 406.0 B, free: 413.9 MB)
2018-04-26 01:59:43 WARN  RandomBlockReplicationPolicy:66 - Expecting 1 replicas with only 0 peer/s.
2018-04-26 01:59:43 WARN  BlockManager:66 - Block input-0-1524722382800 replicated to only 0 peer(s) instead of 1 peers
2018-04-26 01:59:43 INFO  BlockGenerator:54 - Pushed block input-0-1524722382800
2018-04-26 01:59:43 INFO  MemoryStore:54 - Block input-0-1524722383000 stored as bytes in memory (estimated size 302.0 B, free 413.8 MB)
2018-04-26 01:59:43 INFO  DAGScheduler:54 - ResultStage 1 (call at /home/rjha/spark-2.3.0-bin-hadoop2.7/python/lib/py4j-0.10.6-src.zip/py4j/java_gateway.py:2257) finished in 1.052 s
2018-04-26 01:59:43 INFO  BlockManagerInfo:54 - Added input-0-1524722383000 in memory on 10.0.2.15:33681 (size: 302.0 B, free: 413.9 MB)
2018-04-26 01:59:43 WARN  RandomBlockReplicationPolicy:66 - Expecting 1 replicas with only 0 peer/s.
2018-04-26 01:59:43 WARN  BlockManager:66 - Block input-0-1524722383000 replicated to only 0 peer(s) instead of 1 peers
2018-04-26 01:59:43 INFO  BlockGenerator:54 - Pushed block input-0-1524722383000
2018-04-26 01:59:43 INFO  DAGScheduler:54 - Job 3 finished: call at /home/rjha/spark-2.3.0-bin-hadoop2.7/python/lib/py4j-0.10.6-src.zip/py4j/java_gateway.py:2257, took 1.145068 s
2018-04-26 01:59:43 INFO  JobScheduler:54 - Finished job streaming job 1524722382000 ms.0 from job set of time 1524722382000 ms
2018-04-26 01:59:43 INFO  JobScheduler:54 - Starting job streaming job 1524722382000 ms.1 from job set of time 1524722382000 ms
2018-04-26 01:59:43 INFO  JobScheduler:54 - Added jobs for time 1524722383000 ms
2018-04-26 01:59:43 INFO  SparkContext:54 - Starting job: call at /home/rjha/spark-2.3.0-bin-hadoop2.7/python/lib/py4j-0.10.6-src.zip/py4j/java_gateway.py:2257
2018-04-26 01:59:43 INFO  DAGScheduler:54 - Got job 4 (call at /home/rjha/spark-2.3.0-bin-hadoop2.7/python/lib/py4j-0.10.6-src.zip/py4j/java_gateway.py:2257) with 1 output partitions
2018-04-26 01:59:43 INFO  DAGScheduler:54 - Final stage: ResultStage 2 (call at /home/rjha/spark-2.3.0-bin-hadoop2.7/python/lib/py4j-0.10.6-src.zip/py4j/java_gateway.py:2257)
2018-04-26 01:59:43 INFO  DAGScheduler:54 - Parents of final stage: List()
2018-04-26 01:59:43 INFO  DAGScheduler:54 - Missing parents: List()
2018-04-26 01:59:43 INFO  DAGScheduler:54 - Submitting ResultStage 2 (PythonRDD[10] at call at /home/rjha/spark-2.3.0-bin-hadoop2.7/python/lib/py4j-0.10.6-src.zip/py4j/java_gateway.py:2257), which has no missing parents
2018-04-26 01:59:43 INFO  MemoryStore:54 - Block input-0-1524722383200 stored as bytes in memory (estimated size 407.0 B, free 413.8 MB)
2018-04-26 01:59:43 INFO  BlockManagerInfo:54 - Added input-0-1524722383200 in memory on 10.0.2.15:33681 (size: 407.0 B, free: 413.9 MB)
2018-04-26 01:59:43 WARN  RandomBlockReplicationPolicy:66 - Expecting 1 replicas with only 0 peer/s.
2018-04-26 01:59:43 WARN  BlockManager:66 - Block input-0-1524722383200 replicated to only 0 peer(s) instead of 1 peers
2018-04-26 01:59:43 INFO  BlockGenerator:54 - Pushed block input-0-1524722383200
2018-04-26 01:59:43 INFO  MemoryStore:54 - Block broadcast_2 stored as values in memory (estimated size 7.1 KB, free 413.8 MB)
2018-04-26 01:59:43 INFO  MemoryStore:54 - Block broadcast_2_piece0 stored as bytes in memory (estimated size 4.1 KB, free 413.8 MB)
2018-04-26 01:59:43 INFO  BlockManagerInfo:54 - Added broadcast_2_piece0 in memory on 10.0.2.15:33681 (size: 4.1 KB, free: 413.9 MB)
2018-04-26 01:59:43 INFO  SparkContext:54 - Created broadcast 2 from broadcast at DAGScheduler.scala:1039
2018-04-26 01:59:43 INFO  DAGScheduler:54 - Submitting 1 missing tasks from ResultStage 2 (PythonRDD[10] at call at /home/rjha/spark-2.3.0-bin-hadoop2.7/python/lib/py4j-0.10.6-src.zip/py4j/java_gateway.py:2257) (first 15 tasks are for partitions Vector(0))
2018-04-26 01:59:43 INFO  TaskSchedulerImpl:54 - Adding task set 2.0 with 1 tasks
2018-04-26 01:59:43 INFO  TaskSetManager:54 - Starting task 0.0 in stage 2.0 (TID 2, localhost, executor driver, partition 0, ANY, 7772 bytes)
2018-04-26 01:59:43 INFO  Executor:54 - Running task 0.0 in stage 2.0 (TID 2)
2018-04-26 01:59:43 INFO  BlockManager:54 - Found block input-0-1524722381600 locally
2018-04-26 01:59:43 INFO  MemoryStore:54 - Block input-0-1524722383400 stored as bytes in memory (estimated size 345.0 B, free 413.8 MB)
2018-04-26 01:59:43 INFO  BlockManagerInfo:54 - Added input-0-1524722383400 in memory on 10.0.2.15:33681 (size: 345.0 B, free: 413.9 MB)
2018-04-26 01:59:43 WARN  RandomBlockReplicationPolicy:66 - Expecting 1 replicas with only 0 peer/s.
2018-04-26 01:59:43 WARN  BlockManager:66 - Block input-0-1524722383400 replicated to only 0 peer(s) instead of 1 peers
2018-04-26 01:59:43 INFO  PythonRunner:54 - Times: total = 44, boot = -574, init = 618, finish = 0
2018-04-26 01:59:43 INFO  PythonRunner:54 - Times: total = 52, boot = -542, init = 594, finish = 0
2018-04-26 01:59:43 INFO  Executor:54 - Finished task 0.0 in stage 2.0 (TID 2). 1267 bytes result sent to driver
2018-04-26 01:59:43 INFO  BlockGenerator:54 - Pushed block input-0-1524722383400
2018-04-26 01:59:43 INFO  TaskSetManager:54 - Finished task 0.0 in stage 2.0 (TID 2) in 110 ms on localhost (executor driver) (1/1)
2018-04-26 01:59:43 INFO  DAGScheduler:54 - ResultStage 2 (call at /home/rjha/spark-2.3.0-bin-hadoop2.7/python/lib/py4j-0.10.6-src.zip/py4j/java_gateway.py:2257) finished in 0.262 s
2018-04-26 01:59:43 INFO  DAGScheduler:54 - Job 4 finished: call at /home/rjha/spark-2.3.0-bin-hadoop2.7/python/lib/py4j-0.10.6-src.zip/py4j/java_gateway.py:2257, took 0.275308 s
2018-04-26 01:59:43 INFO  JobScheduler:54 - Finished job streaming job 1524722382000 ms.1 from job set of time 1524722382000 ms
2018-04-26 01:59:43 INFO  JobScheduler:54 - Total delay: 1.677 s for time 1524722382000 ms (execution: 1.616 s)
2018-04-26 01:59:43 INFO  PythonRDD:54 - Removing RDD 2 from persistence list
2018-04-26 01:59:43 INFO  JobScheduler:54 - Starting job streaming job 1524722383000 ms.0 from job set of time 1524722383000 ms
2018-04-26 01:59:43 INFO  TaskSchedulerImpl:54 - Removed TaskSet 2.0, whose tasks have all completed, from pool 
2018-04-26 01:59:43 INFO  BlockRDD:54 - Removing RDD 1 from persistence list
2018-04-26 01:59:43 INFO  MemoryStore:54 - Block input-0-1524722383600 stored as bytes in memory (estimated size 323.0 B, free 413.8 MB)
2018-04-26 01:59:43 INFO  ContextCleaner:54 - Cleaned accumulator 35
2018-04-26 01:59:43 INFO  BlockManagerInfo:54 - Added input-0-1524722383600 in memory on 10.0.2.15:33681 (size: 323.0 B, free: 413.9 MB)
2018-04-26 01:59:43 WARN  RandomBlockReplicationPolicy:66 - Expecting 1 replicas with only 0 peer/s.
2018-04-26 01:59:43 WARN  BlockManager:66 - Block input-0-1524722383600 replicated to only 0 peer(s) instead of 1 peers
2018-04-26 01:59:43 INFO  BlockGenerator:54 - Pushed block input-0-1524722383600
2018-04-26 01:59:43 INFO  ContextCleaner:54 - Cleaned accumulator 31
2018-04-26 01:59:43 INFO  ContextCleaner:54 - Cleaned accumulator 45
2018-04-26 01:59:43 INFO  ContextCleaner:54 - Cleaned accumulator 57
2018-04-26 01:59:43 INFO  ContextCleaner:54 - Cleaned accumulator 42
2018-04-26 01:59:43 INFO  ContextCleaner:54 - Cleaned accumulator 46
2018-04-26 01:59:43 INFO  ContextCleaner:54 - Cleaned accumulator 67
2018-04-26 01:59:43 INFO  ContextCleaner:54 - Cleaned accumulator 49
2018-04-26 01:59:43 INFO  ContextCleaner:54 - Cleaned accumulator 72
2018-04-26 01:59:43 INFO  SocketInputDStream:54 - Removing blocks of RDD BlockRDD[1] at socketTextStream at NativeMethodAccessorImpl.java:0 of time 1524722382000 ms
2018-04-26 01:59:43 INFO  BlockManager:54 - Removing RDD 2
2018-04-26 01:59:43 INFO  ReceivedBlockTracker:54 - Deleting batches: 
2018-04-26 01:59:43 INFO  InputInfoTracker:54 - remove old batch metadata: 
2018-04-26 01:59:43 INFO  BlockManager:54 - Removing RDD 1
2018-04-26 01:59:43 INFO  SparkContext:54 - Starting job: call at /home/rjha/spark-2.3.0-bin-hadoop2.7/python/lib/py4j-0.10.6-src.zip/py4j/java_gateway.py:2257
2018-04-26 01:59:43 INFO  DAGScheduler:54 - Got job 5 (call at /home/rjha/spark-2.3.0-bin-hadoop2.7/python/lib/py4j-0.10.6-src.zip/py4j/java_gateway.py:2257) with 5 output partitions
2018-04-26 01:59:43 INFO  DAGScheduler:54 - Final stage: ResultStage 3 (call at /home/rjha/spark-2.3.0-bin-hadoop2.7/python/lib/py4j-0.10.6-src.zip/py4j/java_gateway.py:2257)
2018-04-26 01:59:43 INFO  DAGScheduler:54 - Parents of final stage: List()
2018-04-26 01:59:43 INFO  DAGScheduler:54 - Missing parents: List()
2018-04-26 01:59:44 INFO  MemoryStore:54 - Block input-0-1524722383800 stored as bytes in memory (estimated size 387.0 B, free 413.8 MB)
2018-04-26 01:59:44 INFO  BlockManagerInfo:54 - Added input-0-1524722383800 in memory on 10.0.2.15:33681 (size: 387.0 B, free: 413.9 MB)
2018-04-26 01:59:44 INFO  DAGScheduler:54 - Submitting ResultStage 3 (PythonRDD[11] at call at /home/rjha/spark-2.3.0-bin-hadoop2.7/python/lib/py4j-0.10.6-src.zip/py4j/java_gateway.py:2257), which has no missing parents
2018-04-26 01:59:44 WARN  RandomBlockReplicationPolicy:66 - Expecting 1 replicas with only 0 peer/s.
2018-04-26 01:59:44 WARN  BlockManager:66 - Block input-0-1524722383800 replicated to only 0 peer(s) instead of 1 peers
2018-04-26 01:59:44 INFO  BlockGenerator:54 - Pushed block input-0-1524722383800
2018-04-26 01:59:44 INFO  JobScheduler:54 - Added jobs for time 1524722384000 ms
2018-04-26 01:59:44 INFO  MemoryStore:54 - Block broadcast_3 stored as values in memory (estimated size 7.5 KB, free 413.8 MB)
2018-04-26 01:59:44 INFO  BlockManagerInfo:54 - Removed broadcast_2_piece0 on 10.0.2.15:33681 in memory (size: 4.1 KB, free: 413.9 MB)
2018-04-26 01:59:44 INFO  MemoryStore:54 - Block broadcast_3_piece0 stored as bytes in memory (estimated size 4.4 KB, free 413.8 MB)
2018-04-26 01:59:44 INFO  ContextCleaner:54 - Cleaned accumulator 70
2018-04-26 01:59:44 INFO  ContextCleaner:54 - Cleaned accumulator 71
2018-04-26 01:59:44 INFO  ContextCleaner:54 - Cleaned accumulator 26
2018-04-26 01:59:44 INFO  ContextCleaner:54 - Cleaned accumulator 63
2018-04-26 01:59:44 INFO  ContextCleaner:54 - Cleaned accumulator 39
2018-04-26 01:59:44 INFO  ContextCleaner:54 - Cleaned accumulator 52
2018-04-26 01:59:44 INFO  ContextCleaner:54 - Cleaned accumulator 55
2018-04-26 01:59:44 INFO  ContextCleaner:54 - Cleaned accumulator 60
2018-04-26 01:59:44 INFO  ContextCleaner:54 - Cleaned accumulator 27
2018-04-26 01:59:44 INFO  ContextCleaner:54 - Cleaned accumulator 29
2018-04-26 01:59:44 INFO  ContextCleaner:54 - Cleaned accumulator 34
2018-04-26 01:59:44 INFO  ContextCleaner:54 - Cleaned accumulator 36
2018-04-26 01:59:44 INFO  ContextCleaner:54 - Cleaned accumulator 75
2018-04-26 01:59:44 INFO  ContextCleaner:54 - Cleaned accumulator 38
2018-04-26 01:59:44 INFO  ContextCleaner:54 - Cleaned accumulator 28
2018-04-26 01:59:44 INFO  BlockManagerInfo:54 - Added broadcast_3_piece0 in memory on 10.0.2.15:33681 (size: 4.4 KB, free: 413.9 MB)
2018-04-26 01:59:44 INFO  SparkContext:54 - Created broadcast 3 from broadcast at DAGScheduler.scala:1039
2018-04-26 01:59:44 INFO  ContextCleaner:54 - Cleaned accumulator 59
2018-04-26 01:59:44 INFO  ContextCleaner:54 - Cleaned accumulator 30
2018-04-26 01:59:44 INFO  ContextCleaner:54 - Cleaned accumulator 43
2018-04-26 01:59:44 INFO  ContextCleaner:54 - Cleaned accumulator 48
2018-04-26 01:59:44 INFO  ContextCleaner:54 - Cleaned accumulator 56
2018-04-26 01:59:44 INFO  ContextCleaner:54 - Cleaned accumulator 47
2018-04-26 01:59:44 INFO  ContextCleaner:54 - Cleaned accumulator 33
2018-04-26 01:59:44 INFO  ContextCleaner:54 - Cleaned accumulator 53
2018-04-26 01:59:44 INFO  ContextCleaner:54 - Cleaned accumulator 69
2018-04-26 01:59:44 INFO  ContextCleaner:54 - Cleaned accumulator 32
2018-04-26 01:59:44 INFO  ContextCleaner:54 - Cleaned accumulator 44
2018-04-26 01:59:44 INFO  DAGScheduler:54 - Submitting 5 missing tasks from ResultStage 3 (PythonRDD[11] at call at /home/rjha/spark-2.3.0-bin-hadoop2.7/python/lib/py4j-0.10.6-src.zip/py4j/java_gateway.py:2257) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4))
2018-04-26 01:59:44 INFO  TaskSchedulerImpl:54 - Adding task set 3.0 with 5 tasks
2018-04-26 01:59:44 INFO  ContextCleaner:54 - Cleaned accumulator 54
2018-04-26 01:59:44 INFO  ContextCleaner:54 - Cleaned accumulator 40
2018-04-26 01:59:44 INFO  TaskSetManager:54 - Starting task 0.0 in stage 3.0 (TID 3, localhost, executor driver, partition 0, ANY, 7772 bytes)
2018-04-26 01:59:44 INFO  Executor:54 - Running task 0.0 in stage 3.0 (TID 3)
2018-04-26 01:59:44 INFO  BlockManager:54 - Found block input-0-1524722381800 locally
2018-04-26 01:59:44 INFO  ContextCleaner:54 - Cleaned accumulator 50
2018-04-26 01:59:44 INFO  ContextCleaner:54 - Cleaned accumulator 64
2018-04-26 01:59:44 INFO  BlockManagerInfo:54 - Removed broadcast_1_piece0 on 10.0.2.15:33681 in memory (size: 4.4 KB, free: 413.9 MB)
2018-04-26 01:59:44 INFO  ContextCleaner:54 - Cleaned accumulator 62
2018-04-26 01:59:44 INFO  ContextCleaner:54 - Cleaned accumulator 37
2018-04-26 01:59:44 INFO  ContextCleaner:54 - Cleaned accumulator 66
2018-04-26 01:59:44 INFO  ContextCleaner:54 - Cleaned accumulator 58
2018-04-26 01:59:44 INFO  ContextCleaner:54 - Cleaned accumulator 68
2018-04-26 01:59:44 INFO  ContextCleaner:54 - Cleaned accumulator 41
2018-04-26 01:59:44 INFO  ContextCleaner:54 - Cleaned accumulator 65
2018-04-26 01:59:44 INFO  ContextCleaner:54 - Cleaned accumulator 73
2018-04-26 01:59:44 INFO  ContextCleaner:54 - Cleaned accumulator 61
2018-04-26 01:59:44 INFO  ContextCleaner:54 - Cleaned accumulator 74
2018-04-26 01:59:44 INFO  ContextCleaner:54 - Cleaned accumulator 51
2018-04-26 01:59:44 INFO  MemoryStore:54 - Block input-0-1524722384000 stored as bytes in memory (estimated size 259.0 B, free 413.8 MB)
2018-04-26 01:59:44 INFO  PythonRunner:54 - Times: total = 65, boot = -431, init = 495, finish = 1
2018-04-26 01:59:44 INFO  BlockManagerInfo:54 - Added input-0-1524722384000 in memory on 10.0.2.15:33681 (size: 259.0 B, free: 413.9 MB)
2018-04-26 01:59:44 WARN  RandomBlockReplicationPolicy:66 - Expecting 1 replicas with only 0 peer/s.
2018-04-26 01:59:44 WARN  BlockManager:66 - Block input-0-1524722384000 replicated to only 0 peer(s) instead of 1 peers
2018-04-26 01:59:44 INFO  BlockGenerator:54 - Pushed block input-0-1524722384000
2018-04-26 01:59:44 INFO  PythonRunner:54 - Times: total = 55, boot = -488, init = 539, finish = 4
2018-04-26 01:59:44 INFO  Executor:54 - Finished task 0.0 in stage 3.0 (TID 3). 1267 bytes result sent to driver
2018-04-26 01:59:44 INFO  TaskSetManager:54 - Starting task 1.0 in stage 3.0 (TID 4, localhost, executor driver, partition 1, ANY, 7772 bytes)
2018-04-26 01:59:44 INFO  Executor:54 - Running task 1.0 in stage 3.0 (TID 4)
2018-04-26 01:59:44 INFO  TaskSetManager:54 - Finished task 0.0 in stage 3.0 (TID 3) in 185 ms on localhost (executor driver) (1/5)
2018-04-26 01:59:44 INFO  BlockManager:54 - Found block input-0-1524722382000 locally
2018-04-26 01:59:44 INFO  PythonRunner:54 - Times: total = 49, boot = -98, init = 146, finish = 1
2018-04-26 01:59:44 INFO  MemoryStore:54 - Block input-0-1524722384200 stored as bytes in memory (estimated size 493.0 B, free 413.8 MB)
2018-04-26 01:59:44 INFO  BlockManagerInfo:54 - Added input-0-1524722384200 in memory on 10.0.2.15:33681 (size: 493.0 B, free: 413.9 MB)
2018-04-26 01:59:44 WARN  RandomBlockReplicationPolicy:66 - Expecting 1 replicas with only 0 peer/s.
2018-04-26 01:59:44 WARN  BlockManager:66 - Block input-0-1524722384200 replicated to only 0 peer(s) instead of 1 peers
2018-04-26 01:59:44 INFO  PythonRunner:54 - Times: total = 91, boot = -28, init = 69, finish = 50
2018-04-26 01:59:44 INFO  Executor:54 - Finished task 1.0 in stage 3.0 (TID 4). 1310 bytes result sent to driver
2018-04-26 01:59:44 INFO  BlockGenerator:54 - Pushed block input-0-1524722384200
2018-04-26 01:59:44 INFO  TaskSetManager:54 - Starting task 2.0 in stage 3.0 (TID 5, localhost, executor driver, partition 2, ANY, 7772 bytes)
2018-04-26 01:59:44 INFO  Executor:54 - Running task 2.0 in stage 3.0 (TID 5)
2018-04-26 01:59:44 INFO  TaskSetManager:54 - Finished task 1.0 in stage 3.0 (TID 4) in 303 ms on localhost (executor driver) (2/5)
2018-04-26 01:59:44 INFO  MemoryStore:54 - Block input-0-1524722384400 stored as bytes in memory (estimated size 300.0 B, free 413.8 MB)
2018-04-26 01:59:44 INFO  BlockManagerInfo:54 - Added input-0-1524722384400 in memory on 10.0.2.15:33681 (size: 300.0 B, free: 413.9 MB)
2018-04-26 01:59:44 WARN  RandomBlockReplicationPolicy:66 - Expecting 1 replicas with only 0 peer/s.
2018-04-26 01:59:44 WARN  BlockManager:66 - Block input-0-1524722384400 replicated to only 0 peer(s) instead of 1 peers
2018-04-26 01:59:44 INFO  BlockGenerator:54 - Pushed block input-0-1524722384400
2018-04-26 01:59:44 INFO  BlockManager:54 - Found block input-0-1524722382200 locally
2018-04-26 01:59:44 INFO  PythonRunner:54 - Times: total = 55, boot = -244, init = 299, finish = 0
2018-04-26 01:59:44 INFO  PythonRunner:54 - Times: total = 66, boot = 21, init = 40, finish = 5
2018-04-26 01:59:44 INFO  Executor:54 - Finished task 2.0 in stage 3.0 (TID 5). 1310 bytes result sent to driver
2018-04-26 01:59:44 INFO  TaskSetManager:54 - Starting task 3.0 in stage 3.0 (TID 6, localhost, executor driver, partition 3, ANY, 7772 bytes)
2018-04-26 01:59:44 INFO  TaskSetManager:54 - Finished task 2.0 in stage 3.0 (TID 5) in 428 ms on localhost (executor driver) (3/5)
2018-04-26 01:59:44 INFO  Executor:54 - Running task 3.0 in stage 3.0 (TID 6)
2018-04-26 01:59:44 INFO  BlockManager:54 - Found block input-0-1524722382400 locally
2018-04-26 01:59:44 INFO  MemoryStore:54 - Block input-0-1524722384600 stored as bytes in memory (estimated size 234.0 B, free 413.8 MB)
2018-04-26 01:59:44 INFO  BlockManagerInfo:54 - Added input-0-1524722384600 in memory on 10.0.2.15:33681 (size: 234.0 B, free: 413.9 MB)
2018-04-26 01:59:44 WARN  RandomBlockReplicationPolicy:66 - Expecting 1 replicas with only 0 peer/s.
2018-04-26 01:59:44 WARN  BlockManager:66 - Block input-0-1524722384600 replicated to only 0 peer(s) instead of 1 peers
2018-04-26 01:59:44 INFO  BlockGenerator:54 - Pushed block input-0-1524722384600
2018-04-26 01:59:44 INFO  PythonRunner:54 - Times: total = 50, boot = -36, init = 86, finish = 0
2018-04-26 01:59:45 INFO  MemoryStore:54 - Block input-0-1524722384800 stored as bytes in memory (estimated size 325.0 B, free 413.8 MB)
2018-04-26 01:59:45 INFO  BlockManagerInfo:54 - Added input-0-1524722384800 in memory on 10.0.2.15:33681 (size: 325.0 B, free: 413.9 MB)
2018-04-26 01:59:45 WARN  RandomBlockReplicationPolicy:66 - Expecting 1 replicas with only 0 peer/s.
2018-04-26 01:59:45 WARN  BlockManager:66 - Block input-0-1524722384800 replicated to only 0 peer(s) instead of 1 peers
2018-04-26 01:59:45 INFO  BlockGenerator:54 - Pushed block input-0-1524722384800
2018-04-26 01:59:45 INFO  PythonRunner:54 - Times: total = 136, boot = 0, init = 96, finish = 40
2018-04-26 01:59:45 INFO  Executor:54 - Finished task 3.0 in stage 3.0 (TID 6). 1267 bytes result sent to driver
2018-04-26 01:59:45 INFO  TaskSetManager:54 - Starting task 4.0 in stage 3.0 (TID 7, localhost, executor driver, partition 4, ANY, 7772 bytes)
2018-04-26 01:59:45 INFO  TaskSetManager:54 - Finished task 3.0 in stage 3.0 (TID 6) in 326 ms on localhost (executor driver) (4/5)
2018-04-26 01:59:45 INFO  Executor:54 - Running task 4.0 in stage 3.0 (TID 7)
2018-04-26 01:59:45 INFO  JobScheduler:54 - Added jobs for time 1524722385000 ms
2018-04-26 01:59:45 INFO  BlockManager:54 - Found block input-0-1524722382600 locally
2018-04-26 01:59:45 INFO  MemoryStore:54 - Block input-0-1524722385000 stored as bytes in memory (estimated size 343.0 B, free 413.8 MB)
2018-04-26 01:59:45 INFO  BlockManagerInfo:54 - Added input-0-1524722385000 in memory on 10.0.2.15:33681 (size: 343.0 B, free: 413.9 MB)
2018-04-26 01:59:45 WARN  RandomBlockReplicationPolicy:66 - Expecting 1 replicas with only 0 peer/s.
2018-04-26 01:59:45 WARN  BlockManager:66 - Block input-0-1524722385000 replicated to only 0 peer(s) instead of 1 peers
2018-04-26 01:59:45 INFO  BlockGenerator:54 - Pushed block input-0-1524722385000
2018-04-26 01:59:45 INFO  PythonRunner:54 - Times: total = 149, boot = 148, init = 0, finish = 1
2018-04-26 01:59:45 INFO  PythonRunner:54 - Times: total = 131, boot = -18, init = 143, finish = 6
2018-04-26 01:59:45 INFO  Executor:54 - Finished task 4.0 in stage 3.0 (TID 7). 1267 bytes result sent to driver
2018-04-26 01:59:45 INFO  TaskSetManager:54 - Finished task 4.0 in stage 3.0 (TID 7) in 274 ms on localhost (executor driver) (5/5)
2018-04-26 01:59:45 INFO  TaskSchedulerImpl:54 - Removed TaskSet 3.0, whose tasks have all completed, from pool 
2018-04-26 01:59:45 INFO  DAGScheduler:54 - ResultStage 3 (call at /home/rjha/spark-2.3.0-bin-hadoop2.7/python/lib/py4j-0.10.6-src.zip/py4j/java_gateway.py:2257) finished in 1.339 s
2018-04-26 01:59:45 INFO  DAGScheduler:54 - Job 5 finished: call at /home/rjha/spark-2.3.0-bin-hadoop2.7/python/lib/py4j-0.10.6-src.zip/py4j/java_gateway.py:2257, took 1.400854 s
2018-04-26 01:59:45 INFO  MemoryStore:54 - Block input-0-1524722385200 stored as bytes in memory (estimated size 306.0 B, free 413.8 MB)
2018-04-26 01:59:45 INFO  BlockManagerInfo:54 - Added input-0-1524722385200 in memory on 10.0.2.15:33681 (size: 306.0 B, free: 413.9 MB)
2018-04-26 01:59:45 WARN  RandomBlockReplicationPolicy:66 - Expecting 1 replicas with only 0 peer/s.
2018-04-26 01:59:45 WARN  BlockManager:66 - Block input-0-1524722385200 replicated to only 0 peer(s) instead of 1 peers
2018-04-26 01:59:45 INFO  BlockGenerator:54 - Pushed block input-0-1524722385200
2018-04-26 01:59:45 INFO  JobScheduler:54 - Finished job streaming job 1524722383000 ms.0 from job set of time 1524722383000 ms
2018-04-26 01:59:45 INFO  JobScheduler:54 - Starting job streaming job 1524722383000 ms.1 from job set of time 1524722383000 ms
2018-04-26 01:59:45 INFO  SparkContext:54 - Starting job: call at /home/rjha/spark-2.3.0-bin-hadoop2.7/python/lib/py4j-0.10.6-src.zip/py4j/java_gateway.py:2257
2018-04-26 01:59:45 INFO  DAGScheduler:54 - Got job 6 (call at /home/rjha/spark-2.3.0-bin-hadoop2.7/python/lib/py4j-0.10.6-src.zip/py4j/java_gateway.py:2257) with 5 output partitions
2018-04-26 01:59:45 INFO  DAGScheduler:54 - Final stage: ResultStage 4 (call at /home/rjha/spark-2.3.0-bin-hadoop2.7/python/lib/py4j-0.10.6-src.zip/py4j/java_gateway.py:2257)
2018-04-26 01:59:45 INFO  DAGScheduler:54 - Parents of final stage: List()
2018-04-26 01:59:45 INFO  DAGScheduler:54 - Missing parents: List()
2018-04-26 01:59:45 INFO  DAGScheduler:54 - Submitting ResultStage 4 (PythonRDD[16] at call at /home/rjha/spark-2.3.0-bin-hadoop2.7/python/lib/py4j-0.10.6-src.zip/py4j/java_gateway.py:2257), which has no missing parents
2018-04-26 01:59:45 INFO  MemoryStore:54 - Block broadcast_4 stored as values in memory (estimated size 7.1 KB, free 413.8 MB)
2018-04-26 01:59:45 INFO  MemoryStore:54 - Block broadcast_4_piece0 stored as bytes in memory (estimated size 4.1 KB, free 413.8 MB)
2018-04-26 01:59:45 INFO  BlockManagerInfo:54 - Added broadcast_4_piece0 in memory on 10.0.2.15:33681 (size: 4.1 KB, free: 413.9 MB)
2018-04-26 01:59:45 INFO  SparkContext:54 - Created broadcast 4 from broadcast at DAGScheduler.scala:1039
2018-04-26 01:59:45 INFO  DAGScheduler:54 - Submitting 5 missing tasks from ResultStage 4 (PythonRDD[16] at call at /home/rjha/spark-2.3.0-bin-hadoop2.7/python/lib/py4j-0.10.6-src.zip/py4j/java_gateway.py:2257) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4))
2018-04-26 01:59:45 INFO  TaskSchedulerImpl:54 - Adding task set 4.0 with 5 tasks
2018-04-26 01:59:45 INFO  TaskSetManager:54 - Starting task 0.0 in stage 4.0 (TID 8, localhost, executor driver, partition 0, ANY, 7772 bytes)
2018-04-26 01:59:45 INFO  Executor:54 - Running task 0.0 in stage 4.0 (TID 8)
2018-04-26 01:59:45 INFO  BlockManager:54 - Found block input-0-1524722381800 locally
2018-04-26 01:59:45 INFO  MemoryStore:54 - Block input-0-1524722385400 stored as bytes in memory (estimated size 384.0 B, free 413.8 MB)
2018-04-26 01:59:45 INFO  BlockManagerInfo:54 - Added input-0-1524722385400 in memory on 10.0.2.15:33681 (size: 384.0 B, free: 413.9 MB)
2018-04-26 01:59:45 WARN  RandomBlockReplicationPolicy:66 - Expecting 1 replicas with only 0 peer/s.
2018-04-26 01:59:45 WARN  BlockManager:66 - Block input-0-1524722385400 replicated to only 0 peer(s) instead of 1 peers
2018-04-26 01:59:45 INFO  BlockGenerator:54 - Pushed block input-0-1524722385400
2018-04-26 01:59:45 INFO  PythonRunner:54 - Times: total = 9, boot = -111, init = 120, finish = 0
2018-04-26 01:59:45 INFO  PythonRunner:54 - Times: total = 55, boot = -126, init = 181, finish = 0
2018-04-26 01:59:45 INFO  Executor:54 - Finished task 0.0 in stage 4.0 (TID 8). 1267 bytes result sent to driver
2018-04-26 01:59:45 INFO  TaskSetManager:54 - Starting task 1.0 in stage 4.0 (TID 9, localhost, executor driver, partition 1, ANY, 7772 bytes)
2018-04-26 01:59:45 INFO  TaskSetManager:54 - Finished task 0.0 in stage 4.0 (TID 8) in 99 ms on localhost (executor driver) (1/5)
2018-04-26 01:59:45 INFO  Executor:54 - Running task 1.0 in stage 4.0 (TID 9)
2018-04-26 01:59:45 INFO  BlockManager:54 - Found block input-0-1524722382000 locally
2018-04-26 01:59:45 INFO  PythonRunner:54 - Times: total = 46, boot = -64, init = 110, finish = 0
2018-04-26 01:59:45 INFO  PythonRunner:54 - Times: total = 55, boot = 14, init = 41, finish = 0
2018-04-26 01:59:45 INFO  Executor:54 - Finished task 1.0 in stage 4.0 (TID 9). 1267 bytes result sent to driver
2018-04-26 01:59:45 INFO  TaskSetManager:54 - Starting task 2.0 in stage 4.0 (TID 10, localhost, executor driver, partition 2, ANY, 7772 bytes)
2018-04-26 01:59:45 INFO  TaskSetManager:54 - Finished task 1.0 in stage 4.0 (TID 9) in 110 ms on localhost (executor driver) (2/5)
2018-04-26 01:59:45 INFO  MemoryStore:54 - Block input-0-1524722385600 stored as bytes in memory (estimated size 424.0 B, free 413.8 MB)
2018-04-26 01:59:45 INFO  Executor:54 - Running task 2.0 in stage 4.0 (TID 10)
2018-04-26 01:59:45 INFO  BlockManagerInfo:54 - Added input-0-1524722385600 in memory on 10.0.2.15:33681 (size: 424.0 B, free: 413.9 MB)
2018-04-26 01:59:45 WARN  RandomBlockReplicationPolicy:66 - Expecting 1 replicas with only 0 peer/s.
2018-04-26 01:59:45 WARN  BlockManager:66 - Block input-0-1524722385600 replicated to only 0 peer(s) instead of 1 peers
2018-04-26 01:59:45 INFO  BlockGenerator:54 - Pushed block input-0-1524722385600
2018-04-26 01:59:45 INFO  BlockManager:54 - Found block input-0-1524722382200 locally
2018-04-26 01:59:45 INFO  PythonRunner:54 - Times: total = 93, boot = -53, init = 146, finish = 0
2018-04-26 01:59:45 INFO  PythonRunner:54 - Times: total = 43, boot = 14, init = 29, finish = 0
2018-04-26 01:59:45 INFO  Executor:54 - Finished task 2.0 in stage 4.0 (TID 10). 1267 bytes result sent to driver
2018-04-26 01:59:45 INFO  TaskSetManager:54 - Starting task 3.0 in stage 4.0 (TID 11, localhost, executor driver, partition 3, ANY, 7772 bytes)
2018-04-26 01:59:45 INFO  TaskSetManager:54 - Finished task 2.0 in stage 4.0 (TID 10) in 142 ms on localhost (executor driver) (3/5)
2018-04-26 01:59:45 INFO  ContextCleaner:54 - Cleaned accumulator 83
2018-04-26 01:59:45 INFO  ContextCleaner:54 - Cleaned accumulator 90
2018-04-26 01:59:45 INFO  ContextCleaner:54 - Cleaned accumulator 99
2018-04-26 01:59:45 INFO  ContextCleaner:54 - Cleaned accumulator 80
2018-04-26 01:59:45 INFO  ContextCleaner:54 - Cleaned accumulator 97
2018-04-26 01:59:45 INFO  ContextCleaner:54 - Cleaned accumulator 81
2018-04-26 01:59:45 INFO  ContextCleaner:54 - Cleaned accumulator 76
2018-04-26 01:59:45 INFO  ContextCleaner:54 - Cleaned accumulator 93
2018-04-26 01:59:45 INFO  ContextCleaner:54 - Cleaned accumulator 78
2018-04-26 01:59:45 INFO  ContextCleaner:54 - Cleaned accumulator 95
2018-04-26 01:59:45 INFO  ContextCleaner:54 - Cleaned accumulator 79
2018-04-26 01:59:45 INFO  ContextCleaner:54 - Cleaned accumulator 87
2018-04-26 01:59:45 INFO  Executor:54 - Running task 3.0 in stage 4.0 (TID 11)
2018-04-26 01:59:45 INFO  BlockManagerInfo:54 - Removed broadcast_3_piece0 on 10.0.2.15:33681 in memory (size: 4.4 KB, free: 413.9 MB)
2018-04-26 01:59:45 INFO  ContextCleaner:54 - Cleaned accumulator 88
2018-04-26 01:59:45 INFO  ContextCleaner:54 - Cleaned accumulator 89
2018-04-26 01:59:45 INFO  ContextCleaner:54 - Cleaned accumulator 92
2018-04-26 01:59:45 INFO  ContextCleaner:54 - Cleaned accumulator 77
2018-04-26 01:59:45 INFO  ContextCleaner:54 - Cleaned accumulator 91
2018-04-26 01:59:45 INFO  ContextCleaner:54 - Cleaned accumulator 96
2018-04-26 01:59:45 INFO  ContextCleaner:54 - Cleaned accumulator 98
2018-04-26 01:59:45 INFO  ContextCleaner:54 - Cleaned accumulator 86
2018-04-26 01:59:45 INFO  ContextCleaner:54 - Cleaned accumulator 84
2018-04-26 01:59:45 INFO  ContextCleaner:54 - Cleaned accumulator 100
2018-04-26 01:59:45 INFO  ContextCleaner:54 - Cleaned accumulator 82
2018-04-26 01:59:45 INFO  ContextCleaner:54 - Cleaned accumulator 94
2018-04-26 01:59:45 INFO  ContextCleaner:54 - Cleaned accumulator 85
2018-04-26 01:59:45 INFO  BlockManager:54 - Found block input-0-1524722382400 locally
2018-04-26 01:59:46 INFO  MemoryStore:54 - Block input-0-1524722385800 stored as bytes in memory (estimated size 324.0 B, free 413.8 MB)
2018-04-26 01:59:46 INFO  BlockManagerInfo:54 - Added input-0-1524722385800 in memory on 10.0.2.15:33681 (size: 324.0 B, free: 413.9 MB)
2018-04-26 01:59:46 INFO  PythonRunner:54 - Times: total = 46, boot = -9, init = 55, finish = 0
2018-04-26 01:59:46 WARN  RandomBlockReplicationPolicy:66 - Expecting 1 replicas with only 0 peer/s.
2018-04-26 01:59:46 WARN  BlockManager:66 - Block input-0-1524722385800 replicated to only 0 peer(s) instead of 1 peers
2018-04-26 01:59:46 INFO  BlockGenerator:54 - Pushed block input-0-1524722385800
2018-04-26 01:59:46 INFO  PythonRunner:54 - Times: total = 57, boot = 4, init = 53, finish = 0
2018-04-26 01:59:46 INFO  Executor:54 - Finished task 3.0 in stage 4.0 (TID 11). 1267 bytes result sent to driver
2018-04-26 01:59:46 INFO  TaskSetManager:54 - Starting task 4.0 in stage 4.0 (TID 12, localhost, executor driver, partition 4, ANY, 7772 bytes)
2018-04-26 01:59:46 INFO  TaskSetManager:54 - Finished task 3.0 in stage 4.0 (TID 11) in 110 ms on localhost (executor driver) (4/5)
2018-04-26 01:59:46 INFO  Executor:54 - Running task 4.0 in stage 4.0 (TID 12)
2018-04-26 01:59:46 INFO  BlockManager:54 - Found block input-0-1524722382600 locally
2018-04-26 01:59:46 INFO  JobScheduler:54 - Added jobs for time 1524722386000 ms
2018-04-26 01:59:46 INFO  PythonRunner:54 - Times: total = 49, boot = 25, init = 24, finish = 0
2018-04-26 01:59:46 INFO  PythonRunner:54 - Times: total = 55, boot = 4, init = 51, finish = 0
2018-04-26 01:59:46 INFO  Executor:54 - Finished task 4.0 in stage 4.0 (TID 12). 1224 bytes result sent to driver
2018-04-26 01:59:46 INFO  TaskSetManager:54 - Finished task 4.0 in stage 4.0 (TID 12) in 89 ms on localhost (executor driver) (5/5)
2018-04-26 01:59:46 INFO  TaskSchedulerImpl:54 - Removed TaskSet 4.0, whose tasks have all completed, from pool 
2018-04-26 01:59:46 INFO  DAGScheduler:54 - ResultStage 4 (call at /home/rjha/spark-2.3.0-bin-hadoop2.7/python/lib/py4j-0.10.6-src.zip/py4j/java_gateway.py:2257) finished in 0.573 s
2018-04-26 01:59:46 INFO  DAGScheduler:54 - Job 6 finished: call at /home/rjha/spark-2.3.0-bin-hadoop2.7/python/lib/py4j-0.10.6-src.zip/py4j/java_gateway.py:2257, took 0.599069 s
2018-04-26 01:59:46 INFO  JobScheduler:54 - Finished job streaming job 1524722383000 ms.1 from job set of time 1524722383000 ms
2018-04-26 01:59:46 INFO  JobScheduler:54 - Total delay: 3.156 s for time 1524722383000 ms (execution: 2.453 s)
2018-04-26 01:59:46 INFO  JobScheduler:54 - Starting job streaming job 1524722384000 ms.0 from job set of time 1524722384000 ms
2018-04-26 01:59:46 INFO  PythonRDD:54 - Removing RDD 6 from persistence list
2018-04-26 01:59:46 INFO  BlockManager:54 - Removing RDD 6
2018-04-26 01:59:46 INFO  BlockRDD:54 - Removing RDD 5 from persistence list
2018-04-26 01:59:46 INFO  BlockManager:54 - Removing RDD 5
2018-04-26 01:59:46 INFO  SocketInputDStream:54 - Removing blocks of RDD BlockRDD[5] at socketTextStream at NativeMethodAccessorImpl.java:0 of time 1524722383000 ms
2018-04-26 01:59:46 INFO  BlockManagerInfo:54 - Removed input-0-1524722381600 on 10.0.2.15:33681 in memory (size: 198.0 B, free: 413.9 MB)
2018-04-26 01:59:46 INFO  ReceivedBlockTracker:54 - Deleting batches: 1524722381000 ms
2018-04-26 01:59:46 INFO  InputInfoTracker:54 - remove old batch metadata: 1524722381000 ms
2018-04-26 01:59:46 INFO  MemoryStore:54 - Block input-0-1524722386000 stored as bytes in memory (estimated size 403.0 B, free 413.8 MB)
2018-04-26 01:59:46 INFO  BlockManagerInfo:54 - Added input-0-1524722386000 in memory on 10.0.2.15:33681 (size: 403.0 B, free: 413.9 MB)
2018-04-26 01:59:46 WARN  RandomBlockReplicationPolicy:66 - Expecting 1 replicas with only 0 peer/s.
2018-04-26 01:59:46 WARN  BlockManager:66 - Block input-0-1524722386000 replicated to only 0 peer(s) instead of 1 peers
2018-04-26 01:59:46 INFO  BlockGenerator:54 - Pushed block input-0-1524722386000
2018-04-26 01:59:46 INFO  SparkContext:54 - Starting job: call at /home/rjha/spark-2.3.0-bin-hadoop2.7/python/lib/py4j-0.10.6-src.zip/py4j/java_gateway.py:2257
2018-04-26 01:59:46 INFO  DAGScheduler:54 - Got job 7 (call at /home/rjha/spark-2.3.0-bin-hadoop2.7/python/lib/py4j-0.10.6-src.zip/py4j/java_gateway.py:2257) with 5 output partitions
2018-04-26 01:59:46 INFO  DAGScheduler:54 - Final stage: ResultStage 5 (call at /home/rjha/spark-2.3.0-bin-hadoop2.7/python/lib/py4j-0.10.6-src.zip/py4j/java_gateway.py:2257)
2018-04-26 01:59:46 INFO  DAGScheduler:54 - Parents of final stage: List()
2018-04-26 01:59:46 INFO  DAGScheduler:54 - Missing parents: List()
2018-04-26 01:59:46 INFO  DAGScheduler:54 - Submitting ResultStage 5 (PythonRDD[19] at call at /home/rjha/spark-2.3.0-bin-hadoop2.7/python/lib/py4j-0.10.6-src.zip/py4j/java_gateway.py:2257), which has no missing parents
2018-04-26 01:59:46 INFO  MemoryStore:54 - Block broadcast_5 stored as values in memory (estimated size 7.5 KB, free 413.8 MB)
2018-04-26 01:59:46 INFO  MemoryStore:54 - Block broadcast_5_piece0 stored as bytes in memory (estimated size 4.4 KB, free 413.8 MB)
2018-04-26 01:59:46 INFO  BlockManagerInfo:54 - Added broadcast_5_piece0 in memory on 10.0.2.15:33681 (size: 4.4 KB, free: 413.9 MB)
2018-04-26 01:59:46 INFO  SparkContext:54 - Created broadcast 5 from broadcast at DAGScheduler.scala:1039
2018-04-26 01:59:46 INFO  DAGScheduler:54 - Submitting 5 missing tasks from ResultStage 5 (PythonRDD[19] at call at /home/rjha/spark-2.3.0-bin-hadoop2.7/python/lib/py4j-0.10.6-src.zip/py4j/java_gateway.py:2257) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4))
2018-04-26 01:59:46 INFO  TaskSchedulerImpl:54 - Adding task set 5.0 with 5 tasks
2018-04-26 01:59:46 INFO  TaskSetManager:54 - Starting task 0.0 in stage 5.0 (TID 13, localhost, executor driver, partition 0, ANY, 7772 bytes)
2018-04-26 01:59:46 INFO  Executor:54 - Running task 0.0 in stage 5.0 (TID 13)
2018-04-26 01:59:46 INFO  BlockManager:54 - Found block input-0-1524722382800 locally
2018-04-26 01:59:46 INFO  PythonRunner:54 - Times: total = 47, boot = -235, init = 282, finish = 0
2018-04-26 01:59:46 INFO  PythonRunner:54 - Times: total = 58, boot = -106, init = 159, finish = 5
2018-04-26 01:59:46 INFO  MemoryStore:54 - Block input-0-1524722386200 stored as bytes in memory (estimated size 399.0 B, free 413.8 MB)
2018-04-26 01:59:46 INFO  BlockManagerInfo:54 - Added input-0-1524722386200 in memory on 10.0.2.15:33681 (size: 399.0 B, free: 413.9 MB)
2018-04-26 01:59:46 INFO  Executor:54 - Finished task 0.0 in stage 5.0 (TID 13). 1267 bytes result sent to driver
2018-04-26 01:59:46 INFO  TaskSetManager:54 - Starting task 1.0 in stage 5.0 (TID 14, localhost, executor driver, partition 1, ANY, 7772 bytes)
2018-04-26 01:59:46 INFO  Executor:54 - Running task 1.0 in stage 5.0 (TID 14)
2018-04-26 01:59:46 INFO  TaskSetManager:54 - Finished task 0.0 in stage 5.0 (TID 13) in 95 ms on localhost (executor driver) (1/5)
2018-04-26 01:59:46 WARN  RandomBlockReplicationPolicy:66 - Expecting 1 replicas with only 0 peer/s.
2018-04-26 01:59:46 WARN  BlockManager:66 - Block input-0-1524722386200 replicated to only 0 peer(s) instead of 1 peers
2018-04-26 01:59:46 INFO  BlockManager:54 - Found block input-0-1524722383000 locally
2018-04-26 01:59:46 INFO  BlockGenerator:54 - Pushed block input-0-1524722386200
2018-04-26 01:59:46 INFO  PythonRunner:54 - Times: total = 65, boot = 65, init = 0, finish = 0
2018-04-26 01:59:46 INFO  PythonRunner:54 - Times: total = 68, boot = 46, init = 18, finish = 4
2018-04-26 01:59:46 INFO  Executor:54 - Finished task 1.0 in stage 5.0 (TID 14). 1267 bytes result sent to driver
2018-04-26 01:59:46 INFO  TaskSetManager:54 - Starting task 2.0 in stage 5.0 (TID 15, localhost, executor driver, partition 2, ANY, 7772 bytes)
2018-04-26 01:59:46 INFO  TaskSetManager:54 - Finished task 1.0 in stage 5.0 (TID 14) in 165 ms on localhost (executor driver) (2/5)
2018-04-26 01:59:46 INFO  Executor:54 - Running task 2.0 in stage 5.0 (TID 15)
2018-04-26 01:59:46 INFO  MemoryStore:54 - Block input-0-1524722386400 stored as bytes in memory (estimated size 389.0 B, free 413.8 MB)
2018-04-26 01:59:46 INFO  BlockManagerInfo:54 - Added input-0-1524722386400 in memory on 10.0.2.15:33681 (size: 389.0 B, free: 413.9 MB)
2018-04-26 01:59:46 WARN  RandomBlockReplicationPolicy:66 - Expecting 1 replicas with only 0 peer/s.
2018-04-26 01:59:46 WARN  BlockManager:66 - Block input-0-1524722386400 replicated to only 0 peer(s) instead of 1 peers
2018-04-26 01:59:46 INFO  BlockGenerator:54 - Pushed block input-0-1524722386400
2018-04-26 01:59:46 INFO  BlockManager:54 - Found block input-0-1524722383200 locally
2018-04-26 01:59:46 INFO  PythonRunner:54 - Times: total = 67, boot = 66, init = 1, finish = 0
2018-04-26 01:59:46 INFO  PythonRunner:54 - Times: total = 57, boot = 29, init = 22, finish = 6
2018-04-26 01:59:46 INFO  Executor:54 - Finished task 2.0 in stage 5.0 (TID 15). 1267 bytes result sent to driver
2018-04-26 01:59:46 INFO  TaskSetManager:54 - Starting task 3.0 in stage 5.0 (TID 16, localhost, executor driver, partition 3, ANY, 7772 bytes)
2018-04-26 01:59:46 INFO  Executor:54 - Running task 3.0 in stage 5.0 (TID 16)
2018-04-26 01:59:46 INFO  TaskSetManager:54 - Finished task 2.0 in stage 5.0 (TID 15) in 170 ms on localhost (executor driver) (3/5)
2018-04-26 01:59:46 INFO  BlockManager:54 - Found block input-0-1524722383400 locally
2018-04-26 01:59:46 INFO  MemoryStore:54 - Block input-0-1524722386600 stored as bytes in memory (estimated size 401.0 B, free 413.8 MB)
2018-04-26 01:59:46 INFO  BlockManagerInfo:54 - Added input-0-1524722386600 in memory on 10.0.2.15:33681 (size: 401.0 B, free: 413.9 MB)
2018-04-26 01:59:46 WARN  RandomBlockReplicationPolicy:66 - Expecting 1 replicas with only 0 peer/s.
2018-04-26 01:59:46 WARN  BlockManager:66 - Block input-0-1524722386600 replicated to only 0 peer(s) instead of 1 peers
2018-04-26 01:59:46 INFO  BlockGenerator:54 - Pushed block input-0-1524722386600
2018-04-26 01:59:46 INFO  PythonRunner:54 - Times: total = 87, boot = 86, init = 0, finish = 1
2018-04-26 01:59:46 INFO  PythonRunner:54 - Times: total = 65, boot = 43, init = 14, finish = 8
2018-04-26 01:59:46 INFO  Executor:54 - Finished task 3.0 in stage 5.0 (TID 16). 1310 bytes result sent to driver
2018-04-26 01:59:46 INFO  TaskSetManager:54 - Starting task 4.0 in stage 5.0 (TID 17, localhost, executor driver, partition 4, ANY, 7772 bytes)
2018-04-26 01:59:46 INFO  Executor:54 - Running task 4.0 in stage 5.0 (TID 17)
2018-04-26 01:59:46 INFO  TaskSetManager:54 - Finished task 3.0 in stage 5.0 (TID 16) in 134 ms on localhost (executor driver) (4/5)
2018-04-26 01:59:46 INFO  BlockManager:54 - Found block input-0-1524722383600 locally
2018-04-26 01:59:46 INFO  ContextCleaner:54 - Cleaned accumulator 112
2018-04-26 01:59:46 INFO  ContextCleaner:54 - Cleaned accumulator 110
2018-04-26 01:59:46 INFO  ContextCleaner:54 - Cleaned accumulator 107
2018-04-26 01:59:46 INFO  ContextCleaner:54 - Cleaned accumulator 104
2018-04-26 01:59:46 INFO  ContextCleaner:54 - Cleaned accumulator 102
2018-04-26 01:59:46 INFO  ContextCleaner:54 - Cleaned accumulator 121
2018-04-26 01:59:46 INFO  ContextCleaner:54 - Cleaned accumulator 106
2018-04-26 01:59:46 INFO  ContextCleaner:54 - Cleaned accumulator 115
2018-04-26 01:59:46 INFO  ContextCleaner:54 - Cleaned accumulator 103
2018-04-26 01:59:46 INFO  ContextCleaner:54 - Cleaned accumulator 109
2018-04-26 01:59:46 INFO  ContextCleaner:54 - Cleaned accumulator 108
2018-04-26 01:59:46 INFO  ContextCleaner:54 - Cleaned accumulator 120
2018-04-26 01:59:46 INFO  ContextCleaner:54 - Cleaned accumulator 113
2018-04-26 01:59:46 INFO  ContextCleaner:54 - Cleaned accumulator 117
2018-04-26 01:59:46 INFO  ContextCleaner:54 - Cleaned accumulator 116
2018-04-26 01:59:46 INFO  ContextCleaner:54 - Cleaned accumulator 125
2018-04-26 01:59:46 INFO  ContextCleaner:54 - Cleaned accumulator 101
2018-04-26 01:59:46 INFO  ContextCleaner:54 - Cleaned accumulator 114
2018-04-26 01:59:46 INFO  ContextCleaner:54 - Cleaned accumulator 119
2018-04-26 01:59:46 INFO  ContextCleaner:54 - Cleaned accumulator 123
2018-04-26 01:59:46 INFO  BlockManagerInfo:54 - Removed broadcast_4_piece0 on 10.0.2.15:33681 in memory (size: 4.1 KB, free: 413.9 MB)
2018-04-26 01:59:46 INFO  ContextCleaner:54 - Cleaned accumulator 105
2018-04-26 01:59:46 INFO  ContextCleaner:54 - Cleaned accumulator 122
2018-04-26 01:59:46 INFO  ContextCleaner:54 - Cleaned accumulator 111
2018-04-26 01:59:46 INFO  ContextCleaner:54 - Cleaned accumulator 124
2018-04-26 01:59:46 INFO  ContextCleaner:54 - Cleaned accumulator 118
2018-04-26 01:59:46 INFO  PythonRunner:54 - Times: total = 52, boot = 41, init = 10, finish = 1
2018-04-26 01:59:47 INFO  MemoryStore:54 - Block input-0-1524722386800 stored as bytes in memory (estimated size 412.0 B, free 413.8 MB)
2018-04-26 01:59:47 INFO  BlockManagerInfo:54 - Added input-0-1524722386800 in memory on 10.0.2.15:33681 (size: 412.0 B, free: 413.9 MB)
2018-04-26 01:59:47 WARN  RandomBlockReplicationPolicy:66 - Expecting 1 replicas with only 0 peer/s.
2018-04-26 01:59:47 WARN  BlockManager:66 - Block input-0-1524722386800 replicated to only 0 peer(s) instead of 1 peers
2018-04-26 01:59:47 INFO  BlockGenerator:54 - Pushed block input-0-1524722386800
2018-04-26 01:59:47 INFO  PythonRunner:54 - Times: total = 158, boot = 96, init = 1, finish = 61
2018-04-26 01:59:47 INFO  Executor:54 - Finished task 4.0 in stage 5.0 (TID 17). 1310 bytes result sent to driver
2018-04-26 01:59:47 INFO  TaskSetManager:54 - Finished task 4.0 in stage 5.0 (TID 17) in 270 ms on localhost (executor driver) (5/5)
2018-04-26 01:59:47 INFO  TaskSchedulerImpl:54 - Removed TaskSet 5.0, whose tasks have all completed, from pool 
2018-04-26 01:59:47 INFO  DAGScheduler:54 - ResultStage 5 (call at /home/rjha/spark-2.3.0-bin-hadoop2.7/python/lib/py4j-0.10.6-src.zip/py4j/java_gateway.py:2257) finished in 0.851 s
2018-04-26 01:59:47 INFO  DAGScheduler:54 - Job 7 finished: call at /home/rjha/spark-2.3.0-bin-hadoop2.7/python/lib/py4j-0.10.6-src.zip/py4j/java_gateway.py:2257, took 0.858326 s
2018-04-26 01:59:47 INFO  JobScheduler:54 - Finished job streaming job 1524722384000 ms.0 from job set of time 1524722384000 ms
2018-04-26 01:59:47 INFO  JobScheduler:54 - Starting job streaming job 1524722384000 ms.1 from job set of time 1524722384000 ms
2018-04-26 01:59:47 INFO  MemoryStore:54 - Block input-0-1524722387000 stored as bytes in memory (estimated size 323.0 B, free 413.8 MB)
2018-04-26 01:59:47 INFO  BlockManagerInfo:54 - Added input-0-1524722387000 in memory on 10.0.2.15:33681 (size: 323.0 B, free: 413.9 MB)
2018-04-26 01:59:47 WARN  RandomBlockReplicationPolicy:66 - Expecting 1 replicas with only 0 peer/s.
2018-04-26 01:59:47 WARN  BlockManager:66 - Block input-0-1524722387000 replicated to only 0 peer(s) instead of 1 peers
2018-04-26 01:59:47 INFO  BlockGenerator:54 - Pushed block input-0-1524722387000
2018-04-26 01:59:47 INFO  JobScheduler:54 - Added jobs for time 1524722387000 ms
2018-04-26 01:59:47 INFO  SparkContext:54 - Starting job: call at /home/rjha/spark-2.3.0-bin-hadoop2.7/python/lib/py4j-0.10.6-src.zip/py4j/java_gateway.py:2257
2018-04-26 01:59:47 INFO  DAGScheduler:54 - Got job 8 (call at /home/rjha/spark-2.3.0-bin-hadoop2.7/python/lib/py4j-0.10.6-src.zip/py4j/java_gateway.py:2257) with 5 output partitions
2018-04-26 01:59:47 INFO  DAGScheduler:54 - Final stage: ResultStage 6 (call at /home/rjha/spark-2.3.0-bin-hadoop2.7/python/lib/py4j-0.10.6-src.zip/py4j/java_gateway.py:2257)
2018-04-26 01:59:47 INFO  DAGScheduler:54 - Parents of final stage: List()
2018-04-26 01:59:47 INFO  DAGScheduler:54 - Missing parents: List()
2018-04-26 01:59:47 INFO  DAGScheduler:54 - Submitting ResultStage 6 (PythonRDD[22] at call at /home/rjha/spark-2.3.0-bin-hadoop2.7/python/lib/py4j-0.10.6-src.zip/py4j/java_gateway.py:2257), which has no missing parents
2018-04-26 01:59:47 INFO  MemoryStore:54 - Block broadcast_6 stored as values in memory (estimated size 7.1 KB, free 413.8 MB)
2018-04-26 01:59:47 INFO  MemoryStore:54 - Block broadcast_6_piece0 stored as bytes in memory (estimated size 4.1 KB, free 413.8 MB)
2018-04-26 01:59:47 INFO  BlockManagerInfo:54 - Added broadcast_6_piece0 in memory on 10.0.2.15:33681 (size: 4.1 KB, free: 413.9 MB)
2018-04-26 01:59:47 INFO  SparkContext:54 - Created broadcast 6 from broadcast at DAGScheduler.scala:1039
2018-04-26 01:59:47 INFO  DAGScheduler:54 - Submitting 5 missing tasks from ResultStage 6 (PythonRDD[22] at call at /home/rjha/spark-2.3.0-bin-hadoop2.7/python/lib/py4j-0.10.6-src.zip/py4j/java_gateway.py:2257) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4))
2018-04-26 01:59:47 INFO  TaskSchedulerImpl:54 - Adding task set 6.0 with 5 tasks
2018-04-26 01:59:47 INFO  TaskSetManager:54 - Starting task 0.0 in stage 6.0 (TID 18, localhost, executor driver, partition 0, ANY, 7772 bytes)
2018-04-26 01:59:47 INFO  Executor:54 - Running task 0.0 in stage 6.0 (TID 18)
2018-04-26 01:59:47 INFO  BlockManager:54 - Found block input-0-1524722382800 locally
2018-04-26 01:59:47 INFO  PythonRunner:54 - Times: total = 53, boot = -94, init = 146, finish = 1
2018-04-26 01:59:47 INFO  PythonRunner:54 - Times: total = 43, boot = -91, init = 134, finish = 0
2018-04-26 01:59:47 INFO  MemoryStore:54 - Block input-0-1524722387200 stored as bytes in memory (estimated size 381.0 B, free 413.8 MB)
2018-04-26 01:59:47 INFO  BlockManagerInfo:54 - Added input-0-1524722387200 in memory on 10.0.2.15:33681 (size: 381.0 B, free: 413.9 MB)
2018-04-26 01:59:47 WARN  RandomBlockReplicationPolicy:66 - Expecting 1 replicas with only 0 peer/s.
2018-04-26 01:59:47 WARN  BlockManager:66 - Block input-0-1524722387200 replicated to only 0 peer(s) instead of 1 peers
2018-04-26 01:59:47 INFO  Executor:54 - Finished task 0.0 in stage 6.0 (TID 18). 1267 bytes result sent to driver
2018-04-26 01:59:47 INFO  TaskSetManager:54 - Starting task 1.0 in stage 6.0 (TID 19, localhost, executor driver, partition 1, ANY, 7772 bytes)
2018-04-26 01:59:47 INFO  TaskSetManager:54 - Finished task 0.0 in stage 6.0 (TID 18) in 80 ms on localhost (executor driver) (1/5)
2018-04-26 01:59:47 INFO  Executor:54 - Running task 1.0 in stage 6.0 (TID 19)
2018-04-26 01:59:47 INFO  BlockManager:54 - Found block input-0-1524722383000 locally
2018-04-26 01:59:47 INFO  BlockGenerator:54 - Pushed block input-0-1524722387200
2018-04-26 01:59:47 INFO  PythonRunner:54 - Times: total = 53, boot = 16, init = 37, finish = 0
2018-04-26 01:59:47 INFO  PythonRunner:54 - Times: total = 45, boot = 5, init = 40, finish = 0
2018-04-26 01:59:47 INFO  Executor:54 - Finished task 1.0 in stage 6.0 (TID 19). 1267 bytes result sent to driver
2018-04-26 01:59:47 INFO  TaskSetManager:54 - Starting task 2.0 in stage 6.0 (TID 20, localhost, executor driver, partition 2, ANY, 7772 bytes)
2018-04-26 01:59:47 INFO  Executor:54 - Running task 2.0 in stage 6.0 (TID 20)
2018-04-26 01:59:47 INFO  TaskSetManager:54 - Finished task 1.0 in stage 6.0 (TID 19) in 78 ms on localhost (executor driver) (2/5)
2018-04-26 01:59:47 INFO  BlockManager:54 - Found block input-0-1524722383200 locally
2018-04-26 01:59:47 INFO  PythonRunner:54 - Times: total = 50, boot = 4, init = 46, finish = 0
2018-04-26 01:59:47 INFO  PythonRunner:54 - Times: total = 50, boot = -2, init = 52, finish = 0
2018-04-26 01:59:47 INFO  Executor:54 - Finished task 2.0 in stage 6.0 (TID 20). 1267 bytes result sent to driver
2018-04-26 01:59:47 INFO  TaskSetManager:54 - Starting task 3.0 in stage 6.0 (TID 21, localhost, executor driver, partition 3, ANY, 7772 bytes)
2018-04-26 01:59:47 INFO  TaskSetManager:54 - Finished task 2.0 in stage 6.0 (TID 20) in 96 ms on localhost (executor driver) (3/5)
2018-04-26 01:59:47 INFO  Executor:54 - Running task 3.0 in stage 6.0 (TID 21)
2018-04-26 01:59:47 INFO  BlockManager:54 - Found block input-0-1524722383400 locally
2018-04-26 01:59:47 INFO  MemoryStore:54 - Block input-0-1524722387400 stored as bytes in memory (estimated size 423.0 B, free 413.8 MB)
2018-04-26 01:59:47 INFO  BlockManagerInfo:54 - Added input-0-1524722387400 in memory on 10.0.2.15:33681 (size: 423.0 B, free: 413.9 MB)
2018-04-26 01:59:47 WARN  RandomBlockReplicationPolicy:66 - Expecting 1 replicas with only 0 peer/s.
2018-04-26 01:59:47 WARN  BlockManager:66 - Block input-0-1524722387400 replicated to only 0 peer(s) instead of 1 peers
2018-04-26 01:59:47 INFO  BlockGenerator:54 - Pushed block input-0-1524722387400
2018-04-26 01:59:47 INFO  PythonRunner:54 - Times: total = 104, boot = 7, init = 97, finish = 0
2018-04-26 01:59:47 INFO  PythonRunner:54 - Times: total = 59, boot = 21, init = 38, finish = 0
2018-04-26 01:59:47 INFO  Executor:54 - Finished task 3.0 in stage 6.0 (TID 21). 1267 bytes result sent to driver
2018-04-26 01:59:47 INFO  TaskSetManager:54 - Starting task 4.0 in stage 6.0 (TID 22, localhost, executor driver, partition 4, ANY, 7772 bytes)
2018-04-26 01:59:47 INFO  Executor:54 - Running task 4.0 in stage 6.0 (TID 22)
2018-04-26 01:59:47 INFO  TaskSetManager:54 - Finished task 3.0 in stage 6.0 (TID 21) in 128 ms on localhost (executor driver) (4/5)
2018-04-26 01:59:47 INFO  BlockManager:54 - Found block input-0-1524722383600 locally
2018-04-26 01:59:47 INFO  PythonRunner:54 - Times: total = 47, boot = 21, init = 26, finish = 0
2018-04-26 01:59:47 INFO  PythonRunner:54 - Times: total = 48, boot = 6, init = 42, finish = 0
2018-04-26 01:59:47 INFO  Executor:54 - Finished task 4.0 in stage 6.0 (TID 22). 1267 bytes result sent to driver
2018-04-26 01:59:47 INFO  TaskSetManager:54 - Finished task 4.0 in stage 6.0 (TID 22) in 80 ms on localhost (executor driver) (5/5)
2018-04-26 01:59:47 INFO  TaskSchedulerImpl:54 - Removed TaskSet 6.0, whose tasks have all completed, from pool 
2018-04-26 01:59:47 INFO  DAGScheduler:54 - ResultStage 6 (call at /home/rjha/spark-2.3.0-bin-hadoop2.7/python/lib/py4j-0.10.6-src.zip/py4j/java_gateway.py:2257) finished in 0.495 s
2018-04-26 01:59:47 INFO  DAGScheduler:54 - Job 8 finished: call at /home/rjha/spark-2.3.0-bin-hadoop2.7/python/lib/py4j-0.10.6-src.zip/py4j/java_gateway.py:2257, took 0.500933 s
2018-04-26 01:59:47 INFO  MemoryStore:54 - Block input-0-1524722387600 stored as bytes in memory (estimated size 342.0 B, free 413.8 MB)
2018-04-26 01:59:47 INFO  BlockManagerInfo:54 - Added input-0-1524722387600 in memory on 10.0.2.15:33681 (size: 342.0 B, free: 413.9 MB)
2018-04-26 01:59:47 WARN  RandomBlockReplicationPolicy:66 - Expecting 1 replicas with only 0 peer/s.
2018-04-26 01:59:47 WARN  BlockManager:66 - Block input-0-1524722387600 replicated to only 0 peer(s) instead of 1 peers
2018-04-26 01:59:47 INFO  BlockGenerator:54 - Pushed block input-0-1524722387600
2018-04-26 01:59:47 INFO  JobScheduler:54 - Finished job streaming job 1524722384000 ms.1 from job set of time 1524722384000 ms
2018-04-26 01:59:47 INFO  JobScheduler:54 - Total delay: 3.808 s for time 1524722384000 ms (execution: 1.648 s)
2018-04-26 01:59:47 INFO  JobScheduler:54 - Starting job streaming job 1524722385000 ms.0 from job set of time 1524722385000 ms
2018-04-26 01:59:47 INFO  PythonRDD:54 - Removing RDD 9 from persistence list
2018-04-26 01:59:47 INFO  BlockManager:54 - Removing RDD 9
2018-04-26 01:59:47 INFO  BlockRDD:54 - Removing RDD 8 from persistence list
2018-04-26 01:59:47 INFO  BlockManager:54 - Removing RDD 8
2018-04-26 01:59:47 INFO  SocketInputDStream:54 - Removing blocks of RDD BlockRDD[8] at socketTextStream at NativeMethodAccessorImpl.java:0 of time 1524722384000 ms
2018-04-26 01:59:47 INFO  BlockManagerInfo:54 - Removed input-0-1524722381800 on 10.0.2.15:33681 in memory (size: 371.0 B, free: 413.9 MB)
2018-04-26 01:59:47 INFO  BlockManagerInfo:54 - Removed input-0-1524722382000 on 10.0.2.15:33681 in memory (size: 389.0 B, free: 413.9 MB)
2018-04-26 01:59:47 INFO  BlockManagerInfo:54 - Removed input-0-1524722382200 on 10.0.2.15:33681 in memory (size: 386.0 B, free: 413.9 MB)
2018-04-26 01:59:47 INFO  BlockManagerInfo:54 - Removed input-0-1524722382400 on 10.0.2.15:33681 in memory (size: 387.0 B, free: 413.9 MB)
2018-04-26 01:59:47 INFO  ReceivedBlockTracker:54 - Deleting batches: 1524722382000 ms
2018-04-26 01:59:47 INFO  InputInfoTracker:54 - remove old batch metadata: 1524722382000 ms
2018-04-26 01:59:47 INFO  ContextCleaner:54 - Cleaned accumulator 160
2018-04-26 01:59:47 INFO  ContextCleaner:54 - Cleaned accumulator 148
2018-04-26 01:59:47 INFO  ContextCleaner:54 - Cleaned accumulator 173
2018-04-26 01:59:47 INFO  ContextCleaner:54 - Cleaned accumulator 131
2018-04-26 01:59:47 INFO  ContextCleaner:54 - Cleaned accumulator 132
2018-04-26 01:59:47 INFO  ContextCleaner:54 - Cleaned accumulator 130
2018-04-26 01:59:47 INFO  ContextCleaner:54 - Cleaned accumulator 144
2018-04-26 01:59:47 INFO  ContextCleaner:54 - Cleaned accumulator 126
2018-04-26 01:59:47 INFO  ContextCleaner:54 - Cleaned accumulator 161
2018-04-26 01:59:47 INFO  ContextCleaner:54 - Cleaned accumulator 168
2018-04-26 01:59:47 INFO  ContextCleaner:54 - Cleaned accumulator 156
2018-04-26 01:59:47 INFO  ContextCleaner:54 - Cleaned accumulator 172
2018-04-26 01:59:47 INFO  ContextCleaner:54 - Cleaned accumulator 152
2018-04-26 01:59:47 INFO  ContextCleaner:54 - Cleaned accumulator 162
2018-04-26 01:59:47 INFO  ContextCleaner:54 - Cleaned accumulator 138
2018-04-26 01:59:47 INFO  BlockManagerInfo:54 - Removed input-0-1524722382600 on 10.0.2.15:33681 in memory (size: 369.0 B, free: 413.9 MB)
2018-04-26 01:59:47 INFO  BlockManagerInfo:54 - Removed broadcast_5_piece0 on 10.0.2.15:33681 in memory (size: 4.4 KB, free: 413.9 MB)
2018-04-26 01:59:47 INFO  ContextCleaner:54 - Cleaned accumulator 154
2018-04-26 01:59:47 INFO  ContextCleaner:54 - Cleaned accumulator 129
2018-04-26 01:59:47 INFO  ContextCleaner:54 - Cleaned accumulator 175
2018-04-26 01:59:47 INFO  ContextCleaner:54 - Cleaned accumulator 145
2018-04-26 01:59:47 INFO  ContextCleaner:54 - Cleaned accumulator 166
2018-04-26 01:59:47 INFO  ContextCleaner:54 - Cleaned accumulator 146
2018-04-26 01:59:47 INFO  ContextCleaner:54 - Cleaned accumulator 141
2018-04-26 01:59:47 INFO  ContextCleaner:54 - Cleaned accumulator 127
2018-04-26 01:59:47 INFO  ContextCleaner:54 - Cleaned accumulator 147
2018-04-26 01:59:47 INFO  ContextCleaner:54 - Cleaned accumulator 164
2018-04-26 01:59:47 INFO  ContextCleaner:54 - Cleaned accumulator 151
2018-04-26 01:59:47 INFO  ContextCleaner:54 - Cleaned accumulator 142
2018-04-26 01:59:47 INFO  ContextCleaner:54 - Cleaned accumulator 149
2018-04-26 01:59:47 INFO  ContextCleaner:54 - Cleaned accumulator 139
2018-04-26 01:59:47 INFO  ContextCleaner:54 - Cleaned accumulator 167
2018-04-26 01:59:47 INFO  ContextCleaner:54 - Cleaned accumulator 134
2018-04-26 01:59:47 INFO  ContextCleaner:54 - Cleaned accumulator 136
2018-04-26 01:59:47 INFO  ContextCleaner:54 - Cleaned accumulator 170
2018-04-26 01:59:47 INFO  ContextCleaner:54 - Cleaned accumulator 155
2018-04-26 01:59:47 INFO  ContextCleaner:54 - Cleaned accumulator 150
2018-04-26 01:59:47 INFO  ContextCleaner:54 - Cleaned accumulator 153
2018-04-26 01:59:47 INFO  ContextCleaner:54 - Cleaned accumulator 140
2018-04-26 01:59:47 INFO  ContextCleaner:54 - Cleaned accumulator 135
2018-04-26 01:59:47 INFO  ContextCleaner:54 - Cleaned accumulator 137
2018-04-26 01:59:47 INFO  ContextCleaner:54 - Cleaned accumulator 143
2018-04-26 01:59:47 INFO  ContextCleaner:54 - Cleaned accumulator 128
2018-04-26 01:59:47 INFO  ContextCleaner:54 - Cleaned accumulator 133
2018-04-26 01:59:47 INFO  ContextCleaner:54 - Cleaned accumulator 159
2018-04-26 01:59:47 INFO  ContextCleaner:54 - Cleaned accumulator 165
2018-04-26 01:59:47 INFO  ContextCleaner:54 - Cleaned accumulator 158
2018-04-26 01:59:47 INFO  BlockManagerInfo:54 - Removed broadcast_6_piece0 on 10.0.2.15:33681 in memory (size: 4.1 KB, free: 413.9 MB)
2018-04-26 01:59:47 INFO  ContextCleaner:54 - Cleaned accumulator 163
2018-04-26 01:59:47 INFO  ContextCleaner:54 - Cleaned accumulator 169
2018-04-26 01:59:47 INFO  ContextCleaner:54 - Cleaned accumulator 157
2018-04-26 01:59:47 INFO  ContextCleaner:54 - Cleaned accumulator 171
2018-04-26 01:59:47 INFO  ContextCleaner:54 - Cleaned accumulator 174
2018-04-26 01:59:47 INFO  SparkContext:54 - Starting job: call at /home/rjha/spark-2.3.0-bin-hadoop2.7/python/lib/py4j-0.10.6-src.zip/py4j/java_gateway.py:2257
2018-04-26 01:59:47 INFO  DAGScheduler:54 - Got job 9 (call at /home/rjha/spark-2.3.0-bin-hadoop2.7/python/lib/py4j-0.10.6-src.zip/py4j/java_gateway.py:2257) with 5 output partitions
2018-04-26 01:59:47 INFO  DAGScheduler:54 - Final stage: ResultStage 7 (call at /home/rjha/spark-2.3.0-bin-hadoop2.7/python/lib/py4j-0.10.6-src.zip/py4j/java_gateway.py:2257)
2018-04-26 01:59:47 INFO  DAGScheduler:54 - Parents of final stage: List()
2018-04-26 01:59:47 INFO  DAGScheduler:54 - Missing parents: List()
2018-04-26 01:59:47 INFO  DAGScheduler:54 - Submitting ResultStage 7 (PythonRDD[23] at call at /home/rjha/spark-2.3.0-bin-hadoop2.7/python/lib/py4j-0.10.6-src.zip/py4j/java_gateway.py:2257), which has no missing parents
2018-04-26 01:59:47 INFO  MemoryStore:54 - Block broadcast_7 stored as values in memory (estimated size 7.5 KB, free 413.8 MB)
2018-04-26 01:59:47 INFO  MemoryStore:54 - Block broadcast_7_piece0 stored as bytes in memory (estimated size 4.4 KB, free 413.8 MB)
2018-04-26 01:59:47 INFO  BlockManagerInfo:54 - Added broadcast_7_piece0 in memory on 10.0.2.15:33681 (size: 4.4 KB, free: 413.9 MB)
2018-04-26 01:59:47 INFO  SparkContext:54 - Created broadcast 7 from broadcast at DAGScheduler.scala:1039
2018-04-26 01:59:47 INFO  DAGScheduler:54 - Submitting 5 missing tasks from ResultStage 7 (PythonRDD[23] at call at /home/rjha/spark-2.3.0-bin-hadoop2.7/python/lib/py4j-0.10.6-src.zip/py4j/java_gateway.py:2257) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4))
2018-04-26 01:59:47 INFO  TaskSchedulerImpl:54 - Adding task set 7.0 with 5 tasks
2018-04-26 01:59:47 INFO  TaskSetManager:54 - Starting task 0.0 in stage 7.0 (TID 23, localhost, executor driver, partition 0, ANY, 7772 bytes)
2018-04-26 01:59:47 INFO  Executor:54 - Running task 0.0 in stage 7.0 (TID 23)
2018-04-26 01:59:47 INFO  BlockManager:54 - Found block input-0-1524722383800 locally
2018-04-26 01:59:47 INFO  PythonRunner:54 - Times: total = 48, boot = -159, init = 207, finish = 0
2018-04-26 01:59:47 INFO  PythonRunner:54 - Times: total = 49, boot = -145, init = 190, finish = 4
2018-04-26 01:59:47 INFO  Executor:54 - Finished task 0.0 in stage 7.0 (TID 23). 1267 bytes result sent to driver
2018-04-26 01:59:47 INFO  TaskSetManager:54 - Starting task 1.0 in stage 7.0 (TID 24, localhost, executor driver, partition 1, ANY, 7772 bytes)
2018-04-26 01:59:47 INFO  Executor:54 - Running task 1.0 in stage 7.0 (TID 24)
2018-04-26 01:59:47 INFO  TaskSetManager:54 - Finished task 0.0 in stage 7.0 (TID 23) in 64 ms on localhost (executor driver) (1/5)
2018-04-26 01:59:48 INFO  MemoryStore:54 - Block input-0-1524722387800 stored as bytes in memory (estimated size 408.0 B, free 413.8 MB)
2018-04-26 01:59:48 INFO  BlockManagerInfo:54 - Added input-0-1524722387800 in memory on 10.0.2.15:33681 (size: 408.0 B, free: 413.9 MB)
2018-04-26 01:59:48 WARN  RandomBlockReplicationPolicy:66 - Expecting 1 replicas with only 0 peer/s.
2018-04-26 01:59:48 WARN  BlockManager:66 - Block input-0-1524722387800 replicated to only 0 peer(s) instead of 1 peers
2018-04-26 01:59:48 INFO  BlockManager:54 - Found block input-0-1524722384000 locally
2018-04-26 01:59:48 INFO  BlockGenerator:54 - Pushed block input-0-1524722387800
2018-04-26 01:59:48 INFO  JobScheduler:54 - Added jobs for time 1524722388000 ms
2018-04-26 01:59:48 INFO  PythonRunner:54 - Times: total = 78, boot = 77, init = 1, finish = 0
2018-04-26 01:59:48 INFO  PythonRunner:54 - Times: total = 96, boot = -6, init = 98, finish = 4
2018-04-26 01:59:48 INFO  Executor:54 - Finished task 1.0 in stage 7.0 (TID 24). 1267 bytes result sent to driver
2018-04-26 01:59:48 INFO  TaskSetManager:54 - Starting task 2.0 in stage 7.0 (TID 25, localhost, executor driver, partition 2, ANY, 7772 bytes)
2018-04-26 01:59:48 INFO  TaskSetManager:54 - Finished task 1.0 in stage 7.0 (TID 24) in 181 ms on localhost (executor driver) (2/5)
2018-04-26 01:59:48 INFO  Executor:54 - Running task 2.0 in stage 7.0 (TID 25)
2018-04-26 01:59:48 INFO  BlockManager:54 - Found block input-0-1524722384200 locally
2018-04-26 01:59:48 INFO  MemoryStore:54 - Block input-0-1524722388000 stored as bytes in memory (estimated size 323.0 B, free 413.8 MB)
2018-04-26 01:59:48 INFO  BlockManagerInfo:54 - Added input-0-1524722388000 in memory on 10.0.2.15:33681 (size: 323.0 B, free: 413.9 MB)
2018-04-26 01:59:48 WARN  RandomBlockReplicationPolicy:66 - Expecting 1 replicas with only 0 peer/s.
2018-04-26 01:59:48 WARN  BlockManager:66 - Block input-0-1524722388000 replicated to only 0 peer(s) instead of 1 peers
2018-04-26 01:59:48 INFO  BlockGenerator:54 - Pushed block input-0-1524722388000
2018-04-26 01:59:48 INFO  PythonRunner:54 - Times: total = 94, boot = 88, init = 5, finish = 1
2018-04-26 01:59:48 INFO  PythonRunner:54 - Times: total = 122, boot = 17, init = 40, finish = 65
2018-04-26 01:59:48 INFO  Executor:54 - Finished task 2.0 in stage 7.0 (TID 25). 1267 bytes result sent to driver
2018-04-26 01:59:48 INFO  TaskSetManager:54 - Starting task 3.0 in stage 7.0 (TID 26, localhost, executor driver, partition 3, ANY, 7772 bytes)
2018-04-26 01:59:48 INFO  Executor:54 - Running task 3.0 in stage 7.0 (TID 26)
2018-04-26 01:59:48 INFO  TaskSetManager:54 - Finished task 2.0 in stage 7.0 (TID 25) in 211 ms on localhost (executor driver) (3/5)
2018-04-26 01:59:48 INFO  BlockManager:54 - Found block input-0-1524722384400 locally
2018-04-26 01:59:48 INFO  MemoryStore:54 - Block input-0-1524722388200 stored as bytes in memory (estimated size 410.0 B, free 413.8 MB)
2018-04-26 01:59:48 INFO  BlockManagerInfo:54 - Added input-0-1524722388200 in memory on 10.0.2.15:33681 (size: 410.0 B, free: 413.9 MB)
2018-04-26 01:59:48 WARN  RandomBlockReplicationPolicy:66 - Expecting 1 replicas with only 0 peer/s.
2018-04-26 01:59:48 WARN  BlockManager:66 - Block input-0-1524722388200 replicated to only 0 peer(s) instead of 1 peers
2018-04-26 01:59:48 INFO  BlockGenerator:54 - Pushed block input-0-1524722388200
2018-04-26 01:59:48 INFO  PythonRunner:54 - Times: total = 45, boot = -57, init = 101, finish = 1
2018-04-26 01:59:48 INFO  PythonRunner:54 - Times: total = 64, boot = 32, init = 25, finish = 7
2018-04-26 01:59:48 INFO  Executor:54 - Finished task 3.0 in stage 7.0 (TID 26). 1267 bytes result sent to driver
2018-04-26 01:59:48 INFO  TaskSetManager:54 - Starting task 4.0 in stage 7.0 (TID 27, localhost, executor driver, partition 4, ANY, 7772 bytes)
2018-04-26 01:59:48 INFO  Executor:54 - Running task 4.0 in stage 7.0 (TID 27)
2018-04-26 01:59:48 INFO  TaskSetManager:54 - Finished task 3.0 in stage 7.0 (TID 26) in 206 ms on localhost (executor driver) (4/5)
2018-04-26 01:59:48 INFO  BlockManager:54 - Found block input-0-1524722384600 locally
2018-04-26 01:59:48 INFO  MemoryStore:54 - Block input-0-1524722388400 stored as bytes in memory (estimated size 324.0 B, free 413.8 MB)
2018-04-26 01:59:48 INFO  BlockManagerInfo:54 - Added input-0-1524722388400 in memory on 10.0.2.15:33681 (size: 324.0 B, free: 413.9 MB)
2018-04-26 01:59:48 WARN  RandomBlockReplicationPolicy:66 - Expecting 1 replicas with only 0 peer/s.
2018-04-26 01:59:48 WARN  BlockManager:66 - Block input-0-1524722388400 replicated to only 0 peer(s) instead of 1 peers
2018-04-26 01:59:48 INFO  BlockGenerator:54 - Pushed block input-0-1524722388400
2018-04-26 01:59:48 INFO  PythonRunner:54 - Times: total = 53, boot = 20, init = 33, finish = 0
2018-04-26 01:59:48 INFO  PythonRunner:54 - Times: total = 166, boot = 161, init = 0, finish = 5
2018-04-26 01:59:48 INFO  Executor:54 - Finished task 4.0 in stage 7.0 (TID 27). 1267 bytes result sent to driver
2018-04-26 01:59:48 INFO  TaskSetManager:54 - Finished task 4.0 in stage 7.0 (TID 27) in 248 ms on localhost (executor driver) (5/5)
2018-04-26 01:59:48 INFO  TaskSchedulerImpl:54 - Removed TaskSet 7.0, whose tasks have all completed, from pool 
2018-04-26 01:59:48 INFO  MemoryStore:54 - Block input-0-1524722388600 stored as bytes in memory (estimated size 153.0 B, free 413.8 MB)
2018-04-26 01:59:48 INFO  BlockManagerInfo:54 - Added input-0-1524722388600 in memory on 10.0.2.15:33681 (size: 153.0 B, free: 413.9 MB)
2018-04-26 01:59:48 WARN  RandomBlockReplicationPolicy:66 - Expecting 1 replicas with only 0 peer/s.
2018-04-26 01:59:48 WARN  BlockManager:66 - Block input-0-1524722388600 replicated to only 0 peer(s) instead of 1 peers
2018-04-26 01:59:48 INFO  BlockGenerator:54 - Pushed block input-0-1524722388600
2018-04-26 01:59:48 INFO  DAGScheduler:54 - ResultStage 7 (call at /home/rjha/spark-2.3.0-bin-hadoop2.7/python/lib/py4j-0.10.6-src.zip/py4j/java_gateway.py:2257) finished in 1.079 s
2018-04-26 01:59:48 INFO  DAGScheduler:54 - Job 9 finished: call at /home/rjha/spark-2.3.0-bin-hadoop2.7/python/lib/py4j-0.10.6-src.zip/py4j/java_gateway.py:2257, took 1.090126 s
2018-04-26 01:59:49 INFO  MemoryStore:54 - Block input-0-1524722388800 stored as bytes in memory (estimated size 296.0 B, free 413.8 MB)
2018-04-26 01:59:49 INFO  BlockManagerInfo:54 - Added input-0-1524722388800 in memory on 10.0.2.15:33681 (size: 296.0 B, free: 413.9 MB)
2018-04-26 01:59:49 WARN  RandomBlockReplicationPolicy:66 - Expecting 1 replicas with only 0 peer/s.
2018-04-26 01:59:49 WARN  BlockManager:66 - Block input-0-1524722388800 replicated to only 0 peer(s) instead of 1 peers
2018-04-26 01:59:49 INFO  BlockGenerator:54 - Pushed block input-0-1524722388800
2018-04-26 01:59:49 INFO  JobScheduler:54 - Finished job streaming job 1524722385000 ms.0 from job set of time 1524722385000 ms
2018-04-26 01:59:49 INFO  JobScheduler:54 - Starting job streaming job 1524722385000 ms.1 from job set of time 1524722385000 ms
2018-04-26 01:59:49 INFO  ContextCleaner:54 - Cleaned accumulator 176
2018-04-26 01:59:49 INFO  ContextCleaner:54 - Cleaned accumulator 194
2018-04-26 01:59:49 INFO  ContextCleaner:54 - Cleaned accumulator 185
2018-04-26 01:59:49 INFO  ContextCleaner:54 - Cleaned accumulator 199
2018-04-26 01:59:49 INFO  ContextCleaner:54 - Cleaned accumulator 183
2018-04-26 01:59:49 INFO  ContextCleaner:54 - Cleaned accumulator 181
2018-04-26 01:59:49 INFO  ContextCleaner:54 - Cleaned accumulator 192
2018-04-26 01:59:49 INFO  ContextCleaner:54 - Cleaned accumulator 197
2018-04-26 01:59:49 INFO  ContextCleaner:54 - Cleaned accumulator 177
2018-04-26 01:59:49 INFO  ContextCleaner:54 - Cleaned accumulator 178
2018-04-26 01:59:49 INFO  ContextCleaner:54 - Cleaned accumulator 188
2018-04-26 01:59:49 INFO  ContextCleaner:54 - Cleaned accumulator 187
2018-04-26 01:59:49 INFO  ContextCleaner:54 - Cleaned accumulator 184
2018-04-26 01:59:49 INFO  ContextCleaner:54 - Cleaned accumulator 180
2018-04-26 01:59:49 INFO  ContextCleaner:54 - Cleaned accumulator 186
2018-04-26 01:59:49 INFO  ContextCleaner:54 - Cleaned accumulator 193
2018-04-26 01:59:49 INFO  ContextCleaner:54 - Cleaned accumulator 191
2018-04-26 01:59:49 INFO  ContextCleaner:54 - Cleaned accumulator 195
2018-04-26 01:59:49 INFO  ContextCleaner:54 - Cleaned accumulator 182
2018-04-26 01:59:49 INFO  ContextCleaner:54 - Cleaned accumulator 190
2018-04-26 01:59:49 INFO  ContextCleaner:54 - Cleaned accumulator 200
2018-04-26 01:59:49 INFO  ContextCleaner:54 - Cleaned accumulator 198
2018-04-26 01:59:49 INFO  ContextCleaner:54 - Cleaned accumulator 179
2018-04-26 01:59:49 INFO  ContextCleaner:54 - Cleaned accumulator 189
2018-04-26 01:59:49 INFO  BlockManagerInfo:54 - Removed broadcast_7_piece0 on 10.0.2.15:33681 in memory (size: 4.4 KB, free: 413.9 MB)
2018-04-26 01:59:49 INFO  ContextCleaner:54 - Cleaned accumulator 196
2018-04-26 01:59:49 INFO  JobScheduler:54 - Added jobs for time 1524722389000 ms
2018-04-26 01:59:49 INFO  SparkContext:54 - Starting job: collect at PythonRDD.scala:153
2018-04-26 01:59:49 INFO  DAGScheduler:54 - Got job 10 (collect at PythonRDD.scala:153) with 5 output partitions
2018-04-26 01:59:49 INFO  DAGScheduler:54 - Final stage: ResultStage 8 (collect at PythonRDD.scala:153)
2018-04-26 01:59:49 INFO  DAGScheduler:54 - Parents of final stage: List()
2018-04-26 01:59:49 INFO  DAGScheduler:54 - Missing parents: List()
2018-04-26 01:59:49 INFO  DAGScheduler:54 - Submitting ResultStage 8 (PythonRDD[28] at RDD at PythonRDD.scala:48), which has no missing parents
2018-04-26 01:59:49 INFO  MemoryStore:54 - Block broadcast_8 stored as values in memory (estimated size 7.1 KB, free 413.8 MB)
2018-04-26 01:59:49 INFO  MemoryStore:54 - Block broadcast_8_piece0 stored as bytes in memory (estimated size 4.1 KB, free 413.8 MB)
2018-04-26 01:59:49 INFO  BlockManagerInfo:54 - Added broadcast_8_piece0 in memory on 10.0.2.15:33681 (size: 4.1 KB, free: 413.9 MB)
2018-04-26 01:59:49 INFO  SparkContext:54 - Created broadcast 8 from broadcast at DAGScheduler.scala:1039
2018-04-26 01:59:49 INFO  DAGScheduler:54 - Submitting 5 missing tasks from ResultStage 8 (PythonRDD[28] at RDD at PythonRDD.scala:48) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4))
2018-04-26 01:59:49 INFO  TaskSchedulerImpl:54 - Adding task set 8.0 with 5 tasks
2018-04-26 01:59:49 INFO  TaskSetManager:54 - Starting task 0.0 in stage 8.0 (TID 28, localhost, executor driver, partition 0, ANY, 7772 bytes)
2018-04-26 01:59:49 INFO  Executor:54 - Running task 0.0 in stage 8.0 (TID 28)
2018-04-26 01:59:49 INFO  BlockManager:54 - Found block input-0-1524722383800 locally
2018-04-26 01:59:49 INFO  PythonRunner:54 - Times: total = 46, boot = -364, init = 410, finish = 0
2018-04-26 01:59:49 INFO  PythonRunner:54 - Times: total = 42, boot = -250, init = 292, finish = 0
2018-04-26 01:59:49 INFO  Executor:54 - Finished task 0.0 in stage 8.0 (TID 28). 1310 bytes result sent to driver
2018-04-26 01:59:49 INFO  TaskSetManager:54 - Starting task 1.0 in stage 8.0 (TID 29, localhost, executor driver, partition 1, ANY, 7772 bytes)
2018-04-26 01:59:49 INFO  Executor:54 - Running task 1.0 in stage 8.0 (TID 29)
2018-04-26 01:59:49 INFO  TaskSetManager:54 - Finished task 0.0 in stage 8.0 (TID 28) in 59 ms on localhost (executor driver) (1/5)
2018-04-26 01:59:49 INFO  BlockManager:54 - Found block input-0-1524722384000 locally
2018-04-26 01:59:49 INFO  MemoryStore:54 - Block input-0-1524722389000 stored as bytes in memory (estimated size 407.0 B, free 413.8 MB)
2018-04-26 01:59:49 INFO  BlockManagerInfo:54 - Added input-0-1524722389000 in memory on 10.0.2.15:33681 (size: 407.0 B, free: 413.9 MB)
2018-04-26 01:59:49 WARN  RandomBlockReplicationPolicy:66 - Expecting 1 replicas with only 0 peer/s.
2018-04-26 01:59:49 WARN  BlockManager:66 - Block input-0-1524722389000 replicated to only 0 peer(s) instead of 1 peers
2018-04-26 01:59:49 INFO  BlockGenerator:54 - Pushed block input-0-1524722389000
2018-04-26 01:59:49 INFO  PythonRunner:54 - Times: total = 68, boot = 48, init = 20, finish = 0
2018-04-26 01:59:49 INFO  PythonRunner:54 - Times: total = 49, boot = 27, init = 22, finish = 0
2018-04-26 01:59:49 INFO  Executor:54 - Finished task 1.0 in stage 8.0 (TID 29). 1267 bytes result sent to driver
2018-04-26 01:59:49 INFO  TaskSetManager:54 - Starting task 2.0 in stage 8.0 (TID 30, localhost, executor driver, partition 2, ANY, 7772 bytes)
2018-04-26 01:59:49 INFO  TaskSetManager:54 - Finished task 1.0 in stage 8.0 (TID 29) in 113 ms on localhost (executor driver) (2/5)
2018-04-26 01:59:49 INFO  Executor:54 - Running task 2.0 in stage 8.0 (TID 30)
2018-04-26 01:59:49 INFO  BlockManager:54 - Found block input-0-1524722384200 locally
2018-04-26 01:59:49 INFO  PythonRunner:54 - Times: total = 49, boot = 12, init = 36, finish = 1
2018-04-26 01:59:49 INFO  PythonRunner:54 - Times: total = 51, boot = 16, init = 35, finish = 0
2018-04-26 01:59:49 INFO  Executor:54 - Finished task 2.0 in stage 8.0 (TID 30). 1267 bytes result sent to driver
2018-04-26 01:59:49 INFO  TaskSetManager:54 - Starting task 3.0 in stage 8.0 (TID 31, localhost, executor driver, partition 3, ANY, 7772 bytes)
2018-04-26 01:59:49 INFO  TaskSetManager:54 - Finished task 2.0 in stage 8.0 (TID 30) in 77 ms on localhost (executor driver) (3/5)
2018-04-26 01:59:49 INFO  Executor:54 - Running task 3.0 in stage 8.0 (TID 31)
2018-04-26 01:59:49 INFO  BlockManager:54 - Found block input-0-1524722384400 locally
2018-04-26 01:59:49 INFO  MemoryStore:54 - Block input-0-1524722389200 stored as bytes in memory (estimated size 412.0 B, free 413.8 MB)
2018-04-26 01:59:49 INFO  BlockManagerInfo:54 - Added input-0-1524722389200 in memory on 10.0.2.15:33681 (size: 412.0 B, free: 413.9 MB)
2018-04-26 01:59:49 WARN  RandomBlockReplicationPolicy:66 - Expecting 1 replicas with only 0 peer/s.
2018-04-26 01:59:49 WARN  BlockManager:66 - Block input-0-1524722389200 replicated to only 0 peer(s) instead of 1 peers
2018-04-26 01:59:49 INFO  BlockGenerator:54 - Pushed block input-0-1524722389200
2018-04-26 01:59:49 INFO  PythonRunner:54 - Times: total = 56, boot = 17, init = 39, finish = 0
2018-04-26 01:59:49 INFO  PythonRunner:54 - Times: total = 46, boot = 19, init = 27, finish = 0
2018-04-26 01:59:49 INFO  Executor:54 - Finished task 3.0 in stage 8.0 (TID 31). 1267 bytes result sent to driver
2018-04-26 01:59:49 INFO  TaskSetManager:54 - Starting task 4.0 in stage 8.0 (TID 32, localhost, executor driver, partition 4, ANY, 7772 bytes)
2018-04-26 01:59:49 INFO  TaskSetManager:54 - Finished task 3.0 in stage 8.0 (TID 31) in 76 ms on localhost (executor driver) (4/5)
2018-04-26 01:59:49 INFO  Executor:54 - Running task 4.0 in stage 8.0 (TID 32)
2018-04-26 01:59:49 INFO  BlockManager:54 - Found block input-0-1524722384600 locally
2018-04-26 01:59:49 INFO  PythonRunner:54 - Times: total = 55, boot = 18, init = 36, finish = 1
2018-04-26 01:59:49 INFO  PythonRunner:54 - Times: total = 49, boot = 2, init = 47, finish = 0
2018-04-26 01:59:49 INFO  Executor:54 - Finished task 4.0 in stage 8.0 (TID 32). 1267 bytes result sent to driver
2018-04-26 01:59:49 INFO  TaskSetManager:54 - Finished task 4.0 in stage 8.0 (TID 32) in 75 ms on localhost (executor driver) (5/5)
2018-04-26 01:59:49 INFO  TaskSchedulerImpl:54 - Removed TaskSet 8.0, whose tasks have all completed, from pool 
2018-04-26 01:59:49 INFO  DAGScheduler:54 - ResultStage 8 (collect at PythonRDD.scala:153) finished in 0.419 s
2018-04-26 01:59:49 INFO  DAGScheduler:54 - Job 10 finished: collect at PythonRDD.scala:153, took 0.424502 s
2018-04-26 01:59:49 INFO  JobScheduler:54 - Finished job streaming job 1524722385000 ms.1 from job set of time 1524722385000 ms
2018-04-26 01:59:49 INFO  JobScheduler:54 - Total delay: 4.538 s for time 1524722385000 ms (execution: 1.730 s)
2018-04-26 01:59:49 INFO  JobScheduler:54 - Starting job streaming job 1524722386000 ms.0 from job set of time 1524722386000 ms
2018-04-26 01:59:49 INFO  PythonRDD:54 - Removing RDD 13 from persistence list
2018-04-26 01:59:49 INFO  BlockManager:54 - Removing RDD 13
2018-04-26 01:59:49 INFO  BlockRDD:54 - Removing RDD 12 from persistence list
2018-04-26 01:59:49 INFO  BlockManager:54 - Removing RDD 12
2018-04-26 01:59:49 INFO  SocketInputDStream:54 - Removing blocks of RDD BlockRDD[12] at socketTextStream at NativeMethodAccessorImpl.java:0 of time 1524722385000 ms
2018-04-26 01:59:49 INFO  BlockManagerInfo:54 - Removed input-0-1524722382800 on 10.0.2.15:33681 in memory (size: 406.0 B, free: 413.9 MB)
2018-04-26 01:59:49 INFO  BlockManagerInfo:54 - Removed input-0-1524722383000 on 10.0.2.15:33681 in memory (size: 302.0 B, free: 413.9 MB)
2018-04-26 01:59:49 INFO  BlockManagerInfo:54 - Removed input-0-1524722383200 on 10.0.2.15:33681 in memory (size: 407.0 B, free: 413.9 MB)
2018-04-26 01:59:49 INFO  BlockManagerInfo:54 - Removed input-0-1524722383400 on 10.0.2.15:33681 in memory (size: 345.0 B, free: 413.9 MB)
2018-04-26 01:59:49 INFO  ReceivedBlockTracker:54 - Deleting batches: 1524722383000 ms
2018-04-26 01:59:49 INFO  InputInfoTracker:54 - remove old batch metadata: 1524722383000 ms
2018-04-26 01:59:49 INFO  BlockManagerInfo:54 - Removed input-0-1524722383600 on 10.0.2.15:33681 in memory (size: 323.0 B, free: 413.9 MB)
2018-04-26 01:59:49 INFO  MemoryStore:54 - Block input-0-1524722389400 stored as bytes in memory (estimated size 421.0 B, free 413.8 MB)
2018-04-26 01:59:49 INFO  BlockManagerInfo:54 - Added input-0-1524722389400 in memory on 10.0.2.15:33681 (size: 421.0 B, free: 413.9 MB)
2018-04-26 01:59:49 WARN  RandomBlockReplicationPolicy:66 - Expecting 1 replicas with only 0 peer/s.
2018-04-26 01:59:49 WARN  BlockManager:66 - Block input-0-1524722389400 replicated to only 0 peer(s) instead of 1 peers
2018-04-26 01:59:49 INFO  BlockGenerator:54 - Pushed block input-0-1524722389400
2018-04-26 01:59:49 INFO  SparkContext:54 - Starting job: call at /home/rjha/spark-2.3.0-bin-hadoop2.7/python/lib/py4j-0.10.6-src.zip/py4j/java_gateway.py:2257
2018-04-26 01:59:49 INFO  DAGScheduler:54 - Got job 11 (call at /home/rjha/spark-2.3.0-bin-hadoop2.7/python/lib/py4j-0.10.6-src.zip/py4j/java_gateway.py:2257) with 5 output partitions
2018-04-26 01:59:49 INFO  DAGScheduler:54 - Final stage: ResultStage 9 (call at /home/rjha/spark-2.3.0-bin-hadoop2.7/python/lib/py4j-0.10.6-src.zip/py4j/java_gateway.py:2257)
2018-04-26 01:59:49 INFO  DAGScheduler:54 - Parents of final stage: List()
2018-04-26 01:59:49 INFO  DAGScheduler:54 - Missing parents: List()
2018-04-26 01:59:49 INFO  DAGScheduler:54 - Submitting ResultStage 9 (PythonRDD[29] at call at /home/rjha/spark-2.3.0-bin-hadoop2.7/python/lib/py4j-0.10.6-src.zip/py4j/java_gateway.py:2257), which has no missing parents
2018-04-26 01:59:49 INFO  MemoryStore:54 - Block broadcast_9 stored as values in memory (estimated size 7.5 KB, free 413.8 MB)
2018-04-26 01:59:49 INFO  ContextCleaner:54 - Cleaned accumulator 204
2018-04-26 01:59:49 INFO  ContextCleaner:54 - Cleaned accumulator 217
2018-04-26 01:59:49 INFO  ContextCleaner:54 - Cleaned accumulator 205
2018-04-26 01:59:49 INFO  ContextCleaner:54 - Cleaned accumulator 211
2018-04-26 01:59:49 INFO  ContextCleaner:54 - Cleaned accumulator 221
2018-04-26 01:59:49 INFO  ContextCleaner:54 - Cleaned accumulator 218
2018-04-26 01:59:49 INFO  ContextCleaner:54 - Cleaned accumulator 208
2018-04-26 01:59:49 INFO  ContextCleaner:54 - Cleaned accumulator 222
2018-04-26 01:59:49 INFO  ContextCleaner:54 - Cleaned accumulator 202
2018-04-26 01:59:49 INFO  ContextCleaner:54 - Cleaned accumulator 225
2018-04-26 01:59:49 INFO  ContextCleaner:54 - Cleaned accumulator 224
2018-04-26 01:59:49 INFO  ContextCleaner:54 - Cleaned accumulator 209
2018-04-26 01:59:49 INFO  ContextCleaner:54 - Cleaned accumulator 214
2018-04-26 01:59:49 INFO  ContextCleaner:54 - Cleaned accumulator 201
2018-04-26 01:59:49 INFO  ContextCleaner:54 - Cleaned accumulator 207
2018-04-26 01:59:49 INFO  ContextCleaner:54 - Cleaned accumulator 220
2018-04-26 01:59:49 INFO  ContextCleaner:54 - Cleaned accumulator 210
2018-04-26 01:59:49 INFO  ContextCleaner:54 - Cleaned accumulator 212
2018-04-26 01:59:49 INFO  ContextCleaner:54 - Cleaned accumulator 213
2018-04-26 01:59:49 INFO  ContextCleaner:54 - Cleaned accumulator 206
2018-04-26 01:59:49 INFO  ContextCleaner:54 - Cleaned accumulator 216
2018-04-26 01:59:49 INFO  ContextCleaner:54 - Cleaned accumulator 215
2018-04-26 01:59:49 INFO  ContextCleaner:54 - Cleaned accumulator 223
2018-04-26 01:59:49 INFO  MemoryStore:54 - Block broadcast_9_piece0 stored as bytes in memory (estimated size 4.4 KB, free 413.8 MB)
2018-04-26 01:59:49 INFO  BlockManagerInfo:54 - Removed broadcast_8_piece0 on 10.0.2.15:33681 in memory (size: 4.1 KB, free: 413.9 MB)
2018-04-26 01:59:49 INFO  ContextCleaner:54 - Cleaned accumulator 203
2018-04-26 01:59:49 INFO  ContextCleaner:54 - Cleaned accumulator 219
2018-04-26 01:59:49 INFO  BlockManagerInfo:54 - Added broadcast_9_piece0 in memory on 10.0.2.15:33681 (size: 4.4 KB, free: 413.9 MB)
2018-04-26 01:59:49 INFO  SparkContext:54 - Created broadcast 9 from broadcast at DAGScheduler.scala:1039
2018-04-26 01:59:49 INFO  DAGScheduler:54 - Submitting 5 missing tasks from ResultStage 9 (PythonRDD[29] at call at /home/rjha/spark-2.3.0-bin-hadoop2.7/python/lib/py4j-0.10.6-src.zip/py4j/java_gateway.py:2257) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4))
2018-04-26 01:59:49 INFO  TaskSchedulerImpl:54 - Adding task set 9.0 with 5 tasks
2018-04-26 01:59:49 INFO  TaskSetManager:54 - Starting task 0.0 in stage 9.0 (TID 33, localhost, executor driver, partition 0, ANY, 7772 bytes)
2018-04-26 01:59:49 INFO  Executor:54 - Running task 0.0 in stage 9.0 (TID 33)
2018-04-26 01:59:49 INFO  BlockManager:54 - Found block input-0-1524722384800 locally
2018-04-26 01:59:49 INFO  PythonRunner:54 - Times: total = 46, boot = -130, init = 176, finish = 0
2018-04-26 01:59:49 INFO  PythonRunner:54 - Times: total = 47, boot = -198, init = 242, finish = 3
2018-04-26 01:59:49 INFO  Executor:54 - Finished task 0.0 in stage 9.0 (TID 33). 1224 bytes result sent to driver
2018-04-26 01:59:49 INFO  TaskSetManager:54 - Starting task 1.0 in stage 9.0 (TID 34, localhost, executor driver, partition 1, ANY, 7772 bytes)
2018-04-26 01:59:49 INFO  TaskSetManager:54 - Finished task 0.0 in stage 9.0 (TID 33) in 62 ms on localhost (executor driver) (1/5)
2018-04-26 01:59:49 INFO  Executor:54 - Running task 1.0 in stage 9.0 (TID 34)
2018-04-26 01:59:49 INFO  BlockManager:54 - Found block input-0-1524722385000 locally
2018-04-26 01:59:49 INFO  MemoryStore:54 - Block input-0-1524722389600 stored as bytes in memory (estimated size 340.0 B, free 413.8 MB)
2018-04-26 01:59:49 INFO  BlockManagerInfo:54 - Added input-0-1524722389600 in memory on 10.0.2.15:33681 (size: 340.0 B, free: 413.9 MB)
2018-04-26 01:59:49 WARN  RandomBlockReplicationPolicy:66 - Expecting 1 replicas with only 0 peer/s.
2018-04-26 01:59:49 WARN  BlockManager:66 - Block input-0-1524722389600 replicated to only 0 peer(s) instead of 1 peers
2018-04-26 01:59:49 INFO  BlockGenerator:54 - Pushed block input-0-1524722389600
2018-04-26 01:59:49 INFO  PythonRunner:54 - Times: total = 98, boot = 69, init = 28, finish = 1
2018-04-26 01:59:49 INFO  PythonRunner:54 - Times: total = 64, boot = 21, init = 29, finish = 14
2018-04-26 01:59:49 INFO  Executor:54 - Finished task 1.0 in stage 9.0 (TID 34). 1267 bytes result sent to driver
2018-04-26 01:59:49 INFO  TaskSetManager:54 - Starting task 2.0 in stage 9.0 (TID 35, localhost, executor driver, partition 2, ANY, 7772 bytes)
2018-04-26 01:59:49 INFO  TaskSetManager:54 - Finished task 1.0 in stage 9.0 (TID 34) in 129 ms on localhost (executor driver) (2/5)
2018-04-26 01:59:49 INFO  Executor:54 - Running task 2.0 in stage 9.0 (TID 35)
2018-04-26 01:59:49 INFO  BlockManager:54 - Found block input-0-1524722385200 locally
2018-04-26 01:59:50 INFO  MemoryStore:54 - Block input-0-1524722389800 stored as bytes in memory (estimated size 344.0 B, free 413.8 MB)
2018-04-26 01:59:50 INFO  BlockManagerInfo:54 - Added input-0-1524722389800 in memory on 10.0.2.15:33681 (size: 344.0 B, free: 413.9 MB)
2018-04-26 01:59:50 WARN  RandomBlockReplicationPolicy:66 - Expecting 1 replicas with only 0 peer/s.
2018-04-26 01:59:50 WARN  BlockManager:66 - Block input-0-1524722389800 replicated to only 0 peer(s) instead of 1 peers
2018-04-26 01:59:50 INFO  BlockGenerator:54 - Pushed block input-0-1524722389800
2018-04-26 01:59:50 INFO  PythonRunner:54 - Times: total = 167, boot = 54, init = 34, finish = 79
2018-04-26 01:59:50 INFO  PythonRunner:54 - Times: total = 135, boot = 31, init = 97, finish = 7
2018-04-26 01:59:50 INFO  Executor:54 - Finished task 2.0 in stage 9.0 (TID 35). 1267 bytes result sent to driver
2018-04-26 01:59:50 INFO  TaskSetManager:54 - Starting task 3.0 in stage 9.0 (TID 36, localhost, executor driver, partition 3, ANY, 7772 bytes)
2018-04-26 01:59:50 INFO  TaskSetManager:54 - Finished task 2.0 in stage 9.0 (TID 35) in 193 ms on localhost (executor driver) (3/5)
2018-04-26 01:59:50 INFO  Executor:54 - Running task 3.0 in stage 9.0 (TID 36)
2018-04-26 01:59:50 INFO  BlockManager:54 - Found block input-0-1524722385400 locally
2018-04-26 01:59:50 INFO  JobScheduler:54 - Added jobs for time 1524722390000 ms
2018-04-26 01:59:50 INFO  PythonRunner:54 - Times: total = 63, boot = 63, init = 0, finish = 0
2018-04-26 01:59:50 INFO  MemoryStore:54 - Block input-0-1524722390000 stored as bytes in memory (estimated size 321.0 B, free 413.8 MB)
2018-04-26 01:59:50 INFO  BlockManagerInfo:54 - Added input-0-1524722390000 in memory on 10.0.2.15:33681 (size: 321.0 B, free: 413.9 MB)
2018-04-26 01:59:50 WARN  RandomBlockReplicationPolicy:66 - Expecting 1 replicas with only 0 peer/s.
2018-04-26 01:59:50 WARN  BlockManager:66 - Block input-0-1524722390000 replicated to only 0 peer(s) instead of 1 peers
2018-04-26 01:59:50 INFO  BlockGenerator:54 - Pushed block input-0-1524722390000
2018-04-26 01:59:50 INFO  PythonRunner:54 - Times: total = 113, boot = 52, init = 1, finish = 60
2018-04-26 01:59:50 INFO  Executor:54 - Finished task 3.0 in stage 9.0 (TID 36). 1267 bytes result sent to driver
2018-04-26 01:59:50 INFO  TaskSetManager:54 - Starting task 4.0 in stage 9.0 (TID 37, localhost, executor driver, partition 4, ANY, 7772 bytes)
2018-04-26 01:59:50 INFO  TaskSetManager:54 - Finished task 3.0 in stage 9.0 (TID 36) in 148 ms on localhost (executor driver) (4/5)
2018-04-26 01:59:50 INFO  Executor:54 - Running task 4.0 in stage 9.0 (TID 37)
2018-04-26 01:59:50 INFO  BlockManager:54 - Found block input-0-1524722385600 locally
2018-04-26 01:59:50 INFO  PythonRunner:54 - Times: total = 78, boot = 77, init = 1, finish = 0
2018-04-26 01:59:50 INFO  PythonRunner:54 - Times: total = 79, boot = 61, init = 0, finish = 18
2018-04-26 01:59:50 INFO  MemoryStore:54 - Block input-0-1524722390200 stored as bytes in memory (estimated size 319.0 B, free 413.8 MB)
2018-04-26 01:59:50 INFO  BlockManagerInfo:54 - Added input-0-1524722390200 in memory on 10.0.2.15:33681 (size: 319.0 B, free: 413.9 MB)
2018-04-26 01:59:50 INFO  Executor:54 - Finished task 4.0 in stage 9.0 (TID 37). 1310 bytes result sent to driver
2018-04-26 01:59:50 WARN  RandomBlockReplicationPolicy:66 - Expecting 1 replicas with only 0 peer/s.
2018-04-26 01:59:50 WARN  BlockManager:66 - Block input-0-1524722390200 replicated to only 0 peer(s) instead of 1 peers
2018-04-26 01:59:50 INFO  BlockGenerator:54 - Pushed block input-0-1524722390200
2018-04-26 01:59:50 INFO  TaskSetManager:54 - Finished task 4.0 in stage 9.0 (TID 37) in 212 ms on localhost (executor driver) (5/5)
2018-04-26 01:59:50 INFO  TaskSchedulerImpl:54 - Removed TaskSet 9.0, whose tasks have all completed, from pool 
2018-04-26 01:59:50 INFO  DAGScheduler:54 - ResultStage 9 (call at /home/rjha/spark-2.3.0-bin-hadoop2.7/python/lib/py4j-0.10.6-src.zip/py4j/java_gateway.py:2257) finished in 0.782 s
2018-04-26 01:59:50 INFO  DAGScheduler:54 - Job 11 finished: call at /home/rjha/spark-2.3.0-bin-hadoop2.7/python/lib/py4j-0.10.6-src.zip/py4j/java_gateway.py:2257, took 0.788234 s
2018-04-26 01:59:50 INFO  JobScheduler:54 - Finished job streaming job 1524722386000 ms.0 from job set of time 1524722386000 ms
2018-04-26 01:59:50 INFO  JobScheduler:54 - Starting job streaming job 1524722386000 ms.1 from job set of time 1524722386000 ms
2018-04-26 01:59:50 INFO  MemoryStore:54 - Block input-0-1524722390400 stored as bytes in memory (estimated size 323.0 B, free 413.8 MB)
2018-04-26 01:59:50 INFO  BlockManagerInfo:54 - Added input-0-1524722390400 in memory on 10.0.2.15:33681 (size: 323.0 B, free: 413.9 MB)
2018-04-26 01:59:50 WARN  RandomBlockReplicationPolicy:66 - Expecting 1 replicas with only 0 peer/s.
2018-04-26 01:59:50 WARN  BlockManager:66 - Block input-0-1524722390400 replicated to only 0 peer(s) instead of 1 peers
2018-04-26 01:59:50 INFO  BlockGenerator:54 - Pushed block input-0-1524722390400
2018-04-26 01:59:50 INFO  SparkContext:54 - Starting job: call at /home/rjha/spark-2.3.0-bin-hadoop2.7/python/lib/py4j-0.10.6-src.zip/py4j/java_gateway.py:2257
2018-04-26 01:59:50 INFO  DAGScheduler:54 - Got job 12 (call at /home/rjha/spark-2.3.0-bin-hadoop2.7/python/lib/py4j-0.10.6-src.zip/py4j/java_gateway.py:2257) with 5 output partitions
2018-04-26 01:59:50 INFO  DAGScheduler:54 - Final stage: ResultStage 10 (call at /home/rjha/spark-2.3.0-bin-hadoop2.7/python/lib/py4j-0.10.6-src.zip/py4j/java_gateway.py:2257)
2018-04-26 01:59:50 INFO  DAGScheduler:54 - Parents of final stage: List()
2018-04-26 01:59:50 INFO  DAGScheduler:54 - Missing parents: List()
2018-04-26 01:59:50 INFO  DAGScheduler:54 - Submitting ResultStage 10 (PythonRDD[32] at call at /home/rjha/spark-2.3.0-bin-hadoop2.7/python/lib/py4j-0.10.6-src.zip/py4j/java_gateway.py:2257), which has no missing parents
2018-04-26 01:59:50 INFO  MemoryStore:54 - Block broadcast_10 stored as values in memory (estimated size 7.1 KB, free 413.8 MB)
2018-04-26 01:59:50 INFO  ContextCleaner:54 - Cleaned accumulator 237
2018-04-26 01:59:50 INFO  ContextCleaner:54 - Cleaned accumulator 234
2018-04-26 01:59:50 INFO  ContextCleaner:54 - Cleaned accumulator 239
2018-04-26 01:59:50 INFO  ContextCleaner:54 - Cleaned accumulator 241
2018-04-26 01:59:50 INFO  ContextCleaner:54 - Cleaned accumulator 245
2018-04-26 01:59:50 INFO  ContextCleaner:54 - Cleaned accumulator 238
2018-04-26 01:59:50 INFO  ContextCleaner:54 - Cleaned accumulator 231
2018-04-26 01:59:50 INFO  ContextCleaner:54 - Cleaned accumulator 250
2018-04-26 01:59:50 INFO  ContextCleaner:54 - Cleaned accumulator 236
2018-04-26 01:59:50 INFO  ContextCleaner:54 - Cleaned accumulator 235
2018-04-26 01:59:50 INFO  ContextCleaner:54 - Cleaned accumulator 243
2018-04-26 01:59:50 INFO  ContextCleaner:54 - Cleaned accumulator 248
2018-04-26 01:59:50 INFO  ContextCleaner:54 - Cleaned accumulator 244
2018-04-26 01:59:50 INFO  ContextCleaner:54 - Cleaned accumulator 249
2018-04-26 01:59:50 INFO  ContextCleaner:54 - Cleaned accumulator 227
2018-04-26 01:59:50 INFO  ContextCleaner:54 - Cleaned accumulator 228
2018-04-26 01:59:50 INFO  ContextCleaner:54 - Cleaned accumulator 242
2018-04-26 01:59:50 INFO  ContextCleaner:54 - Cleaned accumulator 232
2018-04-26 01:59:50 INFO  BlockManagerInfo:54 - Removed broadcast_9_piece0 on 10.0.2.15:33681 in memory (size: 4.4 KB, free: 413.9 MB)
2018-04-26 01:59:50 INFO  ContextCleaner:54 - Cleaned accumulator 230
2018-04-26 01:59:50 INFO  ContextCleaner:54 - Cleaned accumulator 233
2018-04-26 01:59:50 INFO  ContextCleaner:54 - Cleaned accumulator 229
2018-04-26 01:59:50 INFO  MemoryStore:54 - Block broadcast_10_piece0 stored as bytes in memory (estimated size 4.1 KB, free 413.8 MB)
2018-04-26 01:59:50 INFO  BlockManagerInfo:54 - Added broadcast_10_piece0 in memory on 10.0.2.15:33681 (size: 4.1 KB, free: 413.9 MB)
2018-04-26 01:59:50 INFO  SparkContext:54 - Created broadcast 10 from broadcast at DAGScheduler.scala:1039
2018-04-26 01:59:50 INFO  DAGScheduler:54 - Submitting 5 missing tasks from ResultStage 10 (PythonRDD[32] at call at /home/rjha/spark-2.3.0-bin-hadoop2.7/python/lib/py4j-0.10.6-src.zip/py4j/java_gateway.py:2257) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4))
2018-04-26 01:59:50 INFO  TaskSchedulerImpl:54 - Adding task set 10.0 with 5 tasks
2018-04-26 01:59:50 INFO  TaskSetManager:54 - Starting task 0.0 in stage 10.0 (TID 38, localhost, executor driver, partition 0, ANY, 7772 bytes)
2018-04-26 01:59:50 INFO  Executor:54 - Running task 0.0 in stage 10.0 (TID 38)
2018-04-26 01:59:50 INFO  BlockManager:54 - Found block input-0-1524722384800 locally
2018-04-26 01:59:50 INFO  ContextCleaner:54 - Cleaned accumulator 240
2018-04-26 01:59:50 INFO  ContextCleaner:54 - Cleaned accumulator 226
2018-04-26 01:59:50 INFO  ContextCleaner:54 - Cleaned accumulator 247
2018-04-26 01:59:50 INFO  ContextCleaner:54 - Cleaned accumulator 246
2018-04-26 01:59:50 INFO  PythonRunner:54 - Times: total = 52, boot = -220, init = 272, finish = 0
2018-04-26 01:59:50 INFO  PythonRunner:54 - Times: total = 48, boot = -182, init = 230, finish = 0
2018-04-26 01:59:50 INFO  Executor:54 - Finished task 0.0 in stage 10.0 (TID 38). 1267 bytes result sent to driver
2018-04-26 01:59:50 INFO  TaskSetManager:54 - Starting task 1.0 in stage 10.0 (TID 39, localhost, executor driver, partition 1, ANY, 7772 bytes)
2018-04-26 01:59:50 INFO  TaskSetManager:54 - Finished task 0.0 in stage 10.0 (TID 38) in 76 ms on localhost (executor driver) (1/5)
2018-04-26 01:59:50 INFO  Executor:54 - Running task 1.0 in stage 10.0 (TID 39)
2018-04-26 01:59:50 INFO  BlockManager:54 - Found block input-0-1524722385000 locally
2018-04-26 01:59:50 INFO  MemoryStore:54 - Block input-0-1524722390600 stored as bytes in memory (estimated size 301.0 B, free 413.8 MB)
2018-04-26 01:59:50 INFO  BlockManagerInfo:54 - Added input-0-1524722390600 in memory on 10.0.2.15:33681 (size: 301.0 B, free: 413.9 MB)
2018-04-26 01:59:50 WARN  RandomBlockReplicationPolicy:66 - Expecting 1 replicas with only 0 peer/s.
2018-04-26 01:59:50 WARN  BlockManager:66 - Block input-0-1524722390600 replicated to only 0 peer(s) instead of 1 peers
2018-04-26 01:59:50 INFO  BlockGenerator:54 - Pushed block input-0-1524722390600
2018-04-26 01:59:50 INFO  PythonRunner:54 - Times: total = 60, boot = 0, init = 60, finish = 0
2018-04-26 01:59:50 INFO  PythonRunner:54 - Times: total = 55, boot = -6, init = 61, finish = 0
2018-04-26 01:59:50 INFO  Executor:54 - Finished task 1.0 in stage 10.0 (TID 39). 1267 bytes result sent to driver
2018-04-26 01:59:50 INFO  TaskSetManager:54 - Starting task 2.0 in stage 10.0 (TID 40, localhost, executor driver, partition 2, ANY, 7772 bytes)
2018-04-26 01:59:50 INFO  TaskSetManager:54 - Finished task 1.0 in stage 10.0 (TID 39) in 84 ms on localhost (executor driver) (2/5)
2018-04-26 01:59:50 INFO  Executor:54 - Running task 2.0 in stage 10.0 (TID 40)
2018-04-26 01:59:50 INFO  BlockManager:54 - Found block input-0-1524722385200 locally
2018-04-26 01:59:50 INFO  PythonRunner:54 - Times: total = 54, boot = 24, init = 30, finish = 0
2018-04-26 01:59:50 INFO  PythonRunner:54 - Times: total = 49, boot = -12, init = 61, finish = 0
2018-04-26 01:59:50 INFO  Executor:54 - Finished task 2.0 in stage 10.0 (TID 40). 1267 bytes result sent to driver
2018-04-26 01:59:50 INFO  TaskSetManager:54 - Starting task 3.0 in stage 10.0 (TID 41, localhost, executor driver, partition 3, ANY, 7772 bytes)
2018-04-26 01:59:50 INFO  TaskSetManager:54 - Finished task 2.0 in stage 10.0 (TID 40) in 81 ms on localhost (executor driver) (3/5)
2018-04-26 01:59:50 INFO  Executor:54 - Running task 3.0 in stage 10.0 (TID 41)
2018-04-26 01:59:50 INFO  BlockManager:54 - Found block input-0-1524722385400 locally
2018-04-26 01:59:51 INFO  MemoryStore:54 - Block input-0-1524722390800 stored as bytes in memory (estimated size 405.0 B, free 413.8 MB)
2018-04-26 01:59:51 INFO  BlockManagerInfo:54 - Added input-0-1524722390800 in memory on 10.0.2.15:33681 (size: 405.0 B, free: 413.9 MB)
2018-04-26 01:59:51 WARN  RandomBlockReplicationPolicy:66 - Expecting 1 replicas with only 0 peer/s.
2018-04-26 01:59:51 WARN  BlockManager:66 - Block input-0-1524722390800 replicated to only 0 peer(s) instead of 1 peers
2018-04-26 01:59:51 INFO  BlockGenerator:54 - Pushed block input-0-1524722390800
2018-04-26 01:59:51 INFO  PythonRunner:54 - Times: total = 95, boot = 7, init = 88, finish = 0
2018-04-26 01:59:51 INFO  PythonRunner:54 - Times: total = 56, boot = 21, init = 35, finish = 0
2018-04-26 01:59:51 INFO  Executor:54 - Finished task 3.0 in stage 10.0 (TID 41). 1267 bytes result sent to driver
2018-04-26 01:59:51 INFO  TaskSetManager:54 - Starting task 4.0 in stage 10.0 (TID 42, localhost, executor driver, partition 4, ANY, 7772 bytes)
2018-04-26 01:59:51 INFO  Executor:54 - Running task 4.0 in stage 10.0 (TID 42)
2018-04-26 01:59:51 INFO  TaskSetManager:54 - Finished task 3.0 in stage 10.0 (TID 41) in 136 ms on localhost (executor driver) (4/5)
2018-04-26 01:59:51 INFO  BlockManager:54 - Found block input-0-1524722385600 locally
2018-04-26 01:59:51 INFO  JobScheduler:54 - Added jobs for time 1524722391000 ms
2018-04-26 01:59:51 INFO  PythonRunner:54 - Times: total = 54, boot = 24, init = 30, finish = 0
2018-04-26 01:59:51 INFO  PythonRunner:54 - Times: total = 49, boot = 14, init = 35, finish = 0
2018-04-26 01:59:51 INFO  Executor:54 - Finished task 4.0 in stage 10.0 (TID 42). 1267 bytes result sent to driver
2018-04-26 01:59:51 INFO  TaskSetManager:54 - Finished task 4.0 in stage 10.0 (TID 42) in 81 ms on localhost (executor driver) (5/5)
2018-04-26 01:59:51 INFO  TaskSchedulerImpl:54 - Removed TaskSet 10.0, whose tasks have all completed, from pool 
2018-04-26 01:59:51 INFO  DAGScheduler:54 - ResultStage 10 (call at /home/rjha/spark-2.3.0-bin-hadoop2.7/python/lib/py4j-0.10.6-src.zip/py4j/java_gateway.py:2257) finished in 0.503 s
2018-04-26 01:59:51 INFO  DAGScheduler:54 - Job 12 finished: call at /home/rjha/spark-2.3.0-bin-hadoop2.7/python/lib/py4j-0.10.6-src.zip/py4j/java_gateway.py:2257, took 0.514450 s
2018-04-26 01:59:51 INFO  JobScheduler:54 - Finished job streaming job 1524722386000 ms.1 from job set of time 1524722386000 ms
2018-04-26 01:59:51 INFO  JobScheduler:54 - Total delay: 5.192 s for time 1524722386000 ms (execution: 1.653 s)
2018-04-26 01:59:51 INFO  JobScheduler:54 - Starting job streaming job 1524722387000 ms.0 from job set of time 1524722387000 ms
2018-04-26 01:59:51 INFO  PythonRDD:54 - Removing RDD 15 from persistence list
2018-04-26 01:59:51 INFO  BlockManager:54 - Removing RDD 15
2018-04-26 01:59:51 INFO  BlockRDD:54 - Removing RDD 14 from persistence list
2018-04-26 01:59:51 INFO  BlockManager:54 - Removing RDD 14
2018-04-26 01:59:51 INFO  SocketInputDStream:54 - Removing blocks of RDD BlockRDD[14] at socketTextStream at NativeMethodAccessorImpl.java:0 of time 1524722386000 ms
2018-04-26 01:59:51 INFO  BlockManagerInfo:54 - Removed input-0-1524722383800 on 10.0.2.15:33681 in memory (size: 387.0 B, free: 413.9 MB)
2018-04-26 01:59:51 INFO  MemoryStore:54 - Block input-0-1524722391000 stored as bytes in memory (estimated size 406.0 B, free 413.8 MB)
2018-04-26 01:59:51 INFO  BlockManagerInfo:54 - Added input-0-1524722391000 in memory on 10.0.2.15:33681 (size: 406.0 B, free: 413.9 MB)
2018-04-26 01:59:51 WARN  RandomBlockReplicationPolicy:66 - Expecting 1 replicas with only 0 peer/s.
2018-04-26 01:59:51 WARN  BlockManager:66 - Block input-0-1524722391000 replicated to only 0 peer(s) instead of 1 peers
2018-04-26 01:59:51 INFO  BlockGenerator:54 - Pushed block input-0-1524722391000
2018-04-26 01:59:51 INFO  BlockManagerInfo:54 - Removed input-0-1524722384000 on 10.0.2.15:33681 in memory (size: 259.0 B, free: 413.9 MB)
2018-04-26 01:59:51 INFO  BlockManagerInfo:54 - Removed input-0-1524722384200 on 10.0.2.15:33681 in memory (size: 493.0 B, free: 413.9 MB)
2018-04-26 01:59:51 INFO  BlockManagerInfo:54 - Removed input-0-1524722384400 on 10.0.2.15:33681 in memory (size: 300.0 B, free: 413.9 MB)
2018-04-26 01:59:51 INFO  BlockManagerInfo:54 - Removed input-0-1524722384600 on 10.0.2.15:33681 in memory (size: 234.0 B, free: 413.9 MB)
2018-04-26 01:59:51 INFO  ReceivedBlockTracker:54 - Deleting batches: 1524722384000 ms
2018-04-26 01:59:51 INFO  InputInfoTracker:54 - remove old batch metadata: 1524722384000 ms
2018-04-26 01:59:51 INFO  SparkContext:54 - Starting job: call at /home/rjha/spark-2.3.0-bin-hadoop2.7/python/lib/py4j-0.10.6-src.zip/py4j/java_gateway.py:2257
2018-04-26 01:59:51 INFO  DAGScheduler:54 - Got job 13 (call at /home/rjha/spark-2.3.0-bin-hadoop2.7/python/lib/py4j-0.10.6-src.zip/py4j/java_gateway.py:2257) with 5 output partitions
2018-04-26 01:59:51 INFO  DAGScheduler:54 - Final stage: ResultStage 11 (call at /home/rjha/spark-2.3.0-bin-hadoop2.7/python/lib/py4j-0.10.6-src.zip/py4j/java_gateway.py:2257)
2018-04-26 01:59:51 INFO  DAGScheduler:54 - Parents of final stage: List()
2018-04-26 01:59:51 INFO  DAGScheduler:54 - Missing parents: List()
2018-04-26 01:59:51 INFO  DAGScheduler:54 - Submitting ResultStage 11 (PythonRDD[35] at call at /home/rjha/spark-2.3.0-bin-hadoop2.7/python/lib/py4j-0.10.6-src.zip/py4j/java_gateway.py:2257), which has no missing parents
2018-04-26 01:59:51 INFO  MemoryStore:54 - Block broadcast_11 stored as values in memory (estimated size 7.5 KB, free 413.8 MB)
2018-04-26 01:59:51 INFO  ContextCleaner:54 - Cleaned accumulator 269
2018-04-26 01:59:51 INFO  ContextCleaner:54 - Cleaned accumulator 251
2018-04-26 01:59:51 INFO  BlockManagerInfo:54 - Removed broadcast_10_piece0 on 10.0.2.15:33681 in memory (size: 4.1 KB, free: 413.9 MB)
2018-04-26 01:59:51 INFO  ContextCleaner:54 - Cleaned accumulator 262
2018-04-26 01:59:51 INFO  ContextCleaner:54 - Cleaned accumulator 256
2018-04-26 01:59:51 INFO  ContextCleaner:54 - Cleaned accumulator 257
2018-04-26 01:59:51 INFO  ContextCleaner:54 - Cleaned accumulator 259
2018-04-26 01:59:51 INFO  ContextCleaner:54 - Cleaned accumulator 252
2018-04-26 01:59:51 INFO  ContextCleaner:54 - Cleaned accumulator 271
2018-04-26 01:59:51 INFO  ContextCleaner:54 - Cleaned accumulator 268
2018-04-26 01:59:51 INFO  ContextCleaner:54 - Cleaned accumulator 270
2018-04-26 01:59:51 INFO  ContextCleaner:54 - Cleaned accumulator 274
2018-04-26 01:59:51 INFO  ContextCleaner:54 - Cleaned accumulator 254
2018-04-26 01:59:51 INFO  ContextCleaner:54 - Cleaned accumulator 253
2018-04-26 01:59:51 INFO  ContextCleaner:54 - Cleaned accumulator 258
2018-04-26 01:59:51 INFO  ContextCleaner:54 - Cleaned accumulator 275
2018-04-26 01:59:51 INFO  ContextCleaner:54 - Cleaned accumulator 261
2018-04-26 01:59:51 INFO  ContextCleaner:54 - Cleaned accumulator 273
2018-04-26 01:59:51 INFO  ContextCleaner:54 - Cleaned accumulator 272
2018-04-26 01:59:51 INFO  ContextCleaner:54 - Cleaned accumulator 260
2018-04-26 01:59:51 INFO  ContextCleaner:54 - Cleaned accumulator 255
2018-04-26 01:59:51 INFO  ContextCleaner:54 - Cleaned accumulator 267
2018-04-26 01:59:51 INFO  ContextCleaner:54 - Cleaned accumulator 265
2018-04-26 01:59:51 INFO  ContextCleaner:54 - Cleaned accumulator 263
2018-04-26 01:59:51 INFO  ContextCleaner:54 - Cleaned accumulator 266
2018-04-26 01:59:51 INFO  ContextCleaner:54 - Cleaned accumulator 264
2018-04-26 01:59:51 INFO  MemoryStore:54 - Block broadcast_11_piece0 stored as bytes in memory (estimated size 4.4 KB, free 413.8 MB)
2018-04-26 01:59:51 INFO  BlockManagerInfo:54 - Added broadcast_11_piece0 in memory on 10.0.2.15:33681 (size: 4.4 KB, free: 413.9 MB)
2018-04-26 01:59:51 INFO  SparkContext:54 - Created broadcast 11 from broadcast at DAGScheduler.scala:1039
2018-04-26 01:59:51 INFO  DAGScheduler:54 - Submitting 5 missing tasks from ResultStage 11 (PythonRDD[35] at call at /home/rjha/spark-2.3.0-bin-hadoop2.7/python/lib/py4j-0.10.6-src.zip/py4j/java_gateway.py:2257) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4))
2018-04-26 01:59:51 INFO  TaskSchedulerImpl:54 - Adding task set 11.0 with 5 tasks
2018-04-26 01:59:51 INFO  TaskSetManager:54 - Starting task 0.0 in stage 11.0 (TID 43, localhost, executor driver, partition 0, ANY, 7772 bytes)
2018-04-26 01:59:51 INFO  Executor:54 - Running task 0.0 in stage 11.0 (TID 43)
2018-04-26 01:59:51 INFO  BlockManager:54 - Found block input-0-1524722385800 locally
2018-04-26 01:59:51 INFO  PythonRunner:54 - Times: total = 48, boot = -77, init = 125, finish = 0
2018-04-26 01:59:51 INFO  PythonRunner:54 - Times: total = 51, boot = -73, init = 117, finish = 7
2018-04-26 01:59:51 INFO  Executor:54 - Finished task 0.0 in stage 11.0 (TID 43). 1267 bytes result sent to driver
2018-04-26 01:59:51 INFO  TaskSetManager:54 - Starting task 1.0 in stage 11.0 (TID 44, localhost, executor driver, partition 1, ANY, 7772 bytes)
2018-04-26 01:59:51 INFO  TaskSetManager:54 - Finished task 0.0 in stage 11.0 (TID 43) in 72 ms on localhost (executor driver) (1/5)
2018-04-26 01:59:51 INFO  Executor:54 - Running task 1.0 in stage 11.0 (TID 44)
2018-04-26 01:59:51 INFO  BlockManager:54 - Found block input-0-1524722386000 locally
2018-04-26 01:59:51 INFO  MemoryStore:54 - Block input-0-1524722391200 stored as bytes in memory (estimated size 407.0 B, free 413.8 MB)
2018-04-26 01:59:51 INFO  BlockManagerInfo:54 - Added input-0-1524722391200 in memory on 10.0.2.15:33681 (size: 407.0 B, free: 413.9 MB)
2018-04-26 01:59:51 WARN  RandomBlockReplicationPolicy:66 - Expecting 1 replicas with only 0 peer/s.
2018-04-26 01:59:51 WARN  BlockManager:66 - Block input-0-1524722391200 replicated to only 0 peer(s) instead of 1 peers
2018-04-26 01:59:51 INFO  BlockGenerator:54 - Pushed block input-0-1524722391200
2018-04-26 01:59:51 INFO  PythonRunner:54 - Times: total = 99, boot = 99, init = 0, finish = 0
2018-04-26 01:59:51 INFO  PythonRunner:54 - Times: total = 85, boot = 62, init = 9, finish = 14
2018-04-26 01:59:51 INFO  Executor:54 - Finished task 1.0 in stage 11.0 (TID 44). 1267 bytes result sent to driver
2018-04-26 01:59:51 INFO  TaskSetManager:54 - Starting task 2.0 in stage 11.0 (TID 45, localhost, executor driver, partition 2, ANY, 7772 bytes)
2018-04-26 01:59:51 INFO  TaskSetManager:54 - Finished task 1.0 in stage 11.0 (TID 44) in 133 ms on localhost (executor driver) (2/5)
2018-04-26 01:59:51 INFO  Executor:54 - Running task 2.0 in stage 11.0 (TID 45)
2018-04-26 01:59:51 INFO  BlockManager:54 - Found block input-0-1524722386200 locally
2018-04-26 01:59:51 INFO  MemoryStore:54 - Block input-0-1524722391400 stored as bytes in memory (estimated size 380.0 B, free 413.8 MB)
2018-04-26 01:59:51 INFO  BlockManagerInfo:54 - Added input-0-1524722391400 in memory on 10.0.2.15:33681 (size: 380.0 B, free: 413.9 MB)
2018-04-26 01:59:51 WARN  RandomBlockReplicationPolicy:66 - Expecting 1 replicas with only 0 peer/s.
2018-04-26 01:59:51 WARN  BlockManager:66 - Block input-0-1524722391400 replicated to only 0 peer(s) instead of 1 peers
2018-04-26 01:59:51 INFO  BlockGenerator:54 - Pushed block input-0-1524722391400
2018-04-26 01:59:51 INFO  PythonRunner:54 - Times: total = 85, boot = 53, init = 32, finish = 0
2018-04-26 01:59:51 INFO  PythonRunner:54 - Times: total = 94, boot = 79, init = 0, finish = 15
2018-04-26 01:59:51 INFO  Executor:54 - Finished task 2.0 in stage 11.0 (TID 45). 1267 bytes result sent to driver
2018-04-26 01:59:51 INFO  TaskSetManager:54 - Starting task 3.0 in stage 11.0 (TID 46, localhost, executor driver, partition 3, ANY, 7772 bytes)
2018-04-26 01:59:51 INFO  Executor:54 - Running task 3.0 in stage 11.0 (TID 46)
2018-04-26 01:59:51 INFO  TaskSetManager:54 - Finished task 2.0 in stage 11.0 (TID 45) in 147 ms on localhost (executor driver) (3/5)
2018-04-26 01:59:51 INFO  BlockManager:54 - Found block input-0-1524722386400 locally
2018-04-26 01:59:51 INFO  PythonRunner:54 - Times: total = 97, boot = 20, init = 77, finish = 0
2018-04-26 01:59:51 INFO  MemoryStore:54 - Block input-0-1524722391600 stored as bytes in memory (estimated size 323.0 B, free 413.8 MB)
2018-04-26 01:59:51 INFO  BlockManagerInfo:54 - Added input-0-1524722391600 in memory on 10.0.2.15:33681 (size: 323.0 B, free: 413.9 MB)
2018-04-26 01:59:51 WARN  RandomBlockReplicationPolicy:66 - Expecting 1 replicas with only 0 peer/s.
2018-04-26 01:59:51 WARN  BlockManager:66 - Block input-0-1524722391600 replicated to only 0 peer(s) instead of 1 peers
2018-04-26 01:59:51 INFO  BlockGenerator:54 - Pushed block input-0-1524722391600
2018-04-26 01:59:51 INFO  PythonRunner:54 - Times: total = 230, boot = 33, init = 35, finish = 162
2018-04-26 01:59:51 INFO  Executor:54 - Finished task 3.0 in stage 11.0 (TID 46). 1267 bytes result sent to driver
2018-04-26 01:59:51 INFO  TaskSetManager:54 - Starting task 4.0 in stage 11.0 (TID 47, localhost, executor driver, partition 4, ANY, 7772 bytes)
2018-04-26 01:59:51 INFO  Executor:54 - Running task 4.0 in stage 11.0 (TID 47)
2018-04-26 01:59:51 INFO  TaskSetManager:54 - Finished task 3.0 in stage 11.0 (TID 46) in 311 ms on localhost (executor driver) (4/5)
2018-04-26 01:59:52 INFO  MemoryStore:54 - Block input-0-1524722391800 stored as bytes in memory (estimated size 303.0 B, free 413.8 MB)
2018-04-26 01:59:52 INFO  BlockManager:54 - Found block input-0-1524722386600 locally
2018-04-26 01:59:52 INFO  BlockManagerInfo:54 - Added input-0-1524722391800 in memory on 10.0.2.15:33681 (size: 303.0 B, free: 413.9 MB)
2018-04-26 01:59:52 WARN  RandomBlockReplicationPolicy:66 - Expecting 1 replicas with only 0 peer/s.
2018-04-26 01:59:52 WARN  BlockManager:66 - Block input-0-1524722391800 replicated to only 0 peer(s) instead of 1 peers
2018-04-26 01:59:52 INFO  BlockGenerator:54 - Pushed block input-0-1524722391800
2018-04-26 01:59:52 INFO  JobScheduler:54 - Added jobs for time 1524722392000 ms
2018-04-26 01:59:52 INFO  PythonRunner:54 - Times: total = 65, boot = -234, init = 299, finish = 0
2018-04-26 01:59:52 INFO  MemoryStore:54 - Block input-0-1524722392000 stored as bytes in memory (estimated size 263.0 B, free 413.8 MB)
2018-04-26 01:59:52 INFO  BlockManagerInfo:54 - Added input-0-1524722392000 in memory on 10.0.2.15:33681 (size: 263.0 B, free: 413.9 MB)
2018-04-26 01:59:52 WARN  RandomBlockReplicationPolicy:66 - Expecting 1 replicas with only 0 peer/s.
2018-04-26 01:59:52 WARN  BlockManager:66 - Block input-0-1524722392000 replicated to only 0 peer(s) instead of 1 peers
2018-04-26 01:59:52 INFO  BlockGenerator:54 - Pushed block input-0-1524722392000
2018-04-26 01:59:52 INFO  PythonRunner:54 - Times: total = 124, boot = 40, init = 26, finish = 58
2018-04-26 01:59:52 INFO  Executor:54 - Finished task 4.0 in stage 11.0 (TID 47). 1267 bytes result sent to driver
2018-04-26 01:59:52 INFO  TaskSetManager:54 - Finished task 4.0 in stage 11.0 (TID 47) in 270 ms on localhost (executor driver) (5/5)
2018-04-26 01:59:52 INFO  DAGScheduler:54 - ResultStage 11 (call at /home/rjha/spark-2.3.0-bin-hadoop2.7/python/lib/py4j-0.10.6-src.zip/py4j/java_gateway.py:2257) finished in 0.955 s
2018-04-26 01:59:52 INFO  DAGScheduler:54 - Job 13 finished: call at /home/rjha/spark-2.3.0-bin-hadoop2.7/python/lib/py4j-0.10.6-src.zip/py4j/java_gateway.py:2257, took 0.962789 s
2018-04-26 01:59:52 INFO  TaskSchedulerImpl:54 - Removed TaskSet 11.0, whose tasks have all completed, from pool 
2018-04-26 01:59:52 INFO  JobScheduler:54 - Finished job streaming job 1524722387000 ms.0 from job set of time 1524722387000 ms
2018-04-26 01:59:52 INFO  JobScheduler:54 - Starting job streaming job 1524722387000 ms.1 from job set of time 1524722387000 ms
2018-04-26 01:59:52 INFO  SparkContext:54 - Starting job: call at /home/rjha/spark-2.3.0-bin-hadoop2.7/python/lib/py4j-0.10.6-src.zip/py4j/java_gateway.py:2257
2018-04-26 01:59:52 INFO  DAGScheduler:54 - Got job 14 (call at /home/rjha/spark-2.3.0-bin-hadoop2.7/python/lib/py4j-0.10.6-src.zip/py4j/java_gateway.py:2257) with 5 output partitions
2018-04-26 01:59:52 INFO  DAGScheduler:54 - Final stage: ResultStage 12 (call at /home/rjha/spark-2.3.0-bin-hadoop2.7/python/lib/py4j-0.10.6-src.zip/py4j/java_gateway.py:2257)
2018-04-26 01:59:52 INFO  DAGScheduler:54 - Parents of final stage: List()
2018-04-26 01:59:52 INFO  DAGScheduler:54 - Missing parents: List()
2018-04-26 01:59:52 INFO  DAGScheduler:54 - Submitting ResultStage 12 (PythonRDD[38] at call at /home/rjha/spark-2.3.0-bin-hadoop2.7/python/lib/py4j-0.10.6-src.zip/py4j/java_gateway.py:2257), which has no missing parents
2018-04-26 01:59:52 INFO  MemoryStore:54 - Block broadcast_12 stored as values in memory (estimated size 7.1 KB, free 413.8 MB)
2018-04-26 01:59:52 INFO  ContextCleaner:54 - Cleaned accumulator 276
2018-04-26 01:59:52 INFO  ContextCleaner:54 - Cleaned accumulator 279
2018-04-26 01:59:52 INFO  ContextCleaner:54 - Cleaned accumulator 297
2018-04-26 01:59:52 INFO  ContextCleaner:54 - Cleaned accumulator 295
2018-04-26 01:59:52 INFO  ContextCleaner:54 - Cleaned accumulator 293
2018-04-26 01:59:52 INFO  ContextCleaner:54 - Cleaned accumulator 283
2018-04-26 01:59:52 INFO  ContextCleaner:54 - Cleaned accumulator 284
2018-04-26 01:59:52 INFO  ContextCleaner:54 - Cleaned accumulator 290
2018-04-26 01:59:52 INFO  ContextCleaner:54 - Cleaned accumulator 280
2018-04-26 01:59:52 INFO  ContextCleaner:54 - Cleaned accumulator 281
2018-04-26 01:59:52 INFO  ContextCleaner:54 - Cleaned accumulator 296
2018-04-26 01:59:52 INFO  ContextCleaner:54 - Cleaned accumulator 300
2018-04-26 01:59:52 INFO  ContextCleaner:54 - Cleaned accumulator 291
2018-04-26 01:59:52 INFO  ContextCleaner:54 - Cleaned accumulator 288
2018-04-26 01:59:52 INFO  ContextCleaner:54 - Cleaned accumulator 294
2018-04-26 01:59:52 INFO  ContextCleaner:54 - Cleaned accumulator 285
2018-04-26 01:59:52 INFO  ContextCleaner:54 - Cleaned accumulator 282
2018-04-26 01:59:52 INFO  ContextCleaner:54 - Cleaned accumulator 287
2018-04-26 01:59:52 INFO  ContextCleaner:54 - Cleaned accumulator 278
2018-04-26 01:59:52 INFO  ContextCleaner:54 - Cleaned accumulator 292
2018-04-26 01:59:52 INFO  ContextCleaner:54 - Cleaned accumulator 277
2018-04-26 01:59:52 INFO  ContextCleaner:54 - Cleaned accumulator 299
2018-04-26 01:59:52 INFO  MemoryStore:54 - Block input-0-1524722392200 stored as bytes in memory (estimated size 322.0 B, free 413.8 MB)
2018-04-26 01:59:52 INFO  BlockManagerInfo:54 - Removed broadcast_11_piece0 on 10.0.2.15:33681 in memory (size: 4.4 KB, free: 413.9 MB)
2018-04-26 01:59:52 INFO  BlockManagerInfo:54 - Added input-0-1524722392200 in memory on 10.0.2.15:33681 (size: 322.0 B, free: 413.9 MB)
2018-04-26 01:59:52 WARN  RandomBlockReplicationPolicy:66 - Expecting 1 replicas with only 0 peer/s.
2018-04-26 01:59:52 WARN  BlockManager:66 - Block input-0-1524722392200 replicated to only 0 peer(s) instead of 1 peers
2018-04-26 01:59:52 INFO  BlockGenerator:54 - Pushed block input-0-1524722392200
2018-04-26 01:59:52 INFO  ContextCleaner:54 - Cleaned accumulator 286
2018-04-26 01:59:52 INFO  ContextCleaner:54 - Cleaned accumulator 289
2018-04-26 01:59:52 INFO  ContextCleaner:54 - Cleaned accumulator 298
2018-04-26 01:59:52 INFO  MemoryStore:54 - Block broadcast_12_piece0 stored as bytes in memory (estimated size 4.1 KB, free 413.8 MB)
2018-04-26 01:59:52 INFO  BlockManagerInfo:54 - Added broadcast_12_piece0 in memory on 10.0.2.15:33681 (size: 4.1 KB, free: 413.9 MB)
2018-04-26 01:59:52 INFO  SparkContext:54 - Created broadcast 12 from broadcast at DAGScheduler.scala:1039
2018-04-26 01:59:52 INFO  DAGScheduler:54 - Submitting 5 missing tasks from ResultStage 12 (PythonRDD[38] at call at /home/rjha/spark-2.3.0-bin-hadoop2.7/python/lib/py4j-0.10.6-src.zip/py4j/java_gateway.py:2257) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4))
2018-04-26 01:59:52 INFO  TaskSchedulerImpl:54 - Adding task set 12.0 with 5 tasks
2018-04-26 01:59:52 INFO  TaskSetManager:54 - Starting task 0.0 in stage 12.0 (TID 48, localhost, executor driver, partition 0, ANY, 7772 bytes)
2018-04-26 01:59:52 INFO  Executor:54 - Running task 0.0 in stage 12.0 (TID 48)
2018-04-26 01:59:52 INFO  BlockManager:54 - Found block input-0-1524722385800 locally
2018-04-26 01:59:52 INFO  PythonRunner:54 - Times: total = 49, boot = -237, init = 285, finish = 1
2018-04-26 01:59:52 INFO  PythonRunner:54 - Times: total = 44, boot = -90, init = 134, finish = 0
2018-04-26 01:59:52 INFO  Executor:54 - Finished task 0.0 in stage 12.0 (TID 48). 1224 bytes result sent to driver
2018-04-26 01:59:52 INFO  TaskSetManager:54 - Starting task 1.0 in stage 12.0 (TID 49, localhost, executor driver, partition 1, ANY, 7772 bytes)
2018-04-26 01:59:52 INFO  Executor:54 - Running task 1.0 in stage 12.0 (TID 49)
2018-04-26 01:59:52 INFO  TaskSetManager:54 - Finished task 0.0 in stage 12.0 (TID 48) in 63 ms on localhost (executor driver) (1/5)
2018-04-26 01:59:52 INFO  BlockManager:54 - Found block input-0-1524722386000 locally
2018-04-26 01:59:52 INFO  MemoryStore:54 - Block input-0-1524722392400 stored as bytes in memory (estimated size 408.0 B, free 413.8 MB)
2018-04-26 01:59:52 INFO  PythonRunner:54 - Times: total = 52, boot = 41, init = 11, finish = 0
2018-04-26 01:59:52 INFO  BlockManagerInfo:54 - Added input-0-1524722392400 in memory on 10.0.2.15:33681 (size: 408.0 B, free: 413.9 MB)
2018-04-26 01:59:52 WARN  RandomBlockReplicationPolicy:66 - Expecting 1 replicas with only 0 peer/s.
2018-04-26 01:59:52 WARN  BlockManager:66 - Block input-0-1524722392400 replicated to only 0 peer(s) instead of 1 peers
2018-04-26 01:59:52 INFO  BlockGenerator:54 - Pushed block input-0-1524722392400
2018-04-26 01:59:52 INFO  PythonRunner:54 - Times: total = 44, boot = 17, init = 27, finish = 0
2018-04-26 01:59:52 INFO  Executor:54 - Finished task 1.0 in stage 12.0 (TID 49). 1267 bytes result sent to driver
2018-04-26 01:59:52 INFO  TaskSetManager:54 - Starting task 2.0 in stage 12.0 (TID 50, localhost, executor driver, partition 2, ANY, 7772 bytes)
2018-04-26 01:59:52 INFO  TaskSetManager:54 - Finished task 1.0 in stage 12.0 (TID 49) in 92 ms on localhost (executor driver) (2/5)
2018-04-26 01:59:52 INFO  Executor:54 - Running task 2.0 in stage 12.0 (TID 50)
2018-04-26 01:59:52 INFO  BlockManager:54 - Found block input-0-1524722386200 locally
2018-04-26 01:59:52 INFO  PythonRunner:54 - Times: total = 50, boot = 15, init = 34, finish = 1
2018-04-26 01:59:52 INFO  PythonRunner:54 - Times: total = 54, boot = -6, init = 60, finish = 0
2018-04-26 01:59:52 INFO  Executor:54 - Finished task 2.0 in stage 12.0 (TID 50). 1267 bytes result sent to driver
2018-04-26 01:59:52 INFO  TaskSetManager:54 - Starting task 3.0 in stage 12.0 (TID 51, localhost, executor driver, partition 3, ANY, 7772 bytes)
2018-04-26 01:59:52 INFO  TaskSetManager:54 - Finished task 2.0 in stage 12.0 (TID 50) in 90 ms on localhost (executor driver) (3/5)
2018-04-26 01:59:52 INFO  Executor:54 - Running task 3.0 in stage 12.0 (TID 51)
2018-04-26 01:59:52 INFO  BlockManager:54 - Found block input-0-1524722386400 locally
2018-04-26 01:59:52 INFO  PythonRunner:54 - Times: total = 50, boot = -16, init = 66, finish = 0
2018-04-26 01:59:52 INFO  PythonRunner:54 - Times: total = 44, boot = -2, init = 46, finish = 0
2018-04-26 01:59:52 INFO  Executor:54 - Finished task 3.0 in stage 12.0 (TID 51). 1267 bytes result sent to driver
2018-04-26 01:59:52 INFO  TaskSetManager:54 - Starting task 4.0 in stage 12.0 (TID 52, localhost, executor driver, partition 4, ANY, 7772 bytes)
2018-04-26 01:59:52 INFO  TaskSetManager:54 - Finished task 3.0 in stage 12.0 (TID 51) in 67 ms on localhost (executor driver) (4/5)
2018-04-26 01:59:52 INFO  Executor:54 - Running task 4.0 in stage 12.0 (TID 52)
2018-04-26 01:59:52 INFO  BlockManager:54 - Found block input-0-1524722386600 locally
2018-04-26 01:59:52 INFO  MemoryStore:54 - Block input-0-1524722392600 stored as bytes in memory (estimated size 411.0 B, free 413.8 MB)
2018-04-26 01:59:52 INFO  BlockManagerInfo:54 - Added input-0-1524722392600 in memory on 10.0.2.15:33681 (size: 411.0 B, free: 413.9 MB)
2018-04-26 01:59:52 WARN  RandomBlockReplicationPolicy:66 - Expecting 1 replicas with only 0 peer/s.
2018-04-26 01:59:52 WARN  BlockManager:66 - Block input-0-1524722392600 replicated to only 0 peer(s) instead of 1 peers
2018-04-26 01:59:52 INFO  BlockGenerator:54 - Pushed block input-0-1524722392600
2018-04-26 01:59:52 INFO  PythonRunner:54 - Times: total = 81, boot = 58, init = 23, finish = 0
2018-04-26 01:59:52 INFO  PythonRunner:54 - Times: total = 54, boot = 10, init = 44, finish = 0
2018-04-26 01:59:52 INFO  Executor:54 - Finished task 4.0 in stage 12.0 (TID 52). 1267 bytes result sent to driver
2018-04-26 01:59:52 INFO  TaskSetManager:54 - Finished task 4.0 in stage 12.0 (TID 52) in 124 ms on localhost (executor driver) (5/5)
2018-04-26 01:59:52 INFO  TaskSchedulerImpl:54 - Removed TaskSet 12.0, whose tasks have all completed, from pool 
2018-04-26 01:59:52 INFO  DAGScheduler:54 - ResultStage 12 (call at /home/rjha/spark-2.3.0-bin-hadoop2.7/python/lib/py4j-0.10.6-src.zip/py4j/java_gateway.py:2257) finished in 0.490 s
2018-04-26 01:59:52 INFO  DAGScheduler:54 - Job 14 finished: call at /home/rjha/spark-2.3.0-bin-hadoop2.7/python/lib/py4j-0.10.6-src.zip/py4j/java_gateway.py:2257, took 0.508866 s
2018-04-26 01:59:52 INFO  JobScheduler:54 - Finished job streaming job 1524722387000 ms.1 from job set of time 1524722387000 ms
2018-04-26 01:59:52 INFO  JobScheduler:54 - Total delay: 5.923 s for time 1524722387000 ms (execution: 1.730 s)
2018-04-26 01:59:52 INFO  JobScheduler:54 - Starting job streaming job 1524722388000 ms.0 from job set of time 1524722388000 ms
2018-04-26 01:59:52 INFO  PythonRDD:54 - Removing RDD 18 from persistence list
2018-04-26 01:59:52 INFO  BlockManager:54 - Removing RDD 18
2018-04-26 01:59:52 INFO  BlockRDD:54 - Removing RDD 17 from persistence list
2018-04-26 01:59:52 INFO  BlockManager:54 - Removing RDD 17
2018-04-26 01:59:52 INFO  SocketInputDStream:54 - Removing blocks of RDD BlockRDD[17] at socketTextStream at NativeMethodAccessorImpl.java:0 of time 1524722387000 ms
2018-04-26 01:59:52 INFO  BlockManagerInfo:54 - Removed input-0-1524722384800 on 10.0.2.15:33681 in memory (size: 325.0 B, free: 413.9 MB)
2018-04-26 01:59:52 INFO  BlockManagerInfo:54 - Removed input-0-1524722385000 on 10.0.2.15:33681 in memory (size: 343.0 B, free: 413.9 MB)
2018-04-26 01:59:52 INFO  BlockManagerInfo:54 - Removed input-0-1524722385200 on 10.0.2.15:33681 in memory (size: 306.0 B, free: 413.9 MB)
2018-04-26 01:59:52 INFO  ReceivedBlockTracker:54 - Deleting batches: 1524722385000 ms
2018-04-26 01:59:52 INFO  InputInfoTracker:54 - remove old batch metadata: 1524722385000 ms
2018-04-26 01:59:53 INFO  MemoryStore:54 - Block input-0-1524722392800 stored as bytes in memory (estimated size 282.0 B, free 413.8 MB)
2018-04-26 01:59:53 INFO  BlockManagerInfo:54 - Added input-0-1524722392800 in memory on 10.0.2.15:33681 (size: 282.0 B, free: 413.9 MB)
2018-04-26 01:59:53 WARN  RandomBlockReplicationPolicy:66 - Expecting 1 replicas with only 0 peer/s.
2018-04-26 01:59:53 WARN  BlockManager:66 - Block input-0-1524722392800 replicated to only 0 peer(s) instead of 1 peers
2018-04-26 01:59:53 INFO  BlockGenerator:54 - Pushed block input-0-1524722392800
2018-04-26 01:59:53 INFO  BlockManagerInfo:54 - Removed input-0-1524722385400 on 10.0.2.15:33681 in memory (size: 384.0 B, free: 413.9 MB)
2018-04-26 01:59:53 INFO  BlockManagerInfo:54 - Removed input-0-1524722385600 on 10.0.2.15:33681 in memory (size: 424.0 B, free: 413.9 MB)
2018-04-26 01:59:53 INFO  JobScheduler:54 - Added jobs for time 1524722393000 ms
2018-04-26 01:59:53 INFO  SparkContext:54 - Starting job: call at /home/rjha/spark-2.3.0-bin-hadoop2.7/python/lib/py4j-0.10.6-src.zip/py4j/java_gateway.py:2257
2018-04-26 01:59:53 INFO  DAGScheduler:54 - Got job 15 (call at /home/rjha/spark-2.3.0-bin-hadoop2.7/python/lib/py4j-0.10.6-src.zip/py4j/java_gateway.py:2257) with 5 output partitions
2018-04-26 01:59:53 INFO  DAGScheduler:54 - Final stage: ResultStage 13 (call at /home/rjha/spark-2.3.0-bin-hadoop2.7/python/lib/py4j-0.10.6-src.zip/py4j/java_gateway.py:2257)
2018-04-26 01:59:53 INFO  DAGScheduler:54 - Parents of final stage: List()
2018-04-26 01:59:53 INFO  DAGScheduler:54 - Missing parents: List()
2018-04-26 01:59:53 INFO  DAGScheduler:54 - Submitting ResultStage 13 (PythonRDD[40] at call at /home/rjha/spark-2.3.0-bin-hadoop2.7/python/lib/py4j-0.10.6-src.zip/py4j/java_gateway.py:2257), which has no missing parents
2018-04-26 01:59:53 INFO  MemoryStore:54 - Block broadcast_13 stored as values in memory (estimated size 7.5 KB, free 413.8 MB)
2018-04-26 01:59:53 INFO  ContextCleaner:54 - Cleaned accumulator 314
2018-04-26 01:59:53 INFO  ContextCleaner:54 - Cleaned accumulator 302
2018-04-26 01:59:53 INFO  ContextCleaner:54 - Cleaned accumulator 310
2018-04-26 01:59:53 INFO  ContextCleaner:54 - Cleaned accumulator 315
2018-04-26 01:59:53 INFO  ContextCleaner:54 - Cleaned accumulator 312
2018-04-26 01:59:53 INFO  ContextCleaner:54 - Cleaned accumulator 304
2018-04-26 01:59:53 INFO  ContextCleaner:54 - Cleaned accumulator 307
2018-04-26 01:59:53 INFO  ContextCleaner:54 - Cleaned accumulator 325
2018-04-26 01:59:53 INFO  ContextCleaner:54 - Cleaned accumulator 311
2018-04-26 01:59:53 INFO  ContextCleaner:54 - Cleaned accumulator 313
2018-04-26 01:59:53 INFO  ContextCleaner:54 - Cleaned accumulator 323
2018-04-26 01:59:53 INFO  ContextCleaner:54 - Cleaned accumulator 318
2018-04-26 01:59:53 INFO  ContextCleaner:54 - Cleaned accumulator 317
2018-04-26 01:59:53 INFO  ContextCleaner:54 - Cleaned accumulator 303
2018-04-26 01:59:53 INFO  ContextCleaner:54 - Cleaned accumulator 306
2018-04-26 01:59:53 INFO  ContextCleaner:54 - Cleaned accumulator 305
2018-04-26 01:59:53 INFO  ContextCleaner:54 - Cleaned accumulator 321
2018-04-26 01:59:53 INFO  ContextCleaner:54 - Cleaned accumulator 322
2018-04-26 01:59:53 INFO  ContextCleaner:54 - Cleaned accumulator 308
2018-04-26 01:59:53 INFO  ContextCleaner:54 - Cleaned accumulator 309
2018-04-26 01:59:53 INFO  ContextCleaner:54 - Cleaned accumulator 319
2018-04-26 01:59:53 INFO  ContextCleaner:54 - Cleaned accumulator 301
2018-04-26 01:59:53 INFO  ContextCleaner:54 - Cleaned accumulator 320
2018-04-26 01:59:53 INFO  BlockManagerInfo:54 - Removed broadcast_12_piece0 on 10.0.2.15:33681 in memory (size: 4.1 KB, free: 413.9 MB)
2018-04-26 01:59:53 INFO  ContextCleaner:54 - Cleaned accumulator 324
2018-04-26 01:59:53 INFO  ContextCleaner:54 - Cleaned accumulator 316
2018-04-26 01:59:53 INFO  MemoryStore:54 - Block broadcast_13_piece0 stored as bytes in memory (estimated size 4.4 KB, free 413.8 MB)
2018-04-26 01:59:53 INFO  BlockManagerInfo:54 - Added broadcast_13_piece0 in memory on 10.0.2.15:33681 (size: 4.4 KB, free: 413.9 MB)
2018-04-26 01:59:53 INFO  SparkContext:54 - Created broadcast 13 from broadcast at DAGScheduler.scala:1039
2018-04-26 01:59:53 INFO  DAGScheduler:54 - Submitting 5 missing tasks from ResultStage 13 (PythonRDD[40] at call at /home/rjha/spark-2.3.0-bin-hadoop2.7/python/lib/py4j-0.10.6-src.zip/py4j/java_gateway.py:2257) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4))
2018-04-26 01:59:53 INFO  TaskSchedulerImpl:54 - Adding task set 13.0 with 5 tasks
2018-04-26 01:59:53 INFO  TaskSetManager:54 - Starting task 0.0 in stage 13.0 (TID 53, localhost, executor driver, partition 0, ANY, 7772 bytes)
2018-04-26 01:59:53 INFO  Executor:54 - Running task 0.0 in stage 13.0 (TID 53)
2018-04-26 01:59:53 INFO  BlockManager:54 - Found block input-0-1524722386800 locally
2018-04-26 01:59:53 INFO  PythonRunner:54 - Times: total = 48, boot = -142, init = 190, finish = 0
2018-04-26 01:59:53 INFO  PythonRunner:54 - Times: total = 64, boot = -128, init = 177, finish = 15
2018-04-26 01:59:53 INFO  Executor:54 - Finished task 0.0 in stage 13.0 (TID 53). 1267 bytes result sent to driver
2018-04-26 01:59:53 INFO  TaskSetManager:54 - Starting task 1.0 in stage 13.0 (TID 54, localhost, executor driver, partition 1, ANY, 7772 bytes)
2018-04-26 01:59:53 INFO  Executor:54 - Running task 1.0 in stage 13.0 (TID 54)
2018-04-26 01:59:53 INFO  TaskSetManager:54 - Finished task 0.0 in stage 13.0 (TID 53) in 82 ms on localhost (executor driver) (1/5)
2018-04-26 01:59:53 INFO  BlockManager:54 - Found block input-0-1524722387000 locally
2018-04-26 01:59:53 INFO  MemoryStore:54 - Block input-0-1524722393000 stored as bytes in memory (estimated size 451.0 B, free 413.8 MB)
2018-04-26 01:59:53 INFO  BlockManagerInfo:54 - Added input-0-1524722393000 in memory on 10.0.2.15:33681 (size: 451.0 B, free: 413.9 MB)
2018-04-26 01:59:53 WARN  RandomBlockReplicationPolicy:66 - Expecting 1 replicas with only 0 peer/s.
2018-04-26 01:59:53 WARN  BlockManager:66 - Block input-0-1524722393000 replicated to only 0 peer(s) instead of 1 peers
2018-04-26 01:59:53 INFO  BlockGenerator:54 - Pushed block input-0-1524722393000
2018-04-26 01:59:53 INFO  MemoryStore:54 - Block input-0-1524722393200 stored as bytes in memory (estimated size 213.0 B, free 413.8 MB)
2018-04-26 01:59:53 INFO  BlockManagerInfo:54 - Added input-0-1524722393200 in memory on 10.0.2.15:33681 (size: 213.0 B, free: 413.9 MB)
2018-04-26 01:59:53 WARN  RandomBlockReplicationPolicy:66 - Expecting 1 replicas with only 0 peer/s.
2018-04-26 01:59:53 WARN  BlockManager:66 - Block input-0-1524722393200 replicated to only 0 peer(s) instead of 1 peers
2018-04-26 01:59:53 INFO  BlockGenerator:54 - Pushed block input-0-1524722393200
2018-04-26 01:59:53 INFO  PythonRunner:54 - Times: total = 79, boot = -48, init = 127, finish = 0
2018-04-26 01:59:53 INFO  PythonRunner:54 - Times: total = 53, boot = -82, init = 130, finish = 5
2018-04-26 01:59:53 INFO  Executor:54 - Finished task 1.0 in stage 13.0 (TID 54). 1267 bytes result sent to driver
2018-04-26 01:59:53 INFO  TaskSetManager:54 - Starting task 2.0 in stage 13.0 (TID 55, localhost, executor driver, partition 2, ANY, 7772 bytes)
2018-04-26 01:59:53 INFO  Executor:54 - Running task 2.0 in stage 13.0 (TID 55)
2018-04-26 01:59:53 INFO  TaskSetManager:54 - Finished task 1.0 in stage 13.0 (TID 54) in 336 ms on localhost (executor driver) (2/5)
2018-04-26 01:59:53 INFO  BlockManager:54 - Found block input-0-1524722387200 locally
2018-04-26 01:59:53 INFO  MemoryStore:54 - Block input-0-1524722393400 stored as bytes in memory (estimated size 405.0 B, free 413.8 MB)
2018-04-26 01:59:53 INFO  PythonRunner:54 - Times: total = 49, boot = 9, init = 39, finish = 1
2018-04-26 01:59:53 INFO  BlockManagerInfo:54 - Added input-0-1524722393400 in memory on 10.0.2.15:33681 (size: 405.0 B, free: 413.9 MB)
2018-04-26 01:59:53 WARN  RandomBlockReplicationPolicy:66 - Expecting 1 replicas with only 0 peer/s.
2018-04-26 01:59:53 WARN  BlockManager:66 - Block input-0-1524722393400 replicated to only 0 peer(s) instead of 1 peers
2018-04-26 01:59:53 INFO  PythonRunner:54 - Times: total = 63, boot = 14, init = 39, finish = 10
2018-04-26 01:59:53 INFO  Executor:54 - Finished task 2.0 in stage 13.0 (TID 55). 1267 bytes result sent to driver
2018-04-26 01:59:53 INFO  TaskSetManager:54 - Starting task 3.0 in stage 13.0 (TID 56, localhost, executor driver, partition 3, ANY, 7772 bytes)
2018-04-26 01:59:53 INFO  TaskSetManager:54 - Finished task 2.0 in stage 13.0 (TID 55) in 147 ms on localhost (executor driver) (3/5)
2018-04-26 01:59:53 INFO  Executor:54 - Running task 3.0 in stage 13.0 (TID 56)
2018-04-26 01:59:53 INFO  BlockManager:54 - Found block input-0-1524722387400 locally
2018-04-26 01:59:53 INFO  BlockGenerator:54 - Pushed block input-0-1524722393400
2018-04-26 01:59:53 INFO  PythonRunner:54 - Times: total = 61, boot = 42, init = 18, finish = 1
2018-04-26 01:59:53 INFO  PythonRunner:54 - Times: total = 101, boot = 45, init = 17, finish = 39
2018-04-26 01:59:53 INFO  Executor:54 - Finished task 3.0 in stage 13.0 (TID 56). 1267 bytes result sent to driver
2018-04-26 01:59:53 INFO  TaskSetManager:54 - Starting task 4.0 in stage 13.0 (TID 57, localhost, executor driver, partition 4, ANY, 7772 bytes)
2018-04-26 01:59:53 INFO  TaskSetManager:54 - Finished task 3.0 in stage 13.0 (TID 56) in 138 ms on localhost (executor driver) (4/5)
2018-04-26 01:59:53 INFO  Executor:54 - Running task 4.0 in stage 13.0 (TID 57)
2018-04-26 01:59:53 INFO  BlockManager:54 - Found block input-0-1524722387600 locally
2018-04-26 01:59:53 INFO  MemoryStore:54 - Block input-0-1524722393600 stored as bytes in memory (estimated size 404.0 B, free 413.8 MB)
2018-04-26 01:59:53 INFO  BlockManagerInfo:54 - Added input-0-1524722393600 in memory on 10.0.2.15:33681 (size: 404.0 B, free: 413.9 MB)
2018-04-26 01:59:53 WARN  RandomBlockReplicationPolicy:66 - Expecting 1 replicas with only 0 peer/s.
2018-04-26 01:59:53 WARN  BlockManager:66 - Block input-0-1524722393600 replicated to only 0 peer(s) instead of 1 peers
2018-04-26 01:59:53 INFO  BlockGenerator:54 - Pushed block input-0-1524722393600
2018-04-26 01:59:53 INFO  PythonRunner:54 - Times: total = 59, boot = 11, init = 48, finish = 0
2018-04-26 01:59:53 INFO  PythonRunner:54 - Times: total = 56, boot = 11, init = 42, finish = 3
2018-04-26 01:59:53 INFO  Executor:54 - Finished task 4.0 in stage 13.0 (TID 57). 1267 bytes result sent to driver
2018-04-26 01:59:53 INFO  TaskSetManager:54 - Finished task 4.0 in stage 13.0 (TID 57) in 121 ms on localhost (executor driver) (5/5)
2018-04-26 01:59:53 INFO  TaskSchedulerImpl:54 - Removed TaskSet 13.0, whose tasks have all completed, from pool 
2018-04-26 01:59:53 INFO  DAGScheduler:54 - ResultStage 13 (call at /home/rjha/spark-2.3.0-bin-hadoop2.7/python/lib/py4j-0.10.6-src.zip/py4j/java_gateway.py:2257) finished in 0.826 s
2018-04-26 01:59:53 INFO  DAGScheduler:54 - Job 15 finished: call at /home/rjha/spark-2.3.0-bin-hadoop2.7/python/lib/py4j-0.10.6-src.zip/py4j/java_gateway.py:2257, took 0.836302 s
2018-04-26 01:59:53 INFO  JobScheduler:54 - Finished job streaming job 1524722388000 ms.0 from job set of time 1524722388000 ms
2018-04-26 01:59:53 INFO  JobScheduler:54 - Starting job streaming job 1524722388000 ms.1 from job set of time 1524722388000 ms
2018-04-26 01:59:54 INFO  MemoryStore:54 - Block input-0-1524722393800 stored as bytes in memory (estimated size 339.0 B, free 413.8 MB)
2018-04-26 01:59:54 INFO  BlockManagerInfo:54 - Added input-0-1524722393800 in memory on 10.0.2.15:33681 (size: 339.0 B, free: 413.9 MB)
2018-04-26 01:59:54 WARN  RandomBlockReplicationPolicy:66 - Expecting 1 replicas with only 0 peer/s.
2018-04-26 01:59:54 WARN  BlockManager:66 - Block input-0-1524722393800 replicated to only 0 peer(s) instead of 1 peers
2018-04-26 01:59:54 INFO  BlockGenerator:54 - Pushed block input-0-1524722393800
2018-04-26 01:59:54 INFO  SparkContext:54 - Starting job: call at /home/rjha/spark-2.3.0-bin-hadoop2.7/python/lib/py4j-0.10.6-src.zip/py4j/java_gateway.py:2257
2018-04-26 01:59:54 INFO  JobScheduler:54 - Added jobs for time 1524722394000 ms
2018-04-26 01:59:54 INFO  DAGScheduler:54 - Got job 16 (call at /home/rjha/spark-2.3.0-bin-hadoop2.7/python/lib/py4j-0.10.6-src.zip/py4j/java_gateway.py:2257) with 5 output partitions
2018-04-26 01:59:54 INFO  DAGScheduler:54 - Final stage: ResultStage 14 (call at /home/rjha/spark-2.3.0-bin-hadoop2.7/python/lib/py4j-0.10.6-src.zip/py4j/java_gateway.py:2257)
2018-04-26 01:59:54 INFO  DAGScheduler:54 - Parents of final stage: List()
2018-04-26 01:59:54 INFO  DAGScheduler:54 - Missing parents: List()
2018-04-26 01:59:54 INFO  DAGScheduler:54 - Submitting ResultStage 14 (PythonRDD[42] at call at /home/rjha/spark-2.3.0-bin-hadoop2.7/python/lib/py4j-0.10.6-src.zip/py4j/java_gateway.py:2257), which has no missing parents
2018-04-26 01:59:54 INFO  MemoryStore:54 - Block broadcast_14 stored as values in memory (estimated size 7.1 KB, free 413.8 MB)
2018-04-26 01:59:54 INFO  ContextCleaner:54 - Cleaned accumulator 344
2018-04-26 01:59:54 INFO  ContextCleaner:54 - Cleaned accumulator 346
2018-04-26 01:59:54 INFO  ContextCleaner:54 - Cleaned accumulator 329
2018-04-26 01:59:54 INFO  ContextCleaner:54 - Cleaned accumulator 332
2018-04-26 01:59:54 INFO  ContextCleaner:54 - Cleaned accumulator 337
2018-04-26 01:59:54 INFO  ContextCleaner:54 - Cleaned accumulator 326
2018-04-26 01:59:54 INFO  ContextCleaner:54 - Cleaned accumulator 342
2018-04-26 01:59:54 INFO  ContextCleaner:54 - Cleaned accumulator 330
2018-04-26 01:59:54 INFO  BlockManagerInfo:54 - Removed broadcast_13_piece0 on 10.0.2.15:33681 in memory (size: 4.4 KB, free: 413.9 MB)
2018-04-26 01:59:54 INFO  ContextCleaner:54 - Cleaned accumulator 350
2018-04-26 01:59:54 INFO  ContextCleaner:54 - Cleaned accumulator 328
2018-04-26 01:59:54 INFO  ContextCleaner:54 - Cleaned accumulator 340
2018-04-26 01:59:54 INFO  ContextCleaner:54 - Cleaned accumulator 335
2018-04-26 01:59:54 INFO  ContextCleaner:54 - Cleaned accumulator 334
2018-04-26 01:59:54 INFO  ContextCleaner:54 - Cleaned accumulator 341
2018-04-26 01:59:54 INFO  ContextCleaner:54 - Cleaned accumulator 333
2018-04-26 01:59:54 INFO  ContextCleaner:54 - Cleaned accumulator 348
2018-04-26 01:59:54 INFO  ContextCleaner:54 - Cleaned accumulator 327
2018-04-26 01:59:54 INFO  ContextCleaner:54 - Cleaned accumulator 338
2018-04-26 01:59:54 INFO  ContextCleaner:54 - Cleaned accumulator 343
2018-04-26 01:59:54 INFO  ContextCleaner:54 - Cleaned accumulator 336
2018-04-26 01:59:54 INFO  ContextCleaner:54 - Cleaned accumulator 347
2018-04-26 01:59:54 INFO  ContextCleaner:54 - Cleaned accumulator 339
2018-04-26 01:59:54 INFO  ContextCleaner:54 - Cleaned accumulator 349
2018-04-26 01:59:54 INFO  ContextCleaner:54 - Cleaned accumulator 331
2018-04-26 01:59:54 INFO  ContextCleaner:54 - Cleaned accumulator 345
2018-04-26 01:59:54 INFO  MemoryStore:54 - Block broadcast_14_piece0 stored as bytes in memory (estimated size 4.1 KB, free 413.8 MB)
2018-04-26 01:59:54 INFO  BlockManagerInfo:54 - Added broadcast_14_piece0 in memory on 10.0.2.15:33681 (size: 4.1 KB, free: 413.9 MB)
2018-04-26 01:59:54 INFO  SparkContext:54 - Created broadcast 14 from broadcast at DAGScheduler.scala:1039
2018-04-26 01:59:54 INFO  DAGScheduler:54 - Submitting 5 missing tasks from ResultStage 14 (PythonRDD[42] at call at /home/rjha/spark-2.3.0-bin-hadoop2.7/python/lib/py4j-0.10.6-src.zip/py4j/java_gateway.py:2257) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4))
2018-04-26 01:59:54 INFO  TaskSchedulerImpl:54 - Adding task set 14.0 with 5 tasks
2018-04-26 01:59:54 INFO  TaskSetManager:54 - Starting task 0.0 in stage 14.0 (TID 58, localhost, executor driver, partition 0, ANY, 7772 bytes)
2018-04-26 01:59:54 INFO  Executor:54 - Running task 0.0 in stage 14.0 (TID 58)
2018-04-26 01:59:54 INFO  BlockManager:54 - Found block input-0-1524722386800 locally
2018-04-26 01:59:54 INFO  MemoryStore:54 - Block input-0-1524722394000 stored as bytes in memory (estimated size 387.0 B, free 413.8 MB)
2018-04-26 01:59:54 INFO  BlockManagerInfo:54 - Added input-0-1524722394000 in memory on 10.0.2.15:33681 (size: 387.0 B, free: 413.9 MB)
2018-04-26 01:59:54 WARN  RandomBlockReplicationPolicy:66 - Expecting 1 replicas with only 0 peer/s.
2018-04-26 01:59:54 WARN  BlockManager:66 - Block input-0-1524722394000 replicated to only 0 peer(s) instead of 1 peers
2018-04-26 01:59:54 INFO  BlockGenerator:54 - Pushed block input-0-1524722394000
2018-04-26 01:59:54 INFO  PythonRunner:54 - Times: total = 45, boot = -174, init = 218, finish = 1
2018-04-26 01:59:54 INFO  PythonRunner:54 - Times: total = 44, boot = -128, init = 172, finish = 0
2018-04-26 01:59:54 INFO  Executor:54 - Finished task 0.0 in stage 14.0 (TID 58). 1267 bytes result sent to driver
2018-04-26 01:59:54 INFO  TaskSetManager:54 - Starting task 1.0 in stage 14.0 (TID 59, localhost, executor driver, partition 1, ANY, 7772 bytes)
2018-04-26 01:59:54 INFO  TaskSetManager:54 - Finished task 0.0 in stage 14.0 (TID 58) in 67 ms on localhost (executor driver) (1/5)
2018-04-26 01:59:54 INFO  Executor:54 - Running task 1.0 in stage 14.0 (TID 59)
2018-04-26 01:59:54 INFO  BlockManager:54 - Found block input-0-1524722387000 locally
2018-04-26 01:59:54 INFO  PythonRunner:54 - Times: total = 65, boot = 7, init = 58, finish = 0
2018-04-26 01:59:54 INFO  PythonRunner:54 - Times: total = 57, boot = 2, init = 55, finish = 0
2018-04-26 01:59:54 INFO  Executor:54 - Finished task 1.0 in stage 14.0 (TID 59). 1267 bytes result sent to driver
2018-04-26 01:59:54 INFO  TaskSetManager:54 - Starting task 2.0 in stage 14.0 (TID 60, localhost, executor driver, partition 2, ANY, 7772 bytes)
2018-04-26 01:59:54 INFO  TaskSetManager:54 - Finished task 1.0 in stage 14.0 (TID 59) in 83 ms on localhost (executor driver) (2/5)
2018-04-26 01:59:54 INFO  Executor:54 - Running task 2.0 in stage 14.0 (TID 60)
2018-04-26 01:59:54 INFO  BlockManager:54 - Found block input-0-1524722387200 locally
2018-04-26 01:59:54 INFO  PythonRunner:54 - Times: total = 59, boot = 24, init = 35, finish = 0
2018-04-26 01:59:54 INFO  PythonRunner:54 - Times: total = 51, boot = 13, init = 38, finish = 0
2018-04-26 01:59:54 INFO  Executor:54 - Finished task 2.0 in stage 14.0 (TID 60). 1267 bytes result sent to driver
2018-04-26 01:59:54 INFO  TaskSetManager:54 - Starting task 3.0 in stage 14.0 (TID 61, localhost, executor driver, partition 3, ANY, 7772 bytes)
2018-04-26 01:59:54 INFO  Executor:54 - Running task 3.0 in stage 14.0 (TID 61)
2018-04-26 01:59:54 INFO  TaskSetManager:54 - Finished task 2.0 in stage 14.0 (TID 60) in 81 ms on localhost (executor driver) (3/5)
2018-04-26 01:59:54 INFO  BlockManager:54 - Found block input-0-1524722387400 locally
2018-04-26 01:59:54 INFO  MemoryStore:54 - Block input-0-1524722394200 stored as bytes in memory (estimated size 421.0 B, free 413.8 MB)
2018-04-26 01:59:54 INFO  BlockManagerInfo:54 - Added input-0-1524722394200 in memory on 10.0.2.15:33681 (size: 421.0 B, free: 413.9 MB)
2018-04-26 01:59:54 WARN  RandomBlockReplicationPolicy:66 - Expecting 1 replicas with only 0 peer/s.
2018-04-26 01:59:54 WARN  BlockManager:66 - Block input-0-1524722394200 replicated to only 0 peer(s) instead of 1 peers
2018-04-26 01:59:54 INFO  BlockGenerator:54 - Pushed block input-0-1524722394200
2018-04-26 01:59:54 INFO  PythonRunner:54 - Times: total = 111, boot = 110, init = 1, finish = 0
2018-04-26 01:59:54 INFO  PythonRunner:54 - Times: total = 58, boot = 5, init = 53, finish = 0
2018-04-26 01:59:54 INFO  Executor:54 - Finished task 3.0 in stage 14.0 (TID 61). 1267 bytes result sent to driver
2018-04-26 01:59:54 INFO  TaskSetManager:54 - Starting task 4.0 in stage 14.0 (TID 62, localhost, executor driver, partition 4, ANY, 7772 bytes)
2018-04-26 01:59:54 INFO  TaskSetManager:54 - Finished task 3.0 in stage 14.0 (TID 61) in 125 ms on localhost (executor driver) (4/5)
2018-04-26 01:59:54 INFO  Executor:54 - Running task 4.0 in stage 14.0 (TID 62)
2018-04-26 01:59:54 INFO  BlockManager:54 - Found block input-0-1524722387600 locally
2018-04-26 01:59:54 INFO  MemoryStore:54 - Block input-0-1524722394400 stored as bytes in memory (estimated size 318.0 B, free 413.8 MB)
2018-04-26 01:59:54 INFO  BlockManagerInfo:54 - Added input-0-1524722394400 in memory on 10.0.2.15:33681 (size: 318.0 B, free: 413.9 MB)
2018-04-26 01:59:54 WARN  RandomBlockReplicationPolicy:66 - Expecting 1 replicas with only 0 peer/s.
2018-04-26 01:59:54 WARN  BlockManager:66 - Block input-0-1524722394400 replicated to only 0 peer(s) instead of 1 peers
2018-04-26 01:59:54 INFO  BlockGenerator:54 - Pushed block input-0-1524722394400
2018-04-26 01:59:54 INFO  PythonRunner:54 - Times: total = 212, boot = 211, init = 0, finish = 1
2018-04-26 01:59:54 INFO  PythonRunner:54 - Times: total = 193, boot = 20, init = 173, finish = 0
2018-04-26 01:59:54 INFO  Executor:54 - Finished task 4.0 in stage 14.0 (TID 62). 1267 bytes result sent to driver
2018-04-26 01:59:54 INFO  TaskSetManager:54 - Finished task 4.0 in stage 14.0 (TID 62) in 239 ms on localhost (executor driver) (5/5)
2018-04-26 01:59:54 INFO  TaskSchedulerImpl:54 - Removed TaskSet 14.0, whose tasks have all completed, from pool 
2018-04-26 01:59:54 INFO  DAGScheduler:54 - ResultStage 14 (call at /home/rjha/spark-2.3.0-bin-hadoop2.7/python/lib/py4j-0.10.6-src.zip/py4j/java_gateway.py:2257) finished in 0.617 s
2018-04-26 01:59:54 INFO  DAGScheduler:54 - Job 16 finished: call at /home/rjha/spark-2.3.0-bin-hadoop2.7/python/lib/py4j-0.10.6-src.zip/py4j/java_gateway.py:2257, took 0.630331 s
2018-04-26 01:59:54 INFO  JobScheduler:54 - Finished job streaming job 1524722388000 ms.1 from job set of time 1524722388000 ms
2018-04-26 01:59:54 INFO  JobScheduler:54 - Total delay: 6.766 s for time 1524722388000 ms (execution: 1.843 s)
2018-04-26 01:59:54 INFO  JobScheduler:54 - Starting job streaming job 1524722389000 ms.0 from job set of time 1524722389000 ms
2018-04-26 01:59:54 INFO  PythonRDD:54 - Removing RDD 21 from persistence list
2018-04-26 01:59:54 INFO  BlockManager:54 - Removing RDD 21
2018-04-26 01:59:54 INFO  BlockRDD:54 - Removing RDD 20 from persistence list
2018-04-26 01:59:54 INFO  BlockManager:54 - Removing RDD 20
2018-04-26 01:59:54 INFO  SocketInputDStream:54 - Removing blocks of RDD BlockRDD[20] at socketTextStream at NativeMethodAccessorImpl.java:0 of time 1524722388000 ms
2018-04-26 01:59:54 INFO  BlockManagerInfo:54 - Removed input-0-1524722386000 on 10.0.2.15:33681 in memory (size: 403.0 B, free: 413.9 MB)
2018-04-26 01:59:54 INFO  BlockManagerInfo:54 - Removed input-0-1524722386200 on 10.0.2.15:33681 in memory (size: 399.0 B, free: 413.9 MB)
2018-04-26 01:59:54 INFO  BlockManagerInfo:54 - Removed input-0-1524722385800 on 10.0.2.15:33681 in memory (size: 324.0 B, free: 413.9 MB)
2018-04-26 01:59:54 INFO  BlockManagerInfo:54 - Removed input-0-1524722386400 on 10.0.2.15:33681 in memory (size: 389.0 B, free: 413.9 MB)
2018-04-26 01:59:54 INFO  MemoryStore:54 - Block input-0-1524722394600 stored as bytes in memory (estimated size 293.0 B, free 413.8 MB)
2018-04-26 01:59:54 INFO  ReceivedBlockTracker:54 - Deleting batches: 1524722386000 ms
2018-04-26 01:59:54 INFO  InputInfoTracker:54 - remove old batch metadata: 1524722386000 ms
2018-04-26 01:59:54 INFO  BlockManagerInfo:54 - Added input-0-1524722394600 in memory on 10.0.2.15:33681 (size: 293.0 B, free: 413.9 MB)
2018-04-26 01:59:54 WARN  RandomBlockReplicationPolicy:66 - Expecting 1 replicas with only 0 peer/s.
2018-04-26 01:59:54 WARN  BlockManager:66 - Block input-0-1524722394600 replicated to only 0 peer(s) instead of 1 peers
2018-04-26 01:59:54 INFO  BlockGenerator:54 - Pushed block input-0-1524722394600
2018-04-26 01:59:54 INFO  BlockManagerInfo:54 - Removed input-0-1524722386600 on 10.0.2.15:33681 in memory (size: 401.0 B, free: 413.9 MB)
2018-04-26 01:59:54 INFO  SparkContext:54 - Starting job: call at /home/rjha/spark-2.3.0-bin-hadoop2.7/python/lib/py4j-0.10.6-src.zip/py4j/java_gateway.py:2257
2018-04-26 01:59:54 INFO  DAGScheduler:54 - Got job 17 (call at /home/rjha/spark-2.3.0-bin-hadoop2.7/python/lib/py4j-0.10.6-src.zip/py4j/java_gateway.py:2257) with 5 output partitions
2018-04-26 01:59:54 INFO  DAGScheduler:54 - Final stage: ResultStage 15 (call at /home/rjha/spark-2.3.0-bin-hadoop2.7/python/lib/py4j-0.10.6-src.zip/py4j/java_gateway.py:2257)
2018-04-26 01:59:54 INFO  DAGScheduler:54 - Parents of final stage: List()
2018-04-26 01:59:54 INFO  DAGScheduler:54 - Missing parents: List()
2018-04-26 01:59:54 INFO  DAGScheduler:54 - Submitting ResultStage 15 (PythonRDD[45] at call at /home/rjha/spark-2.3.0-bin-hadoop2.7/python/lib/py4j-0.10.6-src.zip/py4j/java_gateway.py:2257), which has no missing parents
2018-04-26 01:59:54 INFO  MemoryStore:54 - Block broadcast_15 stored as values in memory (estimated size 7.5 KB, free 413.8 MB)
2018-04-26 01:59:54 INFO  ContextCleaner:54 - Cleaned accumulator 365
2018-04-26 01:59:54 INFO  ContextCleaner:54 - Cleaned accumulator 357
2018-04-26 01:59:54 INFO  ContextCleaner:54 - Cleaned accumulator 351
2018-04-26 01:59:54 INFO  ContextCleaner:54 - Cleaned accumulator 364
2018-04-26 01:59:54 INFO  ContextCleaner:54 - Cleaned accumulator 372
2018-04-26 01:59:54 INFO  ContextCleaner:54 - Cleaned accumulator 359
2018-04-26 01:59:54 INFO  ContextCleaner:54 - Cleaned accumulator 362
2018-04-26 01:59:54 INFO  ContextCleaner:54 - Cleaned accumulator 369
2018-04-26 01:59:54 INFO  ContextCleaner:54 - Cleaned accumulator 371
2018-04-26 01:59:54 INFO  ContextCleaner:54 - Cleaned accumulator 370
2018-04-26 01:59:54 INFO  ContextCleaner:54 - Cleaned accumulator 352
2018-04-26 01:59:54 INFO  ContextCleaner:54 - Cleaned accumulator 356
2018-04-26 01:59:54 INFO  ContextCleaner:54 - Cleaned accumulator 354
2018-04-26 01:59:54 INFO  ContextCleaner:54 - Cleaned accumulator 367
2018-04-26 01:59:54 INFO  ContextCleaner:54 - Cleaned accumulator 366
2018-04-26 01:59:54 INFO  BlockManagerInfo:54 - Removed broadcast_14_piece0 on 10.0.2.15:33681 in memory (size: 4.1 KB, free: 413.9 MB)
2018-04-26 01:59:54 INFO  ContextCleaner:54 - Cleaned accumulator 363
2018-04-26 01:59:54 INFO  ContextCleaner:54 - Cleaned accumulator 358
2018-04-26 01:59:54 INFO  ContextCleaner:54 - Cleaned accumulator 360
2018-04-26 01:59:54 INFO  ContextCleaner:54 - Cleaned accumulator 368
2018-04-26 01:59:54 INFO  ContextCleaner:54 - Cleaned accumulator 353
2018-04-26 01:59:54 INFO  ContextCleaner:54 - Cleaned accumulator 361
2018-04-26 01:59:54 INFO  ContextCleaner:54 - Cleaned accumulator 355
2018-04-26 01:59:54 INFO  ContextCleaner:54 - Cleaned accumulator 374
2018-04-26 01:59:54 INFO  ContextCleaner:54 - Cleaned accumulator 375
2018-04-26 01:59:54 INFO  ContextCleaner:54 - Cleaned accumulator 373
2018-04-26 01:59:54 INFO  MemoryStore:54 - Block broadcast_15_piece0 stored as bytes in memory (estimated size 4.4 KB, free 413.8 MB)
2018-04-26 01:59:54 INFO  BlockManagerInfo:54 - Added broadcast_15_piece0 in memory on 10.0.2.15:33681 (size: 4.4 KB, free: 413.9 MB)
2018-04-26 01:59:54 INFO  SparkContext:54 - Created broadcast 15 from broadcast at DAGScheduler.scala:1039
2018-04-26 01:59:54 INFO  DAGScheduler:54 - Submitting 5 missing tasks from ResultStage 15 (PythonRDD[45] at call at /home/rjha/spark-2.3.0-bin-hadoop2.7/python/lib/py4j-0.10.6-src.zip/py4j/java_gateway.py:2257) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4))
2018-04-26 01:59:54 INFO  TaskSchedulerImpl:54 - Adding task set 15.0 with 5 tasks
2018-04-26 01:59:54 INFO  TaskSetManager:54 - Starting task 0.0 in stage 15.0 (TID 63, localhost, executor driver, partition 0, ANY, 7772 bytes)
2018-04-26 01:59:54 INFO  Executor:54 - Running task 0.0 in stage 15.0 (TID 63)
2018-04-26 01:59:54 INFO  BlockManager:54 - Found block input-0-1524722387800 locally
2018-04-26 01:59:54 INFO  PythonRunner:54 - Times: total = 6, boot = -61, init = 67, finish = 0
2018-04-26 01:59:54 INFO  PythonRunner:54 - Times: total = 55, boot = -77, init = 124, finish = 8
2018-04-26 01:59:54 INFO  Executor:54 - Finished task 0.0 in stage 15.0 (TID 63). 1267 bytes result sent to driver
2018-04-26 01:59:54 INFO  TaskSetManager:54 - Starting task 1.0 in stage 15.0 (TID 64, localhost, executor driver, partition 1, ANY, 7772 bytes)
2018-04-26 01:59:54 INFO  Executor:54 - Running task 1.0 in stage 15.0 (TID 64)
2018-04-26 01:59:54 INFO  TaskSetManager:54 - Finished task 0.0 in stage 15.0 (TID 63) in 78 ms on localhost (executor driver) (1/5)
2018-04-26 01:59:54 INFO  BlockManager:54 - Found block input-0-1524722388000 locally
2018-04-26 01:59:55 INFO  MemoryStore:54 - Block input-0-1524722394800 stored as bytes in memory (estimated size 435.0 B, free 413.8 MB)
2018-04-26 01:59:55 INFO  BlockManagerInfo:54 - Added input-0-1524722394800 in memory on 10.0.2.15:33681 (size: 435.0 B, free: 413.9 MB)
2018-04-26 01:59:55 WARN  RandomBlockReplicationPolicy:66 - Expecting 1 replicas with only 0 peer/s.
2018-04-26 01:59:55 WARN  BlockManager:66 - Block input-0-1524722394800 replicated to only 0 peer(s) instead of 1 peers
2018-04-26 01:59:55 INFO  BlockGenerator:54 - Pushed block input-0-1524722394800
2018-04-26 01:59:55 INFO  PythonRunner:54 - Times: total = 88, boot = -67, init = 154, finish = 1
2018-04-26 01:59:55 INFO  JobScheduler:54 - Added jobs for time 1524722395000 ms
2018-04-26 01:59:55 INFO  PythonRunner:54 - Times: total = 79, boot = 56, init = 19, finish = 4
2018-04-26 01:59:55 INFO  Executor:54 - Finished task 1.0 in stage 15.0 (TID 64). 1267 bytes result sent to driver
2018-04-26 01:59:55 INFO  TaskSetManager:54 - Starting task 2.0 in stage 15.0 (TID 65, localhost, executor driver, partition 2, ANY, 7772 bytes)
2018-04-26 01:59:55 INFO  Executor:54 - Running task 2.0 in stage 15.0 (TID 65)
2018-04-26 01:59:55 INFO  TaskSetManager:54 - Finished task 1.0 in stage 15.0 (TID 64) in 145 ms on localhost (executor driver) (2/5)
2018-04-26 01:59:55 INFO  BlockManager:54 - Found block input-0-1524722388200 locally
2018-04-26 01:59:55 INFO  MemoryStore:54 - Block input-0-1524722395000 stored as bytes in memory (estimated size 340.0 B, free 413.8 MB)
2018-04-26 01:59:55 INFO  BlockManagerInfo:54 - Added input-0-1524722395000 in memory on 10.0.2.15:33681 (size: 340.0 B, free: 413.9 MB)
2018-04-26 01:59:55 WARN  RandomBlockReplicationPolicy:66 - Expecting 1 replicas with only 0 peer/s.
2018-04-26 01:59:55 INFO  PythonRunner:54 - Times: total = 145, boot = -38, init = 183, finish = 0
2018-04-26 01:59:55 WARN  BlockManager:66 - Block input-0-1524722395000 replicated to only 0 peer(s) instead of 1 peers
2018-04-26 01:59:55 INFO  BlockGenerator:54 - Pushed block input-0-1524722395000
2018-04-26 01:59:55 INFO  PythonRunner:54 - Times: total = 168, boot = 142, init = 1, finish = 25
2018-04-26 01:59:55 INFO  MemoryStore:54 - Block input-0-1524722395200 stored as bytes in memory (estimated size 140.0 B, free 413.8 MB)
2018-04-26 01:59:55 INFO  BlockManagerInfo:54 - Added input-0-1524722395200 in memory on 10.0.2.15:33681 (size: 140.0 B, free: 413.9 MB)
2018-04-26 01:59:55 WARN  RandomBlockReplicationPolicy:66 - Expecting 1 replicas with only 0 peer/s.
2018-04-26 01:59:55 WARN  BlockManager:66 - Block input-0-1524722395200 replicated to only 0 peer(s) instead of 1 peers
2018-04-26 01:59:55 INFO  BlockGenerator:54 - Pushed block input-0-1524722395200
2018-04-26 01:59:55 INFO  Executor:54 - Finished task 2.0 in stage 15.0 (TID 65). 1267 bytes result sent to driver
2018-04-26 01:59:55 INFO  TaskSetManager:54 - Starting task 3.0 in stage 15.0 (TID 66, localhost, executor driver, partition 3, ANY, 7772 bytes)
2018-04-26 01:59:55 INFO  TaskSetManager:54 - Finished task 2.0 in stage 15.0 (TID 65) in 428 ms on localhost (executor driver) (3/5)
2018-04-26 01:59:55 INFO  Executor:54 - Running task 3.0 in stage 15.0 (TID 66)
2018-04-26 01:59:55 INFO  BlockManager:54 - Found block input-0-1524722388400 locally
2018-04-26 01:59:55 INFO  PythonRunner:54 - Times: total = 50, boot = -125, init = 174, finish = 1
2018-04-26 01:59:55 INFO  PythonRunner:54 - Times: total = 59, boot = -115, init = 169, finish = 5
2018-04-26 01:59:55 INFO  Executor:54 - Finished task 3.0 in stage 15.0 (TID 66). 1267 bytes result sent to driver
2018-04-26 01:59:55 INFO  TaskSetManager:54 - Starting task 4.0 in stage 15.0 (TID 67, localhost, executor driver, partition 4, ANY, 7772 bytes)
2018-04-26 01:59:55 INFO  TaskSetManager:54 - Finished task 3.0 in stage 15.0 (TID 66) in 67 ms on localhost (executor driver) (4/5)
2018-04-26 01:59:55 INFO  Executor:54 - Running task 4.0 in stage 15.0 (TID 67)
2018-04-26 01:59:55 INFO  MemoryStore:54 - Block input-0-1524722395400 stored as bytes in memory (estimated size 312.0 B, free 413.8 MB)
2018-04-26 01:59:55 INFO  BlockManagerInfo:54 - Added input-0-1524722395400 in memory on 10.0.2.15:33681 (size: 312.0 B, free: 413.9 MB)
2018-04-26 01:59:55 WARN  RandomBlockReplicationPolicy:66 - Expecting 1 replicas with only 0 peer/s.
2018-04-26 01:59:55 WARN  BlockManager:66 - Block input-0-1524722395400 replicated to only 0 peer(s) instead of 1 peers
2018-04-26 01:59:55 INFO  BlockManager:54 - Found block input-0-1524722388600 locally
2018-04-26 01:59:55 INFO  BlockGenerator:54 - Pushed block input-0-1524722395400
2018-04-26 01:59:55 INFO  PythonRunner:54 - Times: total = 104, boot = 103, init = 1, finish = 0
2018-04-26 01:59:55 INFO  PythonRunner:54 - Times: total = 108, boot = 99, init = 0, finish = 9
2018-04-26 01:59:55 INFO  Executor:54 - Finished task 4.0 in stage 15.0 (TID 67). 1267 bytes result sent to driver
2018-04-26 01:59:55 INFO  TaskSetManager:54 - Finished task 4.0 in stage 15.0 (TID 67) in 133 ms on localhost (executor driver) (5/5)
2018-04-26 01:59:55 INFO  TaskSchedulerImpl:54 - Removed TaskSet 15.0, whose tasks have all completed, from pool 
2018-04-26 01:59:55 INFO  DAGScheduler:54 - ResultStage 15 (call at /home/rjha/spark-2.3.0-bin-hadoop2.7/python/lib/py4j-0.10.6-src.zip/py4j/java_gateway.py:2257) finished in 0.879 s
2018-04-26 01:59:55 INFO  DAGScheduler:54 - Job 17 finished: call at /home/rjha/spark-2.3.0-bin-hadoop2.7/python/lib/py4j-0.10.6-src.zip/py4j/java_gateway.py:2257, took 0.886522 s
2018-04-26 01:59:55 INFO  JobScheduler:54 - Finished job streaming job 1524722389000 ms.0 from job set of time 1524722389000 ms
2018-04-26 01:59:55 INFO  JobScheduler:54 - Starting job streaming job 1524722389000 ms.1 from job set of time 1524722389000 ms
2018-04-26 01:59:55 INFO  MemoryStore:54 - Block input-0-1524722395600 stored as bytes in memory (estimated size 333.0 B, free 413.8 MB)
2018-04-26 01:59:55 INFO  BlockManagerInfo:54 - Added input-0-1524722395600 in memory on 10.0.2.15:33681 (size: 333.0 B, free: 413.9 MB)
2018-04-26 01:59:55 WARN  RandomBlockReplicationPolicy:66 - Expecting 1 replicas with only 0 peer/s.
2018-04-26 01:59:55 WARN  BlockManager:66 - Block input-0-1524722395600 replicated to only 0 peer(s) instead of 1 peers
2018-04-26 01:59:55 INFO  BlockGenerator:54 - Pushed block input-0-1524722395600
2018-04-26 01:59:55 INFO  SparkContext:54 - Starting job: call at /home/rjha/spark-2.3.0-bin-hadoop2.7/python/lib/py4j-0.10.6-src.zip/py4j/java_gateway.py:2257
2018-04-26 01:59:55 INFO  DAGScheduler:54 - Got job 18 (call at /home/rjha/spark-2.3.0-bin-hadoop2.7/python/lib/py4j-0.10.6-src.zip/py4j/java_gateway.py:2257) with 5 output partitions
2018-04-26 01:59:55 INFO  DAGScheduler:54 - Final stage: ResultStage 16 (call at /home/rjha/spark-2.3.0-bin-hadoop2.7/python/lib/py4j-0.10.6-src.zip/py4j/java_gateway.py:2257)
2018-04-26 01:59:55 INFO  DAGScheduler:54 - Parents of final stage: List()
2018-04-26 01:59:55 INFO  DAGScheduler:54 - Missing parents: List()
2018-04-26 01:59:55 INFO  DAGScheduler:54 - Submitting ResultStage 16 (PythonRDD[48] at call at /home/rjha/spark-2.3.0-bin-hadoop2.7/python/lib/py4j-0.10.6-src.zip/py4j/java_gateway.py:2257), which has no missing parents
2018-04-26 01:59:55 INFO  MemoryStore:54 - Block broadcast_16 stored as values in memory (estimated size 7.1 KB, free 413.8 MB)
2018-04-26 01:59:55 INFO  ContextCleaner:54 - Cleaned accumulator 397
2018-04-26 01:59:55 INFO  ContextCleaner:54 - Cleaned accumulator 387
2018-04-26 01:59:55 INFO  ContextCleaner:54 - Cleaned accumulator 393
2018-04-26 01:59:55 INFO  ContextCleaner:54 - Cleaned accumulator 377
2018-04-26 01:59:55 INFO  ContextCleaner:54 - Cleaned accumulator 400
2018-04-26 01:59:55 INFO  ContextCleaner:54 - Cleaned accumulator 395
2018-04-26 01:59:55 INFO  ContextCleaner:54 - Cleaned accumulator 398
2018-04-26 01:59:55 INFO  ContextCleaner:54 - Cleaned accumulator 386
2018-04-26 01:59:55 INFO  ContextCleaner:54 - Cleaned accumulator 381
2018-04-26 01:59:55 INFO  ContextCleaner:54 - Cleaned accumulator 382
2018-04-26 01:59:55 INFO  ContextCleaner:54 - Cleaned accumulator 388
2018-04-26 01:59:55 INFO  ContextCleaner:54 - Cleaned accumulator 389
2018-04-26 01:59:55 INFO  ContextCleaner:54 - Cleaned accumulator 379
2018-04-26 01:59:55 INFO  ContextCleaner:54 - Cleaned accumulator 391
2018-04-26 01:59:55 INFO  ContextCleaner:54 - Cleaned accumulator 378
2018-04-26 01:59:55 INFO  ContextCleaner:54 - Cleaned accumulator 399
2018-04-26 01:59:55 INFO  ContextCleaner:54 - Cleaned accumulator 380
2018-04-26 01:59:55 INFO  ContextCleaner:54 - Cleaned accumulator 394
2018-04-26 01:59:55 INFO  BlockManagerInfo:54 - Removed broadcast_15_piece0 on 10.0.2.15:33681 in memory (size: 4.4 KB, free: 413.9 MB)
2018-04-26 01:59:55 INFO  ContextCleaner:54 - Cleaned accumulator 385
2018-04-26 01:59:55 INFO  ContextCleaner:54 - Cleaned accumulator 376
2018-04-26 01:59:55 INFO  ContextCleaner:54 - Cleaned accumulator 383
2018-04-26 01:59:55 INFO  ContextCleaner:54 - Cleaned accumulator 384
2018-04-26 01:59:55 INFO  ContextCleaner:54 - Cleaned accumulator 390
2018-04-26 01:59:55 INFO  ContextCleaner:54 - Cleaned accumulator 392
2018-04-26 01:59:55 INFO  ContextCleaner:54 - Cleaned accumulator 396
2018-04-26 01:59:55 INFO  MemoryStore:54 - Block broadcast_16_piece0 stored as bytes in memory (estimated size 4.1 KB, free 413.8 MB)
2018-04-26 01:59:55 INFO  BlockManagerInfo:54 - Added broadcast_16_piece0 in memory on 10.0.2.15:33681 (size: 4.1 KB, free: 413.9 MB)
2018-04-26 01:59:55 INFO  SparkContext:54 - Created broadcast 16 from broadcast at DAGScheduler.scala:1039
2018-04-26 01:59:55 INFO  DAGScheduler:54 - Submitting 5 missing tasks from ResultStage 16 (PythonRDD[48] at call at /home/rjha/spark-2.3.0-bin-hadoop2.7/python/lib/py4j-0.10.6-src.zip/py4j/java_gateway.py:2257) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4))
2018-04-26 01:59:55 INFO  TaskSchedulerImpl:54 - Adding task set 16.0 with 5 tasks
2018-04-26 01:59:55 INFO  TaskSetManager:54 - Starting task 0.0 in stage 16.0 (TID 68, localhost, executor driver, partition 0, ANY, 7772 bytes)
2018-04-26 01:59:55 INFO  Executor:54 - Running task 0.0 in stage 16.0 (TID 68)
2018-04-26 01:59:55 INFO  BlockManager:54 - Found block input-0-1524722387800 locally
2018-04-26 01:59:55 INFO  PythonRunner:54 - Times: total = 2, boot = -151, init = 153, finish = 0
2018-04-26 01:59:55 INFO  PythonRunner:54 - Times: total = 44, boot = -87, init = 131, finish = 0
2018-04-26 01:59:55 INFO  Executor:54 - Finished task 0.0 in stage 16.0 (TID 68). 1224 bytes result sent to driver
2018-04-26 01:59:55 INFO  TaskSetManager:54 - Starting task 1.0 in stage 16.0 (TID 69, localhost, executor driver, partition 1, ANY, 7772 bytes)
2018-04-26 01:59:55 INFO  TaskSetManager:54 - Finished task 0.0 in stage 16.0 (TID 68) in 61 ms on localhost (executor driver) (1/5)
2018-04-26 01:59:55 INFO  Executor:54 - Running task 1.0 in stage 16.0 (TID 69)
2018-04-26 01:59:55 INFO  BlockManager:54 - Found block input-0-1524722388000 locally
2018-04-26 01:59:56 INFO  MemoryStore:54 - Block input-0-1524722395800 stored as bytes in memory (estimated size 408.0 B, free 413.8 MB)
2018-04-26 01:59:56 INFO  BlockManagerInfo:54 - Added input-0-1524722395800 in memory on 10.0.2.15:33681 (size: 408.0 B, free: 413.9 MB)
2018-04-26 01:59:56 WARN  RandomBlockReplicationPolicy:66 - Expecting 1 replicas with only 0 peer/s.
2018-04-26 01:59:56 WARN  BlockManager:66 - Block input-0-1524722395800 replicated to only 0 peer(s) instead of 1 peers
2018-04-26 01:59:56 INFO  BlockGenerator:54 - Pushed block input-0-1524722395800
2018-04-26 01:59:56 INFO  JobScheduler:54 - Added jobs for time 1524722396000 ms
2018-04-26 01:59:56 INFO  PythonRunner:54 - Times: total = 53, boot = -49, init = 102, finish = 0
2018-04-26 01:59:56 INFO  PythonRunner:54 - Times: total = 43, boot = -1, init = 44, finish = 0
2018-04-26 01:59:56 INFO  Executor:54 - Finished task 1.0 in stage 16.0 (TID 69). 1267 bytes result sent to driver
2018-04-26 01:59:56 INFO  TaskSetManager:54 - Starting task 2.0 in stage 16.0 (TID 70, localhost, executor driver, partition 2, ANY, 7772 bytes)
2018-04-26 01:59:56 INFO  TaskSetManager:54 - Finished task 1.0 in stage 16.0 (TID 69) in 63 ms on localhost (executor driver) (2/5)
2018-04-26 01:59:56 INFO  Executor:54 - Running task 2.0 in stage 16.0 (TID 70)
2018-04-26 01:59:56 INFO  BlockManager:54 - Found block input-0-1524722388200 locally
2018-04-26 01:59:56 INFO  PythonRunner:54 - Times: total = 58, boot = 17, init = 41, finish = 0
2018-04-26 01:59:56 INFO  PythonRunner:54 - Times: total = 48, boot = -6, init = 54, finish = 0
2018-04-26 01:59:56 INFO  Executor:54 - Finished task 2.0 in stage 16.0 (TID 70). 1267 bytes result sent to driver
2018-04-26 01:59:56 INFO  TaskSetManager:54 - Starting task 3.0 in stage 16.0 (TID 71, localhost, executor driver, partition 3, ANY, 7772 bytes)
2018-04-26 01:59:56 INFO  Executor:54 - Running task 3.0 in stage 16.0 (TID 71)
2018-04-26 01:59:56 INFO  TaskSetManager:54 - Finished task 2.0 in stage 16.0 (TID 70) in 78 ms on localhost (executor driver) (3/5)
2018-04-26 01:59:56 INFO  BlockManager:54 - Found block input-0-1524722388400 locally
2018-04-26 01:59:56 INFO  PythonRunner:54 - Times: total = 54, boot = -11, init = 65, finish = 0
2018-04-26 01:59:56 INFO  PythonRunner:54 - Times: total = 51, boot = 1, init = 50, finish = 0
2018-04-26 01:59:56 INFO  Executor:54 - Finished task 3.0 in stage 16.0 (TID 71). 1267 bytes result sent to driver
2018-04-26 01:59:56 INFO  TaskSetManager:54 - Starting task 4.0 in stage 16.0 (TID 72, localhost, executor driver, partition 4, ANY, 7772 bytes)
2018-04-26 01:59:56 INFO  Executor:54 - Running task 4.0 in stage 16.0 (TID 72)
2018-04-26 01:59:56 INFO  TaskSetManager:54 - Finished task 3.0 in stage 16.0 (TID 71) in 76 ms on localhost (executor driver) (4/5)
2018-04-26 01:59:56 INFO  BlockManager:54 - Found block input-0-1524722388600 locally
2018-04-26 01:59:56 INFO  MemoryStore:54 - Block input-0-1524722396000 stored as bytes in memory (estimated size 425.0 B, free 413.8 MB)
2018-04-26 01:59:56 INFO  BlockManagerInfo:54 - Added input-0-1524722396000 in memory on 10.0.2.15:33681 (size: 425.0 B, free: 413.9 MB)
2018-04-26 01:59:56 WARN  RandomBlockReplicationPolicy:66 - Expecting 1 replicas with only 0 peer/s.
2018-04-26 01:59:56 WARN  BlockManager:66 - Block input-0-1524722396000 replicated to only 0 peer(s) instead of 1 peers
2018-04-26 01:59:56 INFO  BlockGenerator:54 - Pushed block input-0-1524722396000
2018-04-26 01:59:56 INFO  PythonRunner:54 - Times: total = 56, boot = 18, init = 38, finish = 0
2018-04-26 01:59:56 INFO  PythonRunner:54 - Times: total = 43, boot = 2, init = 41, finish = 0
2018-04-26 01:59:56 INFO  Executor:54 - Finished task 4.0 in stage 16.0 (TID 72). 1267 bytes result sent to driver
2018-04-26 01:59:56 INFO  TaskSetManager:54 - Finished task 4.0 in stage 16.0 (TID 72) in 80 ms on localhost (executor driver) (5/5)
2018-04-26 01:59:56 INFO  TaskSchedulerImpl:54 - Removed TaskSet 16.0, whose tasks have all completed, from pool 
2018-04-26 01:59:56 INFO  DAGScheduler:54 - ResultStage 16 (call at /home/rjha/spark-2.3.0-bin-hadoop2.7/python/lib/py4j-0.10.6-src.zip/py4j/java_gateway.py:2257) finished in 0.380 s
2018-04-26 01:59:56 INFO  DAGScheduler:54 - Job 18 finished: call at /home/rjha/spark-2.3.0-bin-hadoop2.7/python/lib/py4j-0.10.6-src.zip/py4j/java_gateway.py:2257, took 0.389580 s
2018-04-26 01:59:56 INFO  JobScheduler:54 - Finished job streaming job 1524722389000 ms.1 from job set of time 1524722389000 ms
2018-04-26 01:59:56 INFO  JobScheduler:54 - Total delay: 7.274 s for time 1524722389000 ms (execution: 1.508 s)
2018-04-26 01:59:56 INFO  JobScheduler:54 - Starting job streaming job 1524722390000 ms.0 from job set of time 1524722390000 ms
2018-04-26 01:59:56 INFO  PythonRDD:54 - Removing RDD 25 from persistence list
2018-04-26 01:59:56 INFO  BlockManager:54 - Removing RDD 25
2018-04-26 01:59:56 INFO  BlockRDD:54 - Removing RDD 24 from persistence list
2018-04-26 01:59:56 INFO  BlockManager:54 - Removing RDD 24
2018-04-26 01:59:56 INFO  SocketInputDStream:54 - Removing blocks of RDD BlockRDD[24] at socketTextStream at NativeMethodAccessorImpl.java:0 of time 1524722389000 ms
2018-04-26 01:59:56 INFO  BlockManagerInfo:54 - Removed input-0-1524722386800 on 10.0.2.15:33681 in memory (size: 412.0 B, free: 413.9 MB)
2018-04-26 01:59:56 INFO  BlockManagerInfo:54 - Removed input-0-1524722387000 on 10.0.2.15:33681 in memory (size: 323.0 B, free: 413.9 MB)
2018-04-26 01:59:56 INFO  BlockManagerInfo:54 - Removed input-0-1524722387200 on 10.0.2.15:33681 in memory (size: 381.0 B, free: 413.9 MB)
2018-04-26 01:59:56 INFO  BlockManagerInfo:54 - Removed input-0-1524722387400 on 10.0.2.15:33681 in memory (size: 423.0 B, free: 413.9 MB)
2018-04-26 01:59:56 INFO  ReceivedBlockTracker:54 - Deleting batches: 1524722387000 ms
2018-04-26 01:59:56 INFO  InputInfoTracker:54 - remove old batch metadata: 1524722387000 ms
2018-04-26 01:59:56 INFO  BlockManagerInfo:54 - Removed input-0-1524722387600 on 10.0.2.15:33681 in memory (size: 342.0 B, free: 413.9 MB)
2018-04-26 01:59:56 INFO  SparkContext:54 - Starting job: call at /home/rjha/spark-2.3.0-bin-hadoop2.7/python/lib/py4j-0.10.6-src.zip/py4j/java_gateway.py:2257
2018-04-26 01:59:56 INFO  DAGScheduler:54 - Got job 19 (call at /home/rjha/spark-2.3.0-bin-hadoop2.7/python/lib/py4j-0.10.6-src.zip/py4j/java_gateway.py:2257) with 5 output partitions
2018-04-26 01:59:56 INFO  DAGScheduler:54 - Final stage: ResultStage 17 (call at /home/rjha/spark-2.3.0-bin-hadoop2.7/python/lib/py4j-0.10.6-src.zip/py4j/java_gateway.py:2257)
2018-04-26 01:59:56 INFO  DAGScheduler:54 - Parents of final stage: List()
2018-04-26 01:59:56 INFO  DAGScheduler:54 - Missing parents: List()
2018-04-26 01:59:56 INFO  DAGScheduler:54 - Submitting ResultStage 17 (PythonRDD[51] at call at /home/rjha/spark-2.3.0-bin-hadoop2.7/python/lib/py4j-0.10.6-src.zip/py4j/java_gateway.py:2257), which has no missing parents
2018-04-26 01:59:56 INFO  MemoryStore:54 - Block broadcast_17 stored as values in memory (estimated size 7.5 KB, free 413.8 MB)
2018-04-26 01:59:56 INFO  MemoryStore:54 - Block broadcast_17_piece0 stored as bytes in memory (estimated size 4.4 KB, free 413.8 MB)
2018-04-26 01:59:56 INFO  BlockManagerInfo:54 - Added broadcast_17_piece0 in memory on 10.0.2.15:33681 (size: 4.4 KB, free: 413.9 MB)
2018-04-26 01:59:56 INFO  SparkContext:54 - Created broadcast 17 from broadcast at DAGScheduler.scala:1039
2018-04-26 01:59:56 INFO  DAGScheduler:54 - Submitting 5 missing tasks from ResultStage 17 (PythonRDD[51] at call at /home/rjha/spark-2.3.0-bin-hadoop2.7/python/lib/py4j-0.10.6-src.zip/py4j/java_gateway.py:2257) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4))
2018-04-26 01:59:56 INFO  TaskSchedulerImpl:54 - Adding task set 17.0 with 5 tasks
2018-04-26 01:59:56 INFO  TaskSetManager:54 - Starting task 0.0 in stage 17.0 (TID 73, localhost, executor driver, partition 0, ANY, 7772 bytes)
2018-04-26 01:59:56 INFO  Executor:54 - Running task 0.0 in stage 17.0 (TID 73)
2018-04-26 01:59:56 INFO  BlockManager:54 - Found block input-0-1524722388800 locally
2018-04-26 01:59:56 INFO  ContextCleaner:54 - Cleaned accumulator 422
2018-04-26 01:59:56 INFO  ContextCleaner:54 - Cleaned accumulator 413
2018-04-26 01:59:56 INFO  ContextCleaner:54 - Cleaned accumulator 414
2018-04-26 01:59:56 INFO  ContextCleaner:54 - Cleaned accumulator 411
2018-04-26 01:59:56 INFO  ContextCleaner:54 - Cleaned accumulator 405
2018-04-26 01:59:56 INFO  ContextCleaner:54 - Cleaned accumulator 423
2018-04-26 01:59:56 INFO  BlockManagerInfo:54 - Removed broadcast_16_piece0 on 10.0.2.15:33681 in memory (size: 4.1 KB, free: 413.9 MB)
2018-04-26 01:59:56 INFO  ContextCleaner:54 - Cleaned accumulator 406
2018-04-26 01:59:56 INFO  ContextCleaner:54 - Cleaned accumulator 407
2018-04-26 01:59:56 INFO  ContextCleaner:54 - Cleaned accumulator 404
2018-04-26 01:59:56 INFO  ContextCleaner:54 - Cleaned accumulator 420
2018-04-26 01:59:56 INFO  ContextCleaner:54 - Cleaned accumulator 409
2018-04-26 01:59:56 INFO  ContextCleaner:54 - Cleaned accumulator 408
2018-04-26 01:59:56 INFO  ContextCleaner:54 - Cleaned accumulator 401
2018-04-26 01:59:56 INFO  ContextCleaner:54 - Cleaned accumulator 415
2018-04-26 01:59:56 INFO  ContextCleaner:54 - Cleaned accumulator 410
2018-04-26 01:59:56 INFO  ContextCleaner:54 - Cleaned accumulator 403
2018-04-26 01:59:56 INFO  ContextCleaner:54 - Cleaned accumulator 402
2018-04-26 01:59:56 INFO  ContextCleaner:54 - Cleaned accumulator 412
2018-04-26 01:59:56 INFO  ContextCleaner:54 - Cleaned accumulator 417
2018-04-26 01:59:56 INFO  ContextCleaner:54 - Cleaned accumulator 424
2018-04-26 01:59:56 INFO  ContextCleaner:54 - Cleaned accumulator 419
2018-04-26 01:59:56 INFO  ContextCleaner:54 - Cleaned accumulator 416
2018-04-26 01:59:56 INFO  ContextCleaner:54 - Cleaned accumulator 418
2018-04-26 01:59:56 INFO  ContextCleaner:54 - Cleaned accumulator 421
2018-04-26 01:59:56 INFO  ContextCleaner:54 - Cleaned accumulator 425
2018-04-26 01:59:56 INFO  MemoryStore:54 - Block input-0-1524722396200 stored as bytes in memory (estimated size 423.0 B, free 413.8 MB)
2018-04-26 01:59:56 INFO  BlockManagerInfo:54 - Added input-0-1524722396200 in memory on 10.0.2.15:33681 (size: 423.0 B, free: 413.9 MB)
2018-04-26 01:59:56 WARN  RandomBlockReplicationPolicy:66 - Expecting 1 replicas with only 0 peer/s.
2018-04-26 01:59:56 WARN  BlockManager:66 - Block input-0-1524722396200 replicated to only 0 peer(s) instead of 1 peers
2018-04-26 01:59:56 INFO  BlockGenerator:54 - Pushed block input-0-1524722396200
2018-04-26 01:59:56 INFO  PythonRunner:54 - Times: total = 50, boot = -59, init = 109, finish = 0
2018-04-26 01:59:56 INFO  PythonRunner:54 - Times: total = 50, boot = -75, init = 122, finish = 3
2018-04-26 01:59:56 INFO  Executor:54 - Finished task 0.0 in stage 17.0 (TID 73). 1267 bytes result sent to driver
2018-04-26 01:59:56 INFO  TaskSetManager:54 - Starting task 1.0 in stage 17.0 (TID 74, localhost, executor driver, partition 1, ANY, 7772 bytes)
2018-04-26 01:59:56 INFO  TaskSetManager:54 - Finished task 0.0 in stage 17.0 (TID 73) in 69 ms on localhost (executor driver) (1/5)
2018-04-26 01:59:56 INFO  Executor:54 - Running task 1.0 in stage 17.0 (TID 74)
2018-04-26 01:59:56 INFO  BlockManager:54 - Found block input-0-1524722389000 locally
2018-04-26 01:59:56 INFO  PythonRunner:54 - Times: total = 81, boot = 54, init = 27, finish = 0
2018-04-26 01:59:56 INFO  PythonRunner:54 - Times: total = 59, boot = 17, init = 38, finish = 4
2018-04-26 01:59:56 INFO  Executor:54 - Finished task 1.0 in stage 17.0 (TID 74). 1267 bytes result sent to driver
2018-04-26 01:59:56 INFO  TaskSetManager:54 - Starting task 2.0 in stage 17.0 (TID 75, localhost, executor driver, partition 2, ANY, 7772 bytes)
2018-04-26 01:59:56 INFO  TaskSetManager:54 - Finished task 1.0 in stage 17.0 (TID 74) in 134 ms on localhost (executor driver) (2/5)
2018-04-26 01:59:56 INFO  Executor:54 - Running task 2.0 in stage 17.0 (TID 75)
2018-04-26 01:59:56 INFO  BlockManager:54 - Found block input-0-1524722389200 locally
2018-04-26 01:59:56 INFO  MemoryStore:54 - Block input-0-1524722396400 stored as bytes in memory (estimated size 387.0 B, free 413.8 MB)
2018-04-26 01:59:56 INFO  BlockManagerInfo:54 - Added input-0-1524722396400 in memory on 10.0.2.15:33681 (size: 387.0 B, free: 413.9 MB)
2018-04-26 01:59:56 WARN  RandomBlockReplicationPolicy:66 - Expecting 1 replicas with only 0 peer/s.
2018-04-26 01:59:56 WARN  BlockManager:66 - Block input-0-1524722396400 replicated to only 0 peer(s) instead of 1 peers
2018-04-26 01:59:56 INFO  BlockGenerator:54 - Pushed block input-0-1524722396400
2018-04-26 01:59:56 INFO  PythonRunner:54 - Times: total = 56, boot = 43, init = 13, finish = 0
2018-04-26 01:59:56 INFO  PythonRunner:54 - Times: total = 157, boot = 128, init = 0, finish = 29
2018-04-26 01:59:56 INFO  Executor:54 - Finished task 2.0 in stage 17.0 (TID 75). 1267 bytes result sent to driver
2018-04-26 01:59:56 INFO  TaskSetManager:54 - Starting task 3.0 in stage 17.0 (TID 76, localhost, executor driver, partition 3, ANY, 7772 bytes)
2018-04-26 01:59:56 INFO  TaskSetManager:54 - Finished task 2.0 in stage 17.0 (TID 75) in 193 ms on localhost (executor driver) (3/5)
2018-04-26 01:59:56 INFO  Executor:54 - Running task 3.0 in stage 17.0 (TID 76)
2018-04-26 01:59:56 INFO  BlockManager:54 - Found block input-0-1524722389400 locally
2018-04-26 01:59:56 INFO  MemoryStore:54 - Block input-0-1524722396600 stored as bytes in memory (estimated size 338.0 B, free 413.8 MB)
2018-04-26 01:59:56 INFO  BlockManagerInfo:54 - Added input-0-1524722396600 in memory on 10.0.2.15:33681 (size: 338.0 B, free: 413.9 MB)
2018-04-26 01:59:56 WARN  RandomBlockReplicationPolicy:66 - Expecting 1 replicas with only 0 peer/s.
2018-04-26 01:59:56 WARN  BlockManager:66 - Block input-0-1524722396600 replicated to only 0 peer(s) instead of 1 peers
2018-04-26 01:59:56 INFO  BlockGenerator:54 - Pushed block input-0-1524722396600
2018-04-26 01:59:56 INFO  PythonRunner:54 - Times: total = 54, boot = -85, init = 139, finish = 0
2018-04-26 01:59:56 INFO  PythonRunner:54 - Times: total = 107, boot = 91, init = 0, finish = 16
2018-04-26 01:59:56 INFO  Executor:54 - Finished task 3.0 in stage 17.0 (TID 76). 1267 bytes result sent to driver
2018-04-26 01:59:56 INFO  TaskSetManager:54 - Starting task 4.0 in stage 17.0 (TID 77, localhost, executor driver, partition 4, ANY, 7772 bytes)
2018-04-26 01:59:56 INFO  TaskSetManager:54 - Finished task 3.0 in stage 17.0 (TID 76) in 150 ms on localhost (executor driver) (4/5)
2018-04-26 01:59:56 INFO  Executor:54 - Running task 4.0 in stage 17.0 (TID 77)
2018-04-26 01:59:56 INFO  BlockManager:54 - Found block input-0-1524722389600 locally
2018-04-26 01:59:56 INFO  PythonRunner:54 - Times: total = 52, boot = -61, init = 112, finish = 1
2018-04-26 01:59:57 INFO  MemoryStore:54 - Block input-0-1524722396800 stored as bytes in memory (estimated size 335.0 B, free 413.8 MB)
2018-04-26 01:59:57 INFO  BlockManagerInfo:54 - Added input-0-1524722396800 in memory on 10.0.2.15:33681 (size: 335.0 B, free: 413.9 MB)
2018-04-26 01:59:57 WARN  RandomBlockReplicationPolicy:66 - Expecting 1 replicas with only 0 peer/s.
2018-04-26 01:59:57 WARN  BlockManager:66 - Block input-0-1524722396800 replicated to only 0 peer(s) instead of 1 peers
2018-04-26 01:59:57 INFO  BlockGenerator:54 - Pushed block input-0-1524722396800
2018-04-26 01:59:57 INFO  PythonRunner:54 - Times: total = 96, boot = 36, init = 13, finish = 47
2018-04-26 01:59:57 INFO  Executor:54 - Finished task 4.0 in stage 17.0 (TID 77). 1267 bytes result sent to driver
2018-04-26 01:59:57 INFO  TaskSetManager:54 - Finished task 4.0 in stage 17.0 (TID 77) in 149 ms on localhost (executor driver) (5/5)
2018-04-26 01:59:57 INFO  TaskSchedulerImpl:54 - Removed TaskSet 17.0, whose tasks have all completed, from pool 
2018-04-26 01:59:57 INFO  DAGScheduler:54 - ResultStage 17 (call at /home/rjha/spark-2.3.0-bin-hadoop2.7/python/lib/py4j-0.10.6-src.zip/py4j/java_gateway.py:2257) finished in 0.704 s
2018-04-26 01:59:57 INFO  DAGScheduler:54 - Job 19 finished: call at /home/rjha/spark-2.3.0-bin-hadoop2.7/python/lib/py4j-0.10.6-src.zip/py4j/java_gateway.py:2257, took 0.710332 s
2018-04-26 01:59:57 INFO  JobScheduler:54 - Finished job streaming job 1524722390000 ms.0 from job set of time 1524722390000 ms
2018-04-26 01:59:57 INFO  JobScheduler:54 - Starting job streaming job 1524722390000 ms.1 from job set of time 1524722390000 ms
2018-04-26 01:59:57 INFO  MemoryStore:54 - Block input-0-1524722397000 stored as bytes in memory (estimated size 204.0 B, free 413.8 MB)
2018-04-26 01:59:57 INFO  BlockManagerInfo:54 - Added input-0-1524722397000 in memory on 10.0.2.15:33681 (size: 204.0 B, free: 413.9 MB)
2018-04-26 01:59:57 WARN  RandomBlockReplicationPolicy:66 - Expecting 1 replicas with only 0 peer/s.
2018-04-26 01:59:57 WARN  BlockManager:66 - Block input-0-1524722397000 replicated to only 0 peer(s) instead of 1 peers
2018-04-26 01:59:57 INFO  BlockGenerator:54 - Pushed block input-0-1524722397000
2018-04-26 01:59:57 INFO  JobScheduler:54 - Added jobs for time 1524722397000 ms
2018-04-26 01:59:57 INFO  SparkContext:54 - Starting job: call at /home/rjha/spark-2.3.0-bin-hadoop2.7/python/lib/py4j-0.10.6-src.zip/py4j/java_gateway.py:2257
2018-04-26 01:59:57 INFO  DAGScheduler:54 - Got job 20 (call at /home/rjha/spark-2.3.0-bin-hadoop2.7/python/lib/py4j-0.10.6-src.zip/py4j/java_gateway.py:2257) with 5 output partitions
2018-04-26 01:59:57 INFO  DAGScheduler:54 - Final stage: ResultStage 18 (call at /home/rjha/spark-2.3.0-bin-hadoop2.7/python/lib/py4j-0.10.6-src.zip/py4j/java_gateway.py:2257)
2018-04-26 01:59:57 INFO  DAGScheduler:54 - Parents of final stage: List()
2018-04-26 01:59:57 INFO  DAGScheduler:54 - Missing parents: List()
2018-04-26 01:59:57 INFO  DAGScheduler:54 - Submitting ResultStage 18 (PythonRDD[54] at call at /home/rjha/spark-2.3.0-bin-hadoop2.7/python/lib/py4j-0.10.6-src.zip/py4j/java_gateway.py:2257), which has no missing parents
2018-04-26 01:59:57 INFO  MemoryStore:54 - Block broadcast_18 stored as values in memory (estimated size 7.1 KB, free 413.8 MB)
2018-04-26 01:59:57 INFO  MemoryStore:54 - Block broadcast_18_piece0 stored as bytes in memory (estimated size 4.1 KB, free 413.8 MB)
2018-04-26 01:59:57 INFO  MemoryStore:54 - Block input-0-1524722397200 stored as bytes in memory (estimated size 430.0 B, free 413.8 MB)
2018-04-26 01:59:57 INFO  BlockManagerInfo:54 - Added input-0-1524722397200 in memory on 10.0.2.15:33681 (size: 430.0 B, free: 413.9 MB)
2018-04-26 01:59:57 WARN  RandomBlockReplicationPolicy:66 - Expecting 1 replicas with only 0 peer/s.
2018-04-26 01:59:57 WARN  BlockManager:66 - Block input-0-1524722397200 replicated to only 0 peer(s) instead of 1 peers
2018-04-26 01:59:57 INFO  BlockGenerator:54 - Pushed block input-0-1524722397200
2018-04-26 01:59:57 INFO  BlockManagerInfo:54 - Added broadcast_18_piece0 in memory on 10.0.2.15:33681 (size: 4.1 KB, free: 413.9 MB)
2018-04-26 01:59:57 INFO  SparkContext:54 - Created broadcast 18 from broadcast at DAGScheduler.scala:1039
2018-04-26 01:59:57 INFO  DAGScheduler:54 - Submitting 5 missing tasks from ResultStage 18 (PythonRDD[54] at call at /home/rjha/spark-2.3.0-bin-hadoop2.7/python/lib/py4j-0.10.6-src.zip/py4j/java_gateway.py:2257) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4))
2018-04-26 01:59:57 INFO  TaskSchedulerImpl:54 - Adding task set 18.0 with 5 tasks
2018-04-26 01:59:57 INFO  TaskSetManager:54 - Starting task 0.0 in stage 18.0 (TID 78, localhost, executor driver, partition 0, ANY, 7772 bytes)
2018-04-26 01:59:57 INFO  Executor:54 - Running task 0.0 in stage 18.0 (TID 78)
2018-04-26 01:59:57 INFO  BlockManager:54 - Found block input-0-1524722388800 locally
2018-04-26 01:59:57 INFO  ContextCleaner:54 - Cleaned accumulator 432
2018-04-26 01:59:57 INFO  ContextCleaner:54 - Cleaned accumulator 429
2018-04-26 01:59:57 INFO  ContextCleaner:54 - Cleaned accumulator 427
2018-04-26 01:59:57 INFO  ContextCleaner:54 - Cleaned accumulator 431
2018-04-26 01:59:57 INFO  ContextCleaner:54 - Cleaned accumulator 444
2018-04-26 01:59:57 INFO  ContextCleaner:54 - Cleaned accumulator 430
2018-04-26 01:59:57 INFO  ContextCleaner:54 - Cleaned accumulator 428
2018-04-26 01:59:57 INFO  ContextCleaner:54 - Cleaned accumulator 440
2018-04-26 01:59:57 INFO  ContextCleaner:54 - Cleaned accumulator 436
2018-04-26 01:59:57 INFO  ContextCleaner:54 - Cleaned accumulator 446
2018-04-26 01:59:57 INFO  ContextCleaner:54 - Cleaned accumulator 445
2018-04-26 01:59:57 INFO  ContextCleaner:54 - Cleaned accumulator 447
2018-04-26 01:59:57 INFO  ContextCleaner:54 - Cleaned accumulator 437
2018-04-26 01:59:57 INFO  ContextCleaner:54 - Cleaned accumulator 438
2018-04-26 01:59:57 INFO  ContextCleaner:54 - Cleaned accumulator 441
2018-04-26 01:59:57 INFO  ContextCleaner:54 - Cleaned accumulator 442
2018-04-26 01:59:57 INFO  ContextCleaner:54 - Cleaned accumulator 434
2018-04-26 01:59:57 INFO  BlockManagerInfo:54 - Removed broadcast_17_piece0 on 10.0.2.15:33681 in memory (size: 4.4 KB, free: 413.9 MB)
2018-04-26 01:59:57 INFO  ContextCleaner:54 - Cleaned accumulator 435
2018-04-26 01:59:57 INFO  ContextCleaner:54 - Cleaned accumulator 449
2018-04-26 01:59:57 INFO  ContextCleaner:54 - Cleaned accumulator 433
2018-04-26 01:59:57 INFO  ContextCleaner:54 - Cleaned accumulator 426
2018-04-26 01:59:57 INFO  ContextCleaner:54 - Cleaned accumulator 450
2018-04-26 01:59:57 INFO  ContextCleaner:54 - Cleaned accumulator 439
2018-04-26 01:59:57 INFO  ContextCleaner:54 - Cleaned accumulator 448
2018-04-26 01:59:57 INFO  ContextCleaner:54 - Cleaned accumulator 443
2018-04-26 01:59:57 INFO  PythonRunner:54 - Times: total = 51, boot = -214, init = 265, finish = 0
2018-04-26 01:59:57 INFO  PythonRunner:54 - Times: total = 45, boot = -209, init = 253, finish = 1
2018-04-26 01:59:57 INFO  Executor:54 - Finished task 0.0 in stage 18.0 (TID 78). 1310 bytes result sent to driver
2018-04-26 01:59:57 INFO  TaskSetManager:54 - Starting task 1.0 in stage 18.0 (TID 79, localhost, executor driver, partition 1, ANY, 7772 bytes)
2018-04-26 01:59:57 INFO  TaskSetManager:54 - Finished task 0.0 in stage 18.0 (TID 78) in 79 ms on localhost (executor driver) (1/5)
2018-04-26 01:59:57 INFO  Executor:54 - Running task 1.0 in stage 18.0 (TID 79)
2018-04-26 01:59:57 INFO  BlockManager:54 - Found block input-0-1524722389000 locally
2018-04-26 01:59:57 INFO  PythonRunner:54 - Times: total = 69, boot = 55, init = 14, finish = 0
2018-04-26 01:59:57 INFO  PythonRunner:54 - Times: total = 48, boot = 30, init = 18, finish = 0
2018-04-26 01:59:57 INFO  Executor:54 - Finished task 1.0 in stage 18.0 (TID 79). 1267 bytes result sent to driver
2018-04-26 01:59:57 INFO  TaskSetManager:54 - Starting task 2.0 in stage 18.0 (TID 80, localhost, executor driver, partition 2, ANY, 7772 bytes)
2018-04-26 01:59:57 INFO  TaskSetManager:54 - Finished task 1.0 in stage 18.0 (TID 79) in 83 ms on localhost (executor driver) (2/5)
2018-04-26 01:59:57 INFO  Executor:54 - Running task 2.0 in stage 18.0 (TID 80)
2018-04-26 01:59:57 INFO  BlockManager:54 - Found block input-0-1524722389200 locally
2018-04-26 01:59:57 INFO  MemoryStore:54 - Block input-0-1524722397400 stored as bytes in memory (estimated size 427.0 B, free 413.8 MB)
2018-04-26 01:59:57 INFO  BlockManagerInfo:54 - Added input-0-1524722397400 in memory on 10.0.2.15:33681 (size: 427.0 B, free: 413.9 MB)
2018-04-26 01:59:57 WARN  RandomBlockReplicationPolicy:66 - Expecting 1 replicas with only 0 peer/s.
2018-04-26 01:59:57 WARN  BlockManager:66 - Block input-0-1524722397400 replicated to only 0 peer(s) instead of 1 peers
2018-04-26 01:59:57 INFO  BlockGenerator:54 - Pushed block input-0-1524722397400
2018-04-26 01:59:57 INFO  PythonRunner:54 - Times: total = 56, boot = 29, init = 27, finish = 0
2018-04-26 01:59:57 INFO  PythonRunner:54 - Times: total = 49, boot = 15, init = 34, finish = 0
2018-04-26 01:59:57 INFO  Executor:54 - Finished task 2.0 in stage 18.0 (TID 80). 1267 bytes result sent to driver
2018-04-26 01:59:57 INFO  TaskSetManager:54 - Starting task 3.0 in stage 18.0 (TID 81, localhost, executor driver, partition 3, ANY, 7772 bytes)
2018-04-26 01:59:57 INFO  Executor:54 - Running task 3.0 in stage 18.0 (TID 81)
2018-04-26 01:59:57 INFO  TaskSetManager:54 - Finished task 2.0 in stage 18.0 (TID 80) in 87 ms on localhost (executor driver) (3/5)
2018-04-26 01:59:57 INFO  BlockManager:54 - Found block input-0-1524722389400 locally
2018-04-26 01:59:57 INFO  PythonRunner:54 - Times: total = 46, boot = 27, init = 19, finish = 0
2018-04-26 01:59:57 INFO  PythonRunner:54 - Times: total = 46, boot = 14, init = 32, finish = 0
2018-04-26 01:59:57 INFO  Executor:54 - Finished task 3.0 in stage 18.0 (TID 81). 1267 bytes result sent to driver
2018-04-26 01:59:57 INFO  TaskSetManager:54 - Starting task 4.0 in stage 18.0 (TID 82, localhost, executor driver, partition 4, ANY, 7772 bytes)
2018-04-26 01:59:57 INFO  TaskSetManager:54 - Finished task 3.0 in stage 18.0 (TID 81) in 71 ms on localhost (executor driver) (4/5)
2018-04-26 01:59:57 INFO  Executor:54 - Running task 4.0 in stage 18.0 (TID 82)
2018-04-26 01:59:57 INFO  BlockManager:54 - Found block input-0-1524722389600 locally
2018-04-26 01:59:57 INFO  PythonRunner:54 - Times: total = 57, boot = 9, init = 48, finish = 0
2018-04-26 01:59:57 INFO  MemoryStore:54 - Block input-0-1524722397600 stored as bytes in memory (estimated size 430.0 B, free 413.8 MB)
2018-04-26 01:59:57 INFO  BlockManagerInfo:54 - Added input-0-1524722397600 in memory on 10.0.2.15:33681 (size: 430.0 B, free: 413.9 MB)
2018-04-26 01:59:57 INFO  PythonRunner:54 - Times: total = 59, boot = 16, init = 43, finish = 0
2018-04-26 01:59:57 INFO  Executor:54 - Finished task 4.0 in stage 18.0 (TID 82). 1267 bytes result sent to driver
2018-04-26 01:59:57 INFO  TaskSetManager:54 - Finished task 4.0 in stage 18.0 (TID 82) in 84 ms on localhost (executor driver) (5/5)
2018-04-26 01:59:57 INFO  TaskSchedulerImpl:54 - Removed TaskSet 18.0, whose tasks have all completed, from pool 
2018-04-26 01:59:57 INFO  DAGScheduler:54 - ResultStage 18 (call at /home/rjha/spark-2.3.0-bin-hadoop2.7/python/lib/py4j-0.10.6-src.zip/py4j/java_gateway.py:2257) finished in 0.428 s
2018-04-26 01:59:57 INFO  DAGScheduler:54 - Job 20 finished: call at /home/rjha/spark-2.3.0-bin-hadoop2.7/python/lib/py4j-0.10.6-src.zip/py4j/java_gateway.py:2257, took 0.443305 s
2018-04-26 01:59:57 WARN  RandomBlockReplicationPolicy:66 - Expecting 1 replicas with only 0 peer/s.
2018-04-26 01:59:57 WARN  BlockManager:66 - Block input-0-1524722397600 replicated to only 0 peer(s) instead of 1 peers
2018-04-26 01:59:57 INFO  BlockGenerator:54 - Pushed block input-0-1524722397600
2018-04-26 01:59:57 INFO  JobScheduler:54 - Finished job streaming job 1524722390000 ms.1 from job set of time 1524722390000 ms
2018-04-26 01:59:57 INFO  JobScheduler:54 - Total delay: 7.878 s for time 1524722390000 ms (execution: 1.603 s)
2018-04-26 01:59:57 INFO  JobScheduler:54 - Starting job streaming job 1524722391000 ms.0 from job set of time 1524722391000 ms
2018-04-26 01:59:57 INFO  PythonRDD:54 - Removing RDD 27 from persistence list
2018-04-26 01:59:57 INFO  BlockManager:54 - Removing RDD 27
2018-04-26 01:59:57 INFO  BlockRDD:54 - Removing RDD 26 from persistence list
2018-04-26 01:59:57 INFO  BlockManager:54 - Removing RDD 26
2018-04-26 01:59:57 INFO  SocketInputDStream:54 - Removing blocks of RDD BlockRDD[26] at socketTextStream at NativeMethodAccessorImpl.java:0 of time 1524722390000 ms
2018-04-26 01:59:57 INFO  BlockManagerInfo:54 - Removed input-0-1524722387800 on 10.0.2.15:33681 in memory (size: 408.0 B, free: 413.9 MB)
2018-04-26 01:59:57 INFO  BlockManagerInfo:54 - Removed input-0-1524722388000 on 10.0.2.15:33681 in memory (size: 323.0 B, free: 413.9 MB)
2018-04-26 01:59:57 INFO  BlockManagerInfo:54 - Removed input-0-1524722388200 on 10.0.2.15:33681 in memory (size: 410.0 B, free: 413.9 MB)
2018-04-26 01:59:57 INFO  BlockManagerInfo:54 - Removed input-0-1524722388400 on 10.0.2.15:33681 in memory (size: 324.0 B, free: 413.9 MB)
2018-04-26 01:59:57 INFO  ReceivedBlockTracker:54 - Deleting batches: 1524722388000 ms
2018-04-26 01:59:57 INFO  InputInfoTracker:54 - remove old batch metadata: 1524722388000 ms
2018-04-26 01:59:57 INFO  BlockManagerInfo:54 - Removed input-0-1524722388600 on 10.0.2.15:33681 in memory (size: 153.0 B, free: 413.9 MB)
2018-04-26 01:59:57 INFO  SparkContext:54 - Starting job: call at /home/rjha/spark-2.3.0-bin-hadoop2.7/python/lib/py4j-0.10.6-src.zip/py4j/java_gateway.py:2257
2018-04-26 01:59:57 INFO  DAGScheduler:54 - Got job 21 (call at /home/rjha/spark-2.3.0-bin-hadoop2.7/python/lib/py4j-0.10.6-src.zip/py4j/java_gateway.py:2257) with 5 output partitions
2018-04-26 01:59:57 INFO  DAGScheduler:54 - Final stage: ResultStage 19 (call at /home/rjha/spark-2.3.0-bin-hadoop2.7/python/lib/py4j-0.10.6-src.zip/py4j/java_gateway.py:2257)
2018-04-26 01:59:57 INFO  DAGScheduler:54 - Parents of final stage: List()
2018-04-26 01:59:57 INFO  DAGScheduler:54 - Missing parents: List()
2018-04-26 01:59:57 INFO  DAGScheduler:54 - Submitting ResultStage 19 (PythonRDD[55] at call at /home/rjha/spark-2.3.0-bin-hadoop2.7/python/lib/py4j-0.10.6-src.zip/py4j/java_gateway.py:2257), which has no missing parents
2018-04-26 01:59:57 INFO  MemoryStore:54 - Block broadcast_19 stored as values in memory (estimated size 7.5 KB, free 413.8 MB)
2018-04-26 01:59:57 INFO  MemoryStore:54 - Block broadcast_19_piece0 stored as bytes in memory (estimated size 4.4 KB, free 413.8 MB)
2018-04-26 01:59:57 INFO  BlockManagerInfo:54 - Added broadcast_19_piece0 in memory on 10.0.2.15:33681 (size: 4.4 KB, free: 413.9 MB)
2018-04-26 01:59:57 INFO  SparkContext:54 - Created broadcast 19 from broadcast at DAGScheduler.scala:1039
2018-04-26 01:59:58 INFO  MemoryStore:54 - Block input-0-1524722397800 stored as bytes in memory (estimated size 357.0 B, free 413.8 MB)
2018-04-26 01:59:58 INFO  BlockManagerInfo:54 - Added input-0-1524722397800 in memory on 10.0.2.15:33681 (size: 357.0 B, free: 413.9 MB)
2018-04-26 01:59:58 WARN  RandomBlockReplicationPolicy:66 - Expecting 1 replicas with only 0 peer/s.
2018-04-26 01:59:58 WARN  BlockManager:66 - Block input-0-1524722397800 replicated to only 0 peer(s) instead of 1 peers
2018-04-26 01:59:58 INFO  BlockGenerator:54 - Pushed block input-0-1524722397800
2018-04-26 01:59:58 INFO  DAGScheduler:54 - Submitting 5 missing tasks from ResultStage 19 (PythonRDD[55] at call at /home/rjha/spark-2.3.0-bin-hadoop2.7/python/lib/py4j-0.10.6-src.zip/py4j/java_gateway.py:2257) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4))
2018-04-26 01:59:58 INFO  TaskSchedulerImpl:54 - Adding task set 19.0 with 5 tasks
2018-04-26 01:59:58 INFO  TaskSetManager:54 - Starting task 0.0 in stage 19.0 (TID 83, localhost, executor driver, partition 0, ANY, 7772 bytes)
2018-04-26 01:59:58 INFO  Executor:54 - Running task 0.0 in stage 19.0 (TID 83)
2018-04-26 01:59:58 INFO  BlockManager:54 - Found block input-0-1524722389800 locally
2018-04-26 01:59:58 INFO  JobScheduler:54 - Added jobs for time 1524722398000 ms
2018-04-26 01:59:58 INFO  PythonRunner:54 - Times: total = 47, boot = -78, init = 125, finish = 0
2018-04-26 01:59:58 ERROR Executor:91 - Exception in task 0.0 in stage 19.0 (TID 83)
org.apache.spark.api.python.PythonException: Traceback (most recent call last):
  File "/home/rjha/spark-2.3.0-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/worker.py", line 229, in main
    process()
  File "/home/rjha/spark-2.3.0-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/worker.py", line 224, in process
    serializer.dump_stream(func(split_index, iterator), outfile)
  File "/home/rjha/spark-2.3.0-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/rdd.py", line 2438, in pipeline_func
  File "/home/rjha/spark-2.3.0-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/rdd.py", line 2438, in pipeline_func
  File "/home/rjha/spark-2.3.0-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/rdd.py", line 2438, in pipeline_func
  File "/home/rjha/spark-2.3.0-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/rdd.py", line 362, in func
  File "/home/rjha/spark-2.3.0-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/rdd.py", line 809, in func
  File "/home/rjha/stream-benchmarking-scratch/streaming_test.py", line 37, in <lambda>
    w_2_ts.foreachRDD(lambda rdd: rdd.foreachPartition(lambda partition: send_partition_to_db(partition, out_port))) # storing
  File "/home/rjha/stream-benchmarking-scratch/streaming_test.py", line 17, in send_partition_to_db
    sock.connect(('localhost', out_port))
  File "/usr/lib/python2.7/socket.py", line 228, in meth
    return getattr(self._sock,name)(*args)
error: [Errno 111] Connection refused

	at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:298)
	at org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:438)
	at org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:421)
	at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:252)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)
	at scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48)
	at scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310)
	at org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)
	at scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302)
	at org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)
	at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289)
	at org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)
	at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$12.apply(RDD.scala:939)
	at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$12.apply(RDD.scala:939)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2067)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2067)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
	at org.apache.spark.scheduler.Task.run(Task.scala:109)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
2018-04-26 01:59:58 INFO  TaskSetManager:54 - Starting task 1.0 in stage 19.0 (TID 84, localhost, executor driver, partition 1, ANY, 7772 bytes)
2018-04-26 01:59:58 WARN  TaskSetManager:66 - Lost task 0.0 in stage 19.0 (TID 83, localhost, executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):
  File "/home/rjha/spark-2.3.0-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/worker.py", line 229, in main
    process()
  File "/home/rjha/spark-2.3.0-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/worker.py", line 224, in process
    serializer.dump_stream(func(split_index, iterator), outfile)
  File "/home/rjha/spark-2.3.0-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/rdd.py", line 2438, in pipeline_func
  File "/home/rjha/spark-2.3.0-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/rdd.py", line 2438, in pipeline_func
  File "/home/rjha/spark-2.3.0-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/rdd.py", line 2438, in pipeline_func
  File "/home/rjha/spark-2.3.0-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/rdd.py", line 362, in func
  File "/home/rjha/spark-2.3.0-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/rdd.py", line 809, in func
  File "/home/rjha/stream-benchmarking-scratch/streaming_test.py", line 37, in <lambda>
    w_2_ts.foreachRDD(lambda rdd: rdd.foreachPartition(lambda partition: send_partition_to_db(partition, out_port))) # storing
  File "/home/rjha/stream-benchmarking-scratch/streaming_test.py", line 17, in send_partition_to_db
    sock.connect(('localhost', out_port))
  File "/usr/lib/python2.7/socket.py", line 228, in meth
    return getattr(self._sock,name)(*args)
error: [Errno 111] Connection refused

	at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:298)
	at org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:438)
	at org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:421)
	at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:252)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)
	at scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48)
	at scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310)
	at org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)
	at scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302)
	at org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)
	at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289)
	at org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)
	at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$12.apply(RDD.scala:939)
	at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$12.apply(RDD.scala:939)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2067)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2067)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
	at org.apache.spark.scheduler.Task.run(Task.scala:109)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

2018-04-26 01:59:58 INFO  Executor:54 - Running task 1.0 in stage 19.0 (TID 84)
2018-04-26 01:59:58 INFO  BlockManager:54 - Found block input-0-1524722390000 locally
2018-04-26 01:59:58 ERROR TaskSetManager:70 - Task 0 in stage 19.0 failed 1 times; aborting job
2018-04-26 01:59:58 INFO  ContextCleaner:54 - Cleaned accumulator 471
2018-04-26 01:59:58 INFO  ContextCleaner:54 - Cleaned accumulator 468
2018-04-26 01:59:58 INFO  ContextCleaner:54 - Cleaned accumulator 467
2018-04-26 01:59:58 INFO  ContextCleaner:54 - Cleaned accumulator 466
2018-04-26 01:59:58 INFO  ContextCleaner:54 - Cleaned accumulator 457
2018-04-26 01:59:58 INFO  ContextCleaner:54 - Cleaned accumulator 470
2018-04-26 01:59:58 INFO  ContextCleaner:54 - Cleaned accumulator 451
2018-04-26 01:59:58 INFO  ContextCleaner:54 - Cleaned accumulator 465
2018-04-26 01:59:58 INFO  ContextCleaner:54 - Cleaned accumulator 455
2018-04-26 01:59:58 INFO  ContextCleaner:54 - Cleaned accumulator 475
2018-04-26 01:59:58 INFO  ContextCleaner:54 - Cleaned accumulator 456
2018-04-26 01:59:58 INFO  ContextCleaner:54 - Cleaned accumulator 463
2018-04-26 01:59:58 INFO  ContextCleaner:54 - Cleaned accumulator 453
2018-04-26 01:59:58 INFO  ContextCleaner:54 - Cleaned accumulator 461
2018-04-26 01:59:58 INFO  ContextCleaner:54 - Cleaned accumulator 454
2018-04-26 01:59:58 INFO  ContextCleaner:54 - Cleaned accumulator 458
2018-04-26 01:59:58 INFO  ContextCleaner:54 - Cleaned accumulator 464
2018-04-26 01:59:58 INFO  ContextCleaner:54 - Cleaned accumulator 452
2018-04-26 01:59:58 INFO  ContextCleaner:54 - Cleaned accumulator 460
2018-04-26 01:59:58 INFO  ContextCleaner:54 - Cleaned accumulator 459
2018-04-26 01:59:58 INFO  ContextCleaner:54 - Cleaned accumulator 474
2018-04-26 01:59:58 INFO  ContextCleaner:54 - Cleaned accumulator 472
2018-04-26 01:59:58 INFO  BlockManagerInfo:54 - Removed broadcast_18_piece0 on 10.0.2.15:33681 in memory (size: 4.1 KB, free: 413.9 MB)
2018-04-26 01:59:58 INFO  ContextCleaner:54 - Cleaned accumulator 473
2018-04-26 01:59:58 INFO  ContextCleaner:54 - Cleaned accumulator 462
2018-04-26 01:59:58 INFO  ContextCleaner:54 - Cleaned accumulator 469
2018-04-26 01:59:58 INFO  TaskSchedulerImpl:54 - Cancelling stage 19
2018-04-26 01:59:58 INFO  MemoryStore:54 - Block input-0-1524722398000 stored as bytes in memory (estimated size 421.0 B, free 413.8 MB)
2018-04-26 01:59:58 INFO  BlockManagerInfo:54 - Added input-0-1524722398000 in memory on 10.0.2.15:33681 (size: 421.0 B, free: 413.9 MB)
2018-04-26 01:59:58 WARN  RandomBlockReplicationPolicy:66 - Expecting 1 replicas with only 0 peer/s.
2018-04-26 01:59:58 WARN  BlockManager:66 - Block input-0-1524722398000 replicated to only 0 peer(s) instead of 1 peers
2018-04-26 01:59:58 INFO  BlockGenerator:54 - Pushed block input-0-1524722398000
2018-04-26 01:59:58 INFO  TaskSchedulerImpl:54 - Stage 19 was cancelled
2018-04-26 01:59:58 INFO  Executor:54 - Executor is trying to kill task 1.0 in stage 19.0 (TID 84), reason: Stage cancelled
2018-04-26 01:59:58 INFO  DAGScheduler:54 - ResultStage 19 (call at /home/rjha/spark-2.3.0-bin-hadoop2.7/python/lib/py4j-0.10.6-src.zip/py4j/java_gateway.py:2257) failed in 0.228 s due to Job aborted due to stage failure: Task 0 in stage 19.0 failed 1 times, most recent failure: Lost task 0.0 in stage 19.0 (TID 83, localhost, executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):
  File "/home/rjha/spark-2.3.0-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/worker.py", line 229, in main
    process()
  File "/home/rjha/spark-2.3.0-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/worker.py", line 224, in process
    serializer.dump_stream(func(split_index, iterator), outfile)
  File "/home/rjha/spark-2.3.0-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/rdd.py", line 2438, in pipeline_func
  File "/home/rjha/spark-2.3.0-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/rdd.py", line 2438, in pipeline_func
  File "/home/rjha/spark-2.3.0-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/rdd.py", line 2438, in pipeline_func
  File "/home/rjha/spark-2.3.0-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/rdd.py", line 362, in func
  File "/home/rjha/spark-2.3.0-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/rdd.py", line 809, in func
  File "/home/rjha/stream-benchmarking-scratch/streaming_test.py", line 37, in <lambda>
    w_2_ts.foreachRDD(lambda rdd: rdd.foreachPartition(lambda partition: send_partition_to_db(partition, out_port))) # storing
  File "/home/rjha/stream-benchmarking-scratch/streaming_test.py", line 17, in send_partition_to_db
    sock.connect(('localhost', out_port))
  File "/usr/lib/python2.7/socket.py", line 228, in meth
    return getattr(self._sock,name)(*args)
error: [Errno 111] Connection refused

	at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:298)
	at org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:438)
	at org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:421)
	at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:252)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)
	at scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48)
	at scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310)
	at org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)
	at scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302)
	at org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)
	at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289)
	at org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)
	at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$12.apply(RDD.scala:939)
	at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$12.apply(RDD.scala:939)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2067)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2067)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
	at org.apache.spark.scheduler.Task.run(Task.scala:109)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

Driver stacktrace:
2018-04-26 01:59:58 INFO  Executor:54 - Executor killed task 1.0 in stage 19.0 (TID 84), reason: Stage cancelled
2018-04-26 01:59:58 WARN  TaskSetManager:66 - Lost task 1.0 in stage 19.0 (TID 84, localhost, executor driver): TaskKilled (Stage cancelled)
2018-04-26 01:59:58 INFO  TaskSchedulerImpl:54 - Removed TaskSet 19.0, whose tasks have all completed, from pool 
2018-04-26 01:59:58 INFO  DAGScheduler:54 - Job 21 failed: call at /home/rjha/spark-2.3.0-bin-hadoop2.7/python/lib/py4j-0.10.6-src.zip/py4j/java_gateway.py:2257, took 0.252873 s
2018-04-26 01:59:58 INFO  JobScheduler:54 - Finished job streaming job 1524722391000 ms.0 from job set of time 1524722391000 ms
2018-04-26 01:59:58 INFO  JobScheduler:54 - Starting job streaming job 1524722391000 ms.1 from job set of time 1524722391000 ms
2018-04-26 01:59:58 ERROR JobScheduler:91 - Error running job streaming job 1524722391000 ms.0
org.apache.spark.SparkException: An exception was raised by Python:
Traceback (most recent call last):
  File "/home/rjha/spark-2.3.0-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/streaming/util.py", line 65, in call
    r = self.func(t, *rdds)
  File "/home/rjha/spark-2.3.0-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/streaming/dstream.py", line 159, in <lambda>
    func = lambda t, rdd: old_func(rdd)
  File "/home/rjha/stream-benchmarking-scratch/streaming_test.py", line 37, in <lambda>
    w_2_ts.foreachRDD(lambda rdd: rdd.foreachPartition(lambda partition: send_partition_to_db(partition, out_port))) # storing
  File "/home/rjha/spark-2.3.0-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/rdd.py", line 814, in foreachPartition
    self.mapPartitions(func).count()  # Force evaluation
  File "/home/rjha/spark-2.3.0-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/rdd.py", line 1056, in count
    return self.mapPartitions(lambda i: [sum(1 for _ in i)]).sum()
  File "/home/rjha/spark-2.3.0-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/rdd.py", line 1047, in sum
    return self.mapPartitions(lambda x: [sum(x)]).fold(0, operator.add)
  File "/home/rjha/spark-2.3.0-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/rdd.py", line 921, in fold
    vals = self.mapPartitions(func).collect()
  File "/home/rjha/spark-2.3.0-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/rdd.py", line 824, in collect
    port = self.ctx._jvm.PythonRDD.collectAndServe(self._jrdd.rdd())
  File "/home/rjha/spark-2.3.0-bin-hadoop2.7/python/lib/py4j-0.10.6-src.zip/py4j/java_gateway.py", line 1160, in __call__
    answer, self.gateway_client, self.target_id, self.name)
  File "/home/rjha/spark-2.3.0-bin-hadoop2.7/python/lib/py4j-0.10.6-src.zip/py4j/protocol.py", line 320, in get_return_value
    format(target_id, ".", name), value)
Py4JJavaError: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.
: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 19.0 failed 1 times, most recent failure: Lost task 0.0 in stage 19.0 (TID 83, localhost, executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):
  File "/home/rjha/spark-2.3.0-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/worker.py", line 229, in main
    process()
  File "/home/rjha/spark-2.3.0-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/worker.py", line 224, in process
    serializer.dump_stream(func(split_index, iterator), outfile)
  File "/home/rjha/spark-2.3.0-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/rdd.py", line 2438, in pipeline_func
  File "/home/rjha/spark-2.3.0-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/rdd.py", line 2438, in pipeline_func
  File "/home/rjha/spark-2.3.0-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/rdd.py", line 2438, in pipeline_func
  File "/home/rjha/spark-2.3.0-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/rdd.py", line 362, in func
  File "/home/rjha/spark-2.3.0-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/rdd.py", line 809, in func
  File "/home/rjha/stream-benchmarking-scratch/streaming_test.py", line 37, in <lambda>
    w_2_ts.foreachRDD(lambda rdd: rdd.foreachPartition(lambda partition: send_partition_to_db(partition, out_port))) # storing
  File "/home/rjha/stream-benchmarking-scratch/streaming_test.py", line 17, in send_partition_to_db
    sock.connect(('localhost', out_port))
  File "/usr/lib/python2.7/socket.py", line 228, in meth
    return getattr(self._sock,name)(*args)
error: [Errno 111] Connection refused

	at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:298)
	at org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:438)
	at org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:421)
	at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:252)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)
	at scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48)
	at scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310)
	at org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)
	at scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302)
	at org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)
	at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289)
	at org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)
	at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$12.apply(RDD.scala:939)
	at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$12.apply(RDD.scala:939)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2067)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2067)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
	at org.apache.spark.scheduler.Task.run(Task.scala:109)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1599)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1587)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1586)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1586)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:831)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:831)
	at scala.Option.foreach(Option.scala:257)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:831)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1820)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1769)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1758)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)
	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:642)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2027)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2048)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2067)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2092)
	at org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:939)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)
	at org.apache.spark.rdd.RDD.withScope(RDD.scala:363)
	at org.apache.spark.rdd.RDD.collect(RDD.scala:938)
	at org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:153)
	at org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)
	at sun.reflect.GeneratedMethodAccessor43.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.GatewayConnection.run(GatewayConnection.java:214)
	at java.lang.Thread.run(Thread.java:748)
Caused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):
  File "/home/rjha/spark-2.3.0-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/worker.py", line 229, in main
    process()
  File "/home/rjha/spark-2.3.0-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/worker.py", line 224, in process
    serializer.dump_stream(func(split_index, iterator), outfile)
  File "/home/rjha/spark-2.3.0-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/rdd.py", line 2438, in pipeline_func
  File "/home/rjha/spark-2.3.0-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/rdd.py", line 2438, in pipeline_func
  File "/home/rjha/spark-2.3.0-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/rdd.py", line 2438, in pipeline_func
  File "/home/rjha/spark-2.3.0-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/rdd.py", line 362, in func
  File "/home/rjha/spark-2.3.0-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/rdd.py", line 809, in func
  File "/home/rjha/stream-benchmarking-scratch/streaming_test.py", line 37, in <lambda>
    w_2_ts.foreachRDD(lambda rdd: rdd.foreachPartition(lambda partition: send_partition_to_db(partition, out_port))) # storing
  File "/home/rjha/stream-benchmarking-scratch/streaming_test.py", line 17, in send_partition_to_db
    sock.connect(('localhost', out_port))
  File "/usr/lib/python2.7/socket.py", line 228, in meth
    return getattr(self._sock,name)(*args)
error: [Errno 111] Connection refused

	at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:298)
	at org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:438)
	at org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:421)
	at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:252)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)
	at scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48)
	at scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310)
	at org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)
	at scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302)
	at org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)
	at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289)
	at org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)
	at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$12.apply(RDD.scala:939)
	at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$12.apply(RDD.scala:939)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2067)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2067)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
	at org.apache.spark.scheduler.Task.run(Task.scala:109)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	... 1 more


	at org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)
	at org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)
	at org.apache.spark.streaming.api.python.PythonDStream$$anonfun$callForeachRDD$1.apply(PythonDStream.scala:179)
	at org.apache.spark.streaming.api.python.PythonDStream$$anonfun$callForeachRDD$1.apply(PythonDStream.scala:179)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1$$anonfun$apply$mcV$sp$1.apply$mcV$sp(ForEachDStream.scala:51)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1$$anonfun$apply$mcV$sp$1.apply(ForEachDStream.scala:51)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1$$anonfun$apply$mcV$sp$1.apply(ForEachDStream.scala:51)
	at org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:416)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1.apply$mcV$sp(ForEachDStream.scala:50)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1.apply(ForEachDStream.scala:50)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1.apply(ForEachDStream.scala:50)
	at scala.util.Try$.apply(Try.scala:192)
	at org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)
	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler$$anonfun$run$1.apply$mcV$sp(JobScheduler.scala:257)
	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler$$anonfun$run$1.apply(JobScheduler.scala:257)
	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler$$anonfun$run$1.apply(JobScheduler.scala:257)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)
	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Traceback (most recent call last):
  File "/home/rjha/stream-benchmarking-scratch/streaming_test.py", line 45, in <module>
    ssc.awaitTermination()
  File "/home/rjha/spark-2.3.0-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/streaming/context.py", line 206, in awaitTermination
  File "/home/rjha/spark-2.3.0-bin-hadoop2.7/python/lib/py4j-0.10.6-src.zip/py4j/java_gateway.py", line 1160, in __call__
  File "/home/rjha/spark-2.3.0-bin-hadoop2.7/python/lib/py4j-0.10.6-src.zip/py4j/protocol.py", line 320, in get_return_value
py4j.protocol.Py4JJavaError: An error occurred while calling o22.awaitTermination.
: org.apache.spark.SparkException: An exception was raised by Python:
Traceback (most recent call last):
  File "/home/rjha/spark-2.3.0-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/streaming/util.py", line 65, in call
    r = self.func(t, *rdds)
  File "/home/rjha/spark-2.3.0-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/streaming/dstream.py", line 159, in <lambda>
    func = lambda t, rdd: old_func(rdd)
  File "/home/rjha/stream-benchmarking-scratch/streaming_test.py", line 37, in <lambda>
    w_2_ts.foreachRDD(lambda rdd: rdd.foreachPartition(lambda partition: send_partition_to_db(partition, out_port))) # storing
  File "/home/rjha/spark-2.3.0-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/rdd.py", line 814, in foreachPartition
    self.mapPartitions(func).count()  # Force evaluation
  File "/home/rjha/spark-2.3.0-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/rdd.py", line 1056, in count
    return self.mapPartitions(lambda i: [sum(1 for _ in i)]).sum()
  File "/home/rjha/spark-2.3.0-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/rdd.py", line 1047, in sum
    return self.mapPartitions(lambda x: [sum(x)]).fold(0, operator.add)
  File "/home/rjha/spark-2.3.0-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/rdd.py", line 921, in fold
    vals = self.mapPartitions(func).collect()
  File "/home/rjha/spark-2.3.0-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/rdd.py", line 824, in collect
    port = self.ctx._jvm.PythonRDD.collectAndServe(self._jrdd.rdd())
  File "/home/rjha/spark-2.3.0-bin-hadoop2.7/python/lib/py4j-0.10.6-src.zip/py4j/java_gateway.py", line 1160, in __call__
    answer, self.gateway_client, self.target_id, self.name)
  File "/home/rjha/spark-2.3.0-bin-hadoop2.7/python/lib/py4j-0.10.6-src.zip/py4j/protocol.py", line 320, in get_return_value
    format(target_id, ".", name), value)
Py4JJavaError: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.
: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 19.0 failed 1 times, most recent failure: Lost task 0.0 in stage 19.0 (TID 83, localhost, executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):
  File "/home/rjha/spark-2.3.0-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/worker.py", line 229, in main
    process()
  File "/home/rjha/spark-2.3.0-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/worker.py", line 224, in process
    serializer.dump_stream(func(split_index, iterator), outfile)
  File "/home/rjha/spark-2.3.0-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/rdd.py", line 2438, in pipeline_func
  File "/home/rjha/spark-2.3.0-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/rdd.py", line 2438, in pipeline_func
  File "/home/rjha/spark-2.3.0-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/rdd.py", line 2438, in pipeline_func
  File "/home/rjha/spark-2.3.0-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/rdd.py", line 362, in func
  File "/home/rjha/spark-2.3.0-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/rdd.py", line 809, in func
  File "/home/rjha/stream-benchmarking-scratch/streaming_test.py", line 37, in <lambda>
    w_2_ts.foreachRDD(lambda rdd: rdd.foreachPartition(lambda partition: send_partition_to_db(partition, out_port))) # storing
  File "/home/rjha/stream-benchmarking-scratch/streaming_test.py", line 17, in send_partition_to_db
    sock.connect(('localhost', out_port))
  File "/usr/lib/python2.7/socket.py", line 228, in meth
    return getattr(self._sock,name)(*args)
error: [Errno 111] Connection refused

	at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:298)
	at org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:438)
	at org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:421)
	at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:252)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)
	at scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48)
	at scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310)
	at org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)
	at scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302)
	at org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)
	at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289)
	at org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)
	at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$12.apply(RDD.scala:939)
	at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$12.apply(RDD.scala:939)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2067)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2067)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
	at org.apache.spark.scheduler.Task.run(Task.scala:109)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1599)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1587)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1586)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1586)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:831)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:831)
	at scala.Option.foreach(Option.scala:257)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:831)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1820)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1769)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1758)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)
	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:642)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2027)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2048)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2067)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2092)
	at org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:939)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)
	at org.apache.spark.rdd.RDD.withScope(RDD.scala:363)
	at org.apache.spark.rdd.RDD.collect(RDD.scala:938)
	at org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:153)
	at org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)
	at sun.reflect.GeneratedMethodAccessor43.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.GatewayConnection.run(GatewayConnection.java:214)
	at java.lang.Thread.run(Thread.java:748)
Caused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):
  File "/home/rjha/spark-2.3.0-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/worker.py", line 229, in main
    process()
  File "/home/rjha/spark-2.3.0-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/worker.py", line 224, in process
    serializer.dump_stream(func(split_index, iterator), outfile)
  File "/home/rjha/spark-2.3.0-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/rdd.py", line 2438, in pipeline_func
  File "/home/rjha/spark-2.3.0-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/rdd.py", line 2438, in pipeline_func
  File "/home/rjha/spark-2.3.0-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/rdd.py", line 2438, in pipeline_func
  File "/home/rjha/spark-2.3.0-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/rdd.py", line 362, in func
  File "/home/rjha/spark-2.3.0-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/rdd.py", line 809, in func
  File "/home/rjha/stream-benchmarking-scratch/streaming_test.py", line 37, in <lambda>
    w_2_ts.foreachRDD(lambda rdd: rdd.foreachPartition(lambda partition: send_partition_to_db(partition, out_port))) # storing
  File "/home/rjha/stream-benchmarking-scratch/streaming_test.py", line 17, in send_partition_to_db
    sock.connect(('localhost', out_port))
  File "/usr/lib/python2.7/socket.py", line 228, in meth
    return getattr(self._sock,name)(*args)
error: [Errno 111] Connection refused

	at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:298)
	at org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:438)
	at org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:421)
	at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:252)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)
	at scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48)
	at scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310)
	at org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)
	at scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302)
	at org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)
	at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289)
	at org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)
	at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$12.apply(RDD.scala:939)
	at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$12.apply(RDD.scala:939)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2067)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2067)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
	at org.apache.spark.scheduler.Task.run(Task.scala:109)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	... 1 more


	at org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)
	at org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)
	at org.apache.spark.streaming.api.python.PythonDStream$$anonfun$callForeachRDD$1.apply(PythonDStream.scala:179)
	at org.apache.spark.streaming.api.python.PythonDStream$$anonfun$callForeachRDD$1.apply(PythonDStream.scala:179)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1$$anonfun$apply$mcV$sp$1.apply$mcV$sp(ForEachDStream.scala:51)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1$$anonfun$apply$mcV$sp$1.apply(ForEachDStream.scala:51)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1$$anonfun$apply$mcV$sp$1.apply(ForEachDStream.scala:51)
	at org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:416)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1.apply$mcV$sp(ForEachDStream.scala:50)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1.apply(ForEachDStream.scala:50)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1.apply(ForEachDStream.scala:50)
	at scala.util.Try$.apply(Try.scala:192)
	at org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)
	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler$$anonfun$run$1.apply$mcV$sp(JobScheduler.scala:257)
	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler$$anonfun$run$1.apply(JobScheduler.scala:257)
	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler$$anonfun$run$1.apply(JobScheduler.scala:257)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)
	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

2018-04-26 01:59:58 INFO  SparkContext:54 - Starting job: call at /home/rjha/spark-2.3.0-bin-hadoop2.7/python/lib/py4j-0.10.6-src.zip/py4j/java_gateway.py:2257
2018-04-26 01:59:58 INFO  DAGScheduler:54 - Got job 22 (call at /home/rjha/spark-2.3.0-bin-hadoop2.7/python/lib/py4j-0.10.6-src.zip/py4j/java_gateway.py:2257) with 5 output partitions
2018-04-26 01:59:58 INFO  DAGScheduler:54 - Final stage: ResultStage 20 (call at /home/rjha/spark-2.3.0-bin-hadoop2.7/python/lib/py4j-0.10.6-src.zip/py4j/java_gateway.py:2257)
2018-04-26 01:59:58 INFO  DAGScheduler:54 - Parents of final stage: List()
2018-04-26 01:59:58 INFO  DAGScheduler:54 - Missing parents: List()
2018-04-26 01:59:58 INFO  DAGScheduler:54 - Submitting ResultStage 20 (PythonRDD[58] at call at /home/rjha/spark-2.3.0-bin-hadoop2.7/python/lib/py4j-0.10.6-src.zip/py4j/java_gateway.py:2257), which has no missing parents
2018-04-26 01:59:58 INFO  MemoryStore:54 - Block broadcast_20 stored as values in memory (estimated size 7.1 KB, free 413.8 MB)
2018-04-26 01:59:58 INFO  MemoryStore:54 - Block broadcast_20_piece0 stored as bytes in memory (estimated size 4.1 KB, free 413.8 MB)
2018-04-26 01:59:58 INFO  BlockManagerInfo:54 - Added broadcast_20_piece0 in memory on 10.0.2.15:33681 (size: 4.1 KB, free: 413.9 MB)
2018-04-26 01:59:58 INFO  SparkContext:54 - Created broadcast 20 from broadcast at DAGScheduler.scala:1039
2018-04-26 01:59:58 INFO  DAGScheduler:54 - Submitting 5 missing tasks from ResultStage 20 (PythonRDD[58] at call at /home/rjha/spark-2.3.0-bin-hadoop2.7/python/lib/py4j-0.10.6-src.zip/py4j/java_gateway.py:2257) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4))
2018-04-26 01:59:58 INFO  TaskSchedulerImpl:54 - Adding task set 20.0 with 5 tasks
2018-04-26 01:59:58 INFO  TaskSetManager:54 - Starting task 0.0 in stage 20.0 (TID 85, localhost, executor driver, partition 0, ANY, 7772 bytes)
2018-04-26 01:59:58 INFO  Executor:54 - Running task 0.0 in stage 20.0 (TID 85)
2018-04-26 01:59:58 INFO  BlockManager:54 - Found block input-0-1524722389800 locally
2018-04-26 01:59:58 INFO  PythonRunner:54 - Times: total = 52, boot = 16, init = 36, finish = 0
2018-04-26 01:59:58 INFO  PythonRunner:54 - Times: total = 44, boot = 21, init = 19, finish = 4
2018-04-26 01:59:58 INFO  Executor:54 - Finished task 0.0 in stage 20.0 (TID 85). 1267 bytes result sent to driver
2018-04-26 01:59:58 INFO  TaskSetManager:54 - Starting task 1.0 in stage 20.0 (TID 86, localhost, executor driver, partition 1, ANY, 7772 bytes)
2018-04-26 01:59:58 INFO  Executor:54 - Running task 1.0 in stage 20.0 (TID 86)
2018-04-26 01:59:58 INFO  MemoryStore:54 - Block input-0-1524722398200 stored as bytes in memory (estimated size 419.0 B, free 413.8 MB)
2018-04-26 01:59:58 INFO  TaskSetManager:54 - Finished task 0.0 in stage 20.0 (TID 85) in 67 ms on localhost (executor driver) (1/5)
2018-04-26 01:59:58 INFO  BlockManager:54 - Found block input-0-1524722390000 locally
2018-04-26 01:59:58 INFO  BlockManagerInfo:54 - Added input-0-1524722398200 in memory on 10.0.2.15:33681 (size: 419.0 B, free: 413.9 MB)
2018-04-26 01:59:58 WARN  RandomBlockReplicationPolicy:66 - Expecting 1 replicas with only 0 peer/s.
2018-04-26 01:59:58 WARN  BlockManager:66 - Block input-0-1524722398200 replicated to only 0 peer(s) instead of 1 peers
2018-04-26 01:59:58 INFO  BlockGenerator:54 - Pushed block input-0-1524722398200
2018-04-26 01:59:58 ERROR DAGScheduler:91 - Failed to update accumulators for task 0
java.net.SocketException: Connection reset
	at java.net.SocketInputStream.read(SocketInputStream.java:210)
	at java.net.SocketInputStream.read(SocketInputStream.java:141)
	at java.net.SocketInputStream.read(SocketInputStream.java:224)
	at org.apache.spark.api.python.PythonAccumulatorV2.merge(PythonRDD.scala:617)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$updateAccumulators$1.apply(DAGScheduler.scala:1132)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$updateAccumulators$1.apply(DAGScheduler.scala:1124)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)
	at org.apache.spark.scheduler.DAGScheduler.updateAccumulators(DAGScheduler.scala:1124)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskCompletion(DAGScheduler.scala:1207)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1817)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1769)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1758)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)
2018-04-26 01:59:58 INFO  JobScheduler:54 - Finished job streaming job 1524722391000 ms.1 from job set of time 1524722391000 ms
2018-04-26 01:59:58 INFO  JobScheduler:54 - Starting job streaming job 1524722392000 ms.0 from job set of time 1524722392000 ms
2018-04-26 01:59:58 ERROR JobScheduler:91 - Error running job streaming job 1524722391000 ms.1
py4j.Py4JException: Error while sending a command.
	at py4j.CallbackClient.sendCommand(CallbackClient.java:357)
	at py4j.CallbackClient.sendCommand(CallbackClient.java:316)
	at py4j.reflection.PythonProxyHandler.invoke(PythonProxyHandler.java:106)
	at com.sun.proxy.$Proxy16.call(Unknown Source)
	at org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:92)
	at org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)
	at org.apache.spark.streaming.api.python.PythonDStream$$anonfun$callForeachRDD$1.apply(PythonDStream.scala:179)
	at org.apache.spark.streaming.api.python.PythonDStream$$anonfun$callForeachRDD$1.apply(PythonDStream.scala:179)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1$$anonfun$apply$mcV$sp$1.apply$mcV$sp(ForEachDStream.scala:51)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1$$anonfun$apply$mcV$sp$1.apply(ForEachDStream.scala:51)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1$$anonfun$apply$mcV$sp$1.apply(ForEachDStream.scala:51)
	at org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:416)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1.apply$mcV$sp(ForEachDStream.scala:50)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1.apply(ForEachDStream.scala:50)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1.apply(ForEachDStream.scala:50)
	at scala.util.Try$.apply(Try.scala:192)
	at org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)
	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler$$anonfun$run$1.apply$mcV$sp(JobScheduler.scala:257)
	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler$$anonfun$run$1.apply(JobScheduler.scala:257)
	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler$$anonfun$run$1.apply(JobScheduler.scala:257)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)
	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: py4j.Py4JNetworkException
	at py4j.CallbackConnection.sendCommand(CallbackConnection.java:138)
	at py4j.CallbackClient.sendCommand(CallbackClient.java:344)
	... 24 more
2018-04-26 01:59:58 INFO  JobScheduler:54 - Finished job streaming job 1524722392000 ms.0 from job set of time 1524722392000 ms
2018-04-26 01:59:58 INFO  JobScheduler:54 - Starting job streaming job 1524722392000 ms.1 from job set of time 1524722392000 ms
2018-04-26 01:59:58 INFO  JobScheduler:54 - Finished job streaming job 1524722392000 ms.1 from job set of time 1524722392000 ms
2018-04-26 01:59:58 INFO  StreamingContext:54 - Invoking stop(stopGracefully=false) from shutdown hook
2018-04-26 01:59:58 INFO  JobScheduler:54 - Starting job streaming job 1524722393000 ms.0 from job set of time 1524722393000 ms
2018-04-26 01:59:58 INFO  JobScheduler:54 - Finished job streaming job 1524722393000 ms.0 from job set of time 1524722393000 ms
2018-04-26 01:59:58 INFO  JobScheduler:54 - Starting job streaming job 1524722393000 ms.1 from job set of time 1524722393000 ms
2018-04-26 01:59:58 INFO  JobScheduler:54 - Finished job streaming job 1524722393000 ms.1 from job set of time 1524722393000 ms
2018-04-26 01:59:58 INFO  PythonRunner:54 - Times: total = 214, boot = 213, init = 0, finish = 1
2018-04-26 01:59:58 INFO  ReceiverTracker:54 - Sent stop signal to all 1 receivers
2018-04-26 01:59:58 INFO  MemoryStore:54 - Block input-0-1524722398400 stored as bytes in memory (estimated size 421.0 B, free 413.8 MB)
2018-04-26 01:59:58 INFO  ReceiverSupervisorImpl:54 - Received stop signal
2018-04-26 01:59:58 INFO  PythonRunner:54 - Times: total = 202, boot = 187, init = 15, finish = 0
2018-04-26 01:59:58 INFO  Executor:54 - Finished task 1.0 in stage 20.0 (TID 86). 1310 bytes result sent to driver
2018-04-26 01:59:58 INFO  BlockManagerInfo:54 - Added input-0-1524722398400 in memory on 10.0.2.15:33681 (size: 421.0 B, free: 413.9 MB)
2018-04-26 01:59:58 INFO  TaskSetManager:54 - Starting task 2.0 in stage 20.0 (TID 87, localhost, executor driver, partition 2, ANY, 7772 bytes)
2018-04-26 01:59:58 INFO  TaskSetManager:54 - Finished task 1.0 in stage 20.0 (TID 86) in 248 ms on localhost (executor driver) (2/5)
2018-04-26 01:59:58 ERROR DAGScheduler:91 - Failed to update accumulators for task 1
java.net.SocketException: Broken pipe (Write failed)
	at java.net.SocketOutputStream.socketWrite0(Native Method)
	at java.net.SocketOutputStream.socketWrite(SocketOutputStream.java:111)
	at java.net.SocketOutputStream.write(SocketOutputStream.java:155)
	at java.io.BufferedOutputStream.flushBuffer(BufferedOutputStream.java:82)
	at java.io.BufferedOutputStream.flush(BufferedOutputStream.java:140)
	at java.io.DataOutputStream.flush(DataOutputStream.java:123)
	at org.apache.spark.api.python.PythonAccumulatorV2.merge(PythonRDD.scala:615)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$updateAccumulators$1.apply(DAGScheduler.scala:1132)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$updateAccumulators$1.apply(DAGScheduler.scala:1124)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)
	at org.apache.spark.scheduler.DAGScheduler.updateAccumulators(DAGScheduler.scala:1124)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskCompletion(DAGScheduler.scala:1207)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1817)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1769)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1758)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)
2018-04-26 01:59:58 WARN  RandomBlockReplicationPolicy:66 - Expecting 1 replicas with only 0 peer/s.
2018-04-26 01:59:58 WARN  BlockManager:66 - Block input-0-1524722398400 replicated to only 0 peer(s) instead of 1 peers
2018-04-26 01:59:58 INFO  Executor:54 - Running task 2.0 in stage 20.0 (TID 87)
2018-04-26 01:59:58 INFO  BlockManager:54 - Found block input-0-1524722390200 locally
2018-04-26 01:59:58 INFO  BlockGenerator:54 - Pushed block input-0-1524722398400
2018-04-26 01:59:58 INFO  ReceiverSupervisorImpl:54 - Stopping receiver with message: Stopped by driver: 
2018-04-26 01:59:58 INFO  JobScheduler:54 - Starting job streaming job 1524722394000 ms.0 from job set of time 1524722394000 ms
2018-04-26 01:59:58 ERROR JobScheduler:91 - Error running job streaming job 1524722392000 ms.0
py4j.Py4JException: Error while sending a command.
	at py4j.CallbackClient.sendCommand(CallbackClient.java:357)
	at py4j.CallbackClient.sendCommand(CallbackClient.java:316)
	at py4j.reflection.PythonProxyHandler.invoke(PythonProxyHandler.java:106)
	at com.sun.proxy.$Proxy16.call(Unknown Source)
	at org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:92)
	at org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)
	at org.apache.spark.streaming.api.python.PythonDStream$$anonfun$callForeachRDD$1.apply(PythonDStream.scala:179)
	at org.apache.spark.streaming.api.python.PythonDStream$$anonfun$callForeachRDD$1.apply(PythonDStream.scala:179)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1$$anonfun$apply$mcV$sp$1.apply$mcV$sp(ForEachDStream.scala:51)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1$$anonfun$apply$mcV$sp$1.apply(ForEachDStream.scala:51)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1$$anonfun$apply$mcV$sp$1.apply(ForEachDStream.scala:51)
	at org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:416)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1.apply$mcV$sp(ForEachDStream.scala:50)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1.apply(ForEachDStream.scala:50)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1.apply(ForEachDStream.scala:50)
	at scala.util.Try$.apply(Try.scala:192)
	at org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)
	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler$$anonfun$run$1.apply$mcV$sp(JobScheduler.scala:257)
	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler$$anonfun$run$1.apply(JobScheduler.scala:257)
	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler$$anonfun$run$1.apply(JobScheduler.scala:257)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)
	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: py4j.Py4JNetworkException
	at py4j.CallbackConnection.sendCommand(CallbackConnection.java:138)
	at py4j.CallbackClient.sendCommand(CallbackClient.java:344)
	... 24 more
2018-04-26 01:59:58 INFO  JobScheduler:54 - Finished job streaming job 1524722394000 ms.0 from job set of time 1524722394000 ms
2018-04-26 01:59:58 INFO  JobScheduler:54 - Starting job streaming job 1524722394000 ms.1 from job set of time 1524722394000 ms
2018-04-26 01:59:58 ERROR JobScheduler:91 - Error running job streaming job 1524722392000 ms.1
py4j.Py4JException: Error while sending a command.
	at py4j.CallbackClient.sendCommand(CallbackClient.java:357)
	at py4j.CallbackClient.sendCommand(CallbackClient.java:316)
	at py4j.reflection.PythonProxyHandler.invoke(PythonProxyHandler.java:106)
	at com.sun.proxy.$Proxy16.call(Unknown Source)
	at org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:92)
	at org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)
	at org.apache.spark.streaming.api.python.PythonDStream$$anonfun$callForeachRDD$1.apply(PythonDStream.scala:179)
	at org.apache.spark.streaming.api.python.PythonDStream$$anonfun$callForeachRDD$1.apply(PythonDStream.scala:179)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1$$anonfun$apply$mcV$sp$1.apply$mcV$sp(ForEachDStream.scala:51)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1$$anonfun$apply$mcV$sp$1.apply(ForEachDStream.scala:51)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1$$anonfun$apply$mcV$sp$1.apply(ForEachDStream.scala:51)
	at org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:416)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1.apply$mcV$sp(ForEachDStream.scala:50)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1.apply(ForEachDStream.scala:50)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1.apply(ForEachDStream.scala:50)
	at scala.util.Try$.apply(Try.scala:192)
	at org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)
	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler$$anonfun$run$1.apply$mcV$sp(JobScheduler.scala:257)
	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler$$anonfun$run$1.apply(JobScheduler.scala:257)
	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler$$anonfun$run$1.apply(JobScheduler.scala:257)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)
	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: py4j.Py4JNetworkException: Error while sending a command: c
p3
call
L1524722392000
lo493
e

	at py4j.CallbackConnection.sendCommand(CallbackConnection.java:133)
	at py4j.CallbackClient.sendCommand(CallbackClient.java:344)
	... 24 more
Caused by: java.net.SocketException: Connection reset
	at java.net.SocketInputStream.read(SocketInputStream.java:210)
	at java.net.SocketInputStream.read(SocketInputStream.java:141)
	at sun.nio.cs.StreamDecoder.readBytes(StreamDecoder.java:284)
	at sun.nio.cs.StreamDecoder.implRead(StreamDecoder.java:326)
	at sun.nio.cs.StreamDecoder.read(StreamDecoder.java:178)
	at java.io.InputStreamReader.read(InputStreamReader.java:184)
	at java.io.BufferedReader.fill(BufferedReader.java:161)
	at java.io.BufferedReader.readLine(BufferedReader.java:324)
	at java.io.BufferedReader.readLine(BufferedReader.java:389)
	at py4j.CallbackConnection.readBlockingResponse(CallbackConnection.java:149)
	at py4j.CallbackConnection.sendCommand(CallbackConnection.java:128)
	... 25 more
2018-04-26 01:59:58 INFO  JobScheduler:54 - Finished job streaming job 1524722394000 ms.1 from job set of time 1524722394000 ms
2018-04-26 01:59:58 INFO  SocketReceiver:54 - Closed socket to localhost:9998
2018-04-26 01:59:58 INFO  ReceiverSupervisorImpl:54 - Called receiver onStop
2018-04-26 01:59:58 INFO  ReceiverSupervisorImpl:54 - Deregistering receiver 0
2018-04-26 01:59:58 WARN  SocketReceiver:87 - Error receiving data
java.net.SocketException: Socket closed
	at java.net.SocketInputStream.socketRead0(Native Method)
	at java.net.SocketInputStream.socketRead(SocketInputStream.java:116)
	at java.net.SocketInputStream.read(SocketInputStream.java:171)
	at java.net.SocketInputStream.read(SocketInputStream.java:141)
	at sun.nio.cs.StreamDecoder.readBytes(StreamDecoder.java:284)
	at sun.nio.cs.StreamDecoder.implRead(StreamDecoder.java:326)
	at sun.nio.cs.StreamDecoder.read(StreamDecoder.java:178)
	at java.io.InputStreamReader.read(InputStreamReader.java:184)
	at java.io.BufferedReader.fill(BufferedReader.java:161)
	at java.io.BufferedReader.readLine(BufferedReader.java:324)
	at java.io.BufferedReader.readLine(BufferedReader.java:389)
	at org.apache.spark.streaming.dstream.SocketReceiver$$anon$1.getNext(SocketInputDStream.scala:121)
	at org.apache.spark.streaming.dstream.SocketReceiver$$anon$1.getNext(SocketInputDStream.scala:119)
	at org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:73)
	at org.apache.spark.streaming.dstream.SocketReceiver.receive(SocketInputDStream.scala:91)
	at org.apache.spark.streaming.dstream.SocketReceiver$$anon$2.run(SocketInputDStream.scala:72)
2018-04-26 01:59:58 ERROR ReceiverTracker:70 - Deregistered receiver for stream 0: Stopped by driver
2018-04-26 01:59:58 INFO  ReceiverSupervisorImpl:54 - Stopped receiver 0
2018-04-26 01:59:58 INFO  BlockGenerator:54 - Stopping BlockGenerator
2018-04-26 01:59:58 INFO  JobScheduler:54 - Starting job streaming job 1524722395000 ms.0 from job set of time 1524722395000 ms
2018-04-26 01:59:58 INFO  JobScheduler:54 - Finished job streaming job 1524722395000 ms.0 from job set of time 1524722395000 ms
2018-04-26 01:59:58 INFO  JobScheduler:54 - Starting job streaming job 1524722395000 ms.1 from job set of time 1524722395000 ms
2018-04-26 01:59:58 INFO  JobScheduler:54 - Finished job streaming job 1524722395000 ms.1 from job set of time 1524722395000 ms
2018-04-26 01:59:58 WARN  ReceiverSupervisorImpl:87 - Restarting receiver with delay 2000 ms: Error receiving data
java.net.SocketException: Socket closed
	at java.net.SocketInputStream.socketRead0(Native Method)
	at java.net.SocketInputStream.socketRead(SocketInputStream.java:116)
	at java.net.SocketInputStream.read(SocketInputStream.java:171)
	at java.net.SocketInputStream.read(SocketInputStream.java:141)
	at sun.nio.cs.StreamDecoder.readBytes(StreamDecoder.java:284)
	at sun.nio.cs.StreamDecoder.implRead(StreamDecoder.java:326)
	at sun.nio.cs.StreamDecoder.read(StreamDecoder.java:178)
	at java.io.InputStreamReader.read(InputStreamReader.java:184)
	at java.io.BufferedReader.fill(BufferedReader.java:161)
	at java.io.BufferedReader.readLine(BufferedReader.java:324)
	at java.io.BufferedReader.readLine(BufferedReader.java:389)
	at org.apache.spark.streaming.dstream.SocketReceiver$$anon$1.getNext(SocketInputDStream.scala:121)
	at org.apache.spark.streaming.dstream.SocketReceiver$$anon$1.getNext(SocketInputDStream.scala:119)
	at org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:73)
	at org.apache.spark.streaming.dstream.SocketReceiver.receive(SocketInputDStream.scala:91)
	at org.apache.spark.streaming.dstream.SocketReceiver$$anon$2.run(SocketInputDStream.scala:72)
2018-04-26 01:59:58 INFO  PythonRunner:54 - Times: total = 15, boot = 3, init = 12, finish = 0
2018-04-26 01:59:58 INFO  PythonRunner:54 - Times: total = 99, boot = 2, init = 97, finish = 0
2018-04-26 01:59:58 INFO  Executor:54 - Finished task 2.0 in stage 20.0 (TID 87). 1267 bytes result sent to driver
2018-04-26 01:59:58 INFO  TaskSetManager:54 - Starting task 3.0 in stage 20.0 (TID 88, localhost, executor driver, partition 3, ANY, 7772 bytes)
2018-04-26 01:59:58 INFO  TaskSetManager:54 - Finished task 2.0 in stage 20.0 (TID 87) in 126 ms on localhost (executor driver) (3/5)
2018-04-26 01:59:58 ERROR DAGScheduler:91 - Failed to update accumulators for task 2
java.net.SocketException: Broken pipe (Write failed)
	at java.net.SocketOutputStream.socketWrite0(Native Method)
	at java.net.SocketOutputStream.socketWrite(SocketOutputStream.java:111)
	at java.net.SocketOutputStream.write(SocketOutputStream.java:155)
	at java.io.BufferedOutputStream.flushBuffer(BufferedOutputStream.java:82)
	at java.io.BufferedOutputStream.flush(BufferedOutputStream.java:140)
	at java.io.DataOutputStream.flush(DataOutputStream.java:123)
	at org.apache.spark.api.python.PythonAccumulatorV2.merge(PythonRDD.scala:615)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$updateAccumulators$1.apply(DAGScheduler.scala:1132)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$updateAccumulators$1.apply(DAGScheduler.scala:1124)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)
	at org.apache.spark.scheduler.DAGScheduler.updateAccumulators(DAGScheduler.scala:1124)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskCompletion(DAGScheduler.scala:1207)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1817)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1769)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1758)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)
2018-04-26 01:59:58 INFO  Executor:54 - Running task 3.0 in stage 20.0 (TID 88)
2018-04-26 01:59:58 INFO  BlockManager:54 - Found block input-0-1524722390400 locally
2018-04-26 01:59:58 INFO  JobScheduler:54 - Starting job streaming job 1524722396000 ms.0 from job set of time 1524722396000 ms
2018-04-26 01:59:58 INFO  JobScheduler:54 - Finished job streaming job 1524722396000 ms.0 from job set of time 1524722396000 ms
2018-04-26 01:59:58 INFO  JobScheduler:54 - Starting job streaming job 1524722396000 ms.1 from job set of time 1524722396000 ms
2018-04-26 01:59:58 INFO  JobScheduler:54 - Finished job streaming job 1524722396000 ms.1 from job set of time 1524722396000 ms
2018-04-26 01:59:58 INFO  ReceiverSupervisorImpl:54 - Stopping receiver with message: Restarting receiver with delay 2000ms: Error receiving data: java.net.SocketException: Socket closed
2018-04-26 01:59:58 WARN  ReceiverSupervisorImpl:66 - Receiver has been stopped
2018-04-26 01:59:58 INFO  MemoryStore:54 - Block input-0-1524722398600 stored as bytes in memory (estimated size 135.0 B, free 413.8 MB)
2018-04-26 01:59:58 INFO  BlockManagerInfo:54 - Added input-0-1524722398600 in memory on 10.0.2.15:33681 (size: 135.0 B, free: 413.9 MB)
2018-04-26 01:59:58 WARN  RandomBlockReplicationPolicy:66 - Expecting 1 replicas with only 0 peer/s.
2018-04-26 01:59:58 WARN  BlockManager:66 - Block input-0-1524722398600 replicated to only 0 peer(s) instead of 1 peers
2018-04-26 01:59:58 INFO  BlockGenerator:54 - Pushed block input-0-1524722398600
2018-04-26 01:59:58 INFO  JobScheduler:54 - Starting job streaming job 1524722397000 ms.0 from job set of time 1524722397000 ms
2018-04-26 01:59:58 INFO  JobScheduler:54 - Finished job streaming job 1524722397000 ms.0 from job set of time 1524722397000 ms
2018-04-26 01:59:58 INFO  JobScheduler:54 - Starting job streaming job 1524722397000 ms.1 from job set of time 1524722397000 ms
2018-04-26 01:59:58 ERROR JobScheduler:91 - Error running job streaming job 1524722393000 ms.0
py4j.Py4JException: Error while obtaining a new communication channel
	at py4j.CallbackClient.getConnectionLock(CallbackClient.java:218)
	at py4j.CallbackClient.sendCommand(CallbackClient.java:337)
	at py4j.CallbackClient.sendCommand(CallbackClient.java:316)
	at py4j.reflection.PythonProxyHandler.invoke(PythonProxyHandler.java:106)
	at com.sun.proxy.$Proxy16.call(Unknown Source)
	at org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:92)
	at org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)
	at org.apache.spark.streaming.api.python.PythonDStream$$anonfun$callForeachRDD$1.apply(PythonDStream.scala:179)
	at org.apache.spark.streaming.api.python.PythonDStream$$anonfun$callForeachRDD$1.apply(PythonDStream.scala:179)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1$$anonfun$apply$mcV$sp$1.apply$mcV$sp(ForEachDStream.scala:51)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1$$anonfun$apply$mcV$sp$1.apply(ForEachDStream.scala:51)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1$$anonfun$apply$mcV$sp$1.apply(ForEachDStream.scala:51)
	at org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:416)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1.apply$mcV$sp(ForEachDStream.scala:50)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1.apply(ForEachDStream.scala:50)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1.apply(ForEachDStream.scala:50)
	at scala.util.Try$.apply(Try.scala:192)
	at org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)
	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler$$anonfun$run$1.apply$mcV$sp(JobScheduler.scala:257)
	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler$$anonfun$run$1.apply(JobScheduler.scala:257)
	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler$$anonfun$run$1.apply(JobScheduler.scala:257)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)
	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.net.ConnectException: Connection refused (Connection refused)
	at java.net.PlainSocketImpl.socketConnect(Native Method)
	at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:350)
	at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:206)
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:188)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:589)
	at java.net.Socket.connect(Socket.java:538)
	at java.net.Socket.<init>(Socket.java:434)
	at java.net.Socket.<init>(Socket.java:244)
	at javax.net.DefaultSocketFactory.createSocket(SocketFactory.java:277)
	at py4j.CallbackConnection.start(CallbackConnection.java:206)
	at py4j.CallbackClient.getConnection(CallbackClient.java:199)
	at py4j.CallbackClient.getConnectionLock(CallbackClient.java:211)
	... 25 more
2018-04-26 01:59:58 INFO  JobScheduler:54 - Finished job streaming job 1524722397000 ms.1 from job set of time 1524722397000 ms
2018-04-26 01:59:58 INFO  PythonRunner:54 - Times: total = 58, boot = 42, init = 16, finish = 0
2018-04-26 01:59:58 INFO  JobScheduler:54 - Starting job streaming job 1524722398000 ms.0 from job set of time 1524722398000 ms
2018-04-26 01:59:58 INFO  JobScheduler:54 - Finished job streaming job 1524722398000 ms.0 from job set of time 1524722398000 ms
2018-04-26 01:59:58 INFO  JobScheduler:54 - Starting job streaming job 1524722398000 ms.1 from job set of time 1524722398000 ms
2018-04-26 01:59:58 INFO  JobScheduler:54 - Finished job streaming job 1524722398000 ms.1 from job set of time 1524722398000 ms
2018-04-26 01:59:58 ERROR PythonDStream$$anon$1:91 - Cannot connect to Python process. It's probably dead. Stopping StreamingContext.
py4j.Py4JException: Error while obtaining a new communication channel
	at py4j.CallbackClient.getConnectionLock(CallbackClient.java:218)
	at py4j.CallbackClient.sendCommand(CallbackClient.java:337)
	at py4j.CallbackClient.sendCommand(CallbackClient.java:316)
	at py4j.reflection.PythonProxyHandler.invoke(PythonProxyHandler.java:106)
	at com.sun.proxy.$Proxy16.call(Unknown Source)
	at org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:92)
	at org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)
	at org.apache.spark.streaming.api.python.PythonDStream$$anonfun$callForeachRDD$1.apply(PythonDStream.scala:179)
	at org.apache.spark.streaming.api.python.PythonDStream$$anonfun$callForeachRDD$1.apply(PythonDStream.scala:179)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1$$anonfun$apply$mcV$sp$1.apply$mcV$sp(ForEachDStream.scala:51)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1$$anonfun$apply$mcV$sp$1.apply(ForEachDStream.scala:51)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1$$anonfun$apply$mcV$sp$1.apply(ForEachDStream.scala:51)
	at org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:416)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1.apply$mcV$sp(ForEachDStream.scala:50)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1.apply(ForEachDStream.scala:50)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1.apply(ForEachDStream.scala:50)
	at scala.util.Try$.apply(Try.scala:192)
	at org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)
	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler$$anonfun$run$1.apply$mcV$sp(JobScheduler.scala:257)
	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler$$anonfun$run$1.apply(JobScheduler.scala:257)
	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler$$anonfun$run$1.apply(JobScheduler.scala:257)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)
	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.net.ConnectException: Connection refused (Connection refused)
	at java.net.PlainSocketImpl.socketConnect(Native Method)
	at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:350)
	at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:206)
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:188)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:589)
	at java.net.Socket.connect(Socket.java:538)
	at java.net.Socket.<init>(Socket.java:434)
	at java.net.Socket.<init>(Socket.java:244)
	at javax.net.DefaultSocketFactory.createSocket(SocketFactory.java:277)
	at py4j.CallbackConnection.start(CallbackConnection.java:206)
	at py4j.CallbackClient.getConnection(CallbackClient.java:199)
	at py4j.CallbackClient.getConnectionLock(CallbackClient.java:211)
	... 25 more
2018-04-26 01:59:58 INFO  PythonRunner:54 - Times: total = 61, boot = 53, init = 8, finish = 0
2018-04-26 01:59:58 INFO  Executor:54 - Finished task 3.0 in stage 20.0 (TID 88). 1267 bytes result sent to driver
2018-04-26 01:59:58 INFO  TaskSetManager:54 - Starting task 4.0 in stage 20.0 (TID 89, localhost, executor driver, partition 4, ANY, 7772 bytes)
2018-04-26 01:59:58 INFO  TaskSetManager:54 - Finished task 3.0 in stage 20.0 (TID 88) in 87 ms on localhost (executor driver) (4/5)
2018-04-26 01:59:58 ERROR DAGScheduler:91 - Failed to update accumulators for task 3
java.net.SocketException: Broken pipe (Write failed)
	at java.net.SocketOutputStream.socketWrite0(Native Method)
	at java.net.SocketOutputStream.socketWrite(SocketOutputStream.java:111)
	at java.net.SocketOutputStream.write(SocketOutputStream.java:155)
	at java.io.BufferedOutputStream.flushBuffer(BufferedOutputStream.java:82)
	at java.io.BufferedOutputStream.flush(BufferedOutputStream.java:140)
	at java.io.DataOutputStream.flush(DataOutputStream.java:123)
	at org.apache.spark.api.python.PythonAccumulatorV2.merge(PythonRDD.scala:615)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$updateAccumulators$1.apply(DAGScheduler.scala:1132)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$updateAccumulators$1.apply(DAGScheduler.scala:1124)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)
	at org.apache.spark.scheduler.DAGScheduler.updateAccumulators(DAGScheduler.scala:1124)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskCompletion(DAGScheduler.scala:1207)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1817)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1769)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1758)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)
2018-04-26 01:59:58 INFO  Executor:54 - Running task 4.0 in stage 20.0 (TID 89)
2018-04-26 01:59:58 INFO  BlockManager:54 - Found block input-0-1524722390600 locally
2018-04-26 01:59:58 ERROR JobScheduler:91 - Error running job streaming job 1524722393000 ms.1
py4j.Py4JException: Error while obtaining a new communication channel
	at py4j.CallbackClient.getConnectionLock(CallbackClient.java:218)
	at py4j.CallbackClient.sendCommand(CallbackClient.java:337)
	at py4j.CallbackClient.sendCommand(CallbackClient.java:316)
	at py4j.reflection.PythonProxyHandler.invoke(PythonProxyHandler.java:106)
	at com.sun.proxy.$Proxy16.call(Unknown Source)
	at org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:92)
	at org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)
	at org.apache.spark.streaming.api.python.PythonDStream$$anonfun$callForeachRDD$1.apply(PythonDStream.scala:179)
	at org.apache.spark.streaming.api.python.PythonDStream$$anonfun$callForeachRDD$1.apply(PythonDStream.scala:179)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1$$anonfun$apply$mcV$sp$1.apply$mcV$sp(ForEachDStream.scala:51)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1$$anonfun$apply$mcV$sp$1.apply(ForEachDStream.scala:51)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1$$anonfun$apply$mcV$sp$1.apply(ForEachDStream.scala:51)
	at org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:416)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1.apply$mcV$sp(ForEachDStream.scala:50)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1.apply(ForEachDStream.scala:50)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1.apply(ForEachDStream.scala:50)
	at scala.util.Try$.apply(Try.scala:192)
	at org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)
	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler$$anonfun$run$1.apply$mcV$sp(JobScheduler.scala:257)
	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler$$anonfun$run$1.apply(JobScheduler.scala:257)
	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler$$anonfun$run$1.apply(JobScheduler.scala:257)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)
	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.net.ConnectException: Connection refused (Connection refused)
	at java.net.PlainSocketImpl.socketConnect(Native Method)
	at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:350)
	at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:206)
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:188)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:589)
	at java.net.Socket.connect(Socket.java:538)
	at java.net.Socket.<init>(Socket.java:434)
	at java.net.Socket.<init>(Socket.java:244)
	at javax.net.DefaultSocketFactory.createSocket(SocketFactory.java:277)
	at py4j.CallbackConnection.start(CallbackConnection.java:206)
	at py4j.CallbackClient.getConnection(CallbackClient.java:199)
	at py4j.CallbackClient.getConnectionLock(CallbackClient.java:211)
	... 25 more
2018-04-26 01:59:58 ERROR JobScheduler:91 - Error running job streaming job 1524722394000 ms.0
py4j.Py4JException: Error while obtaining a new communication channel
	at py4j.CallbackClient.getConnectionLock(CallbackClient.java:218)
	at py4j.CallbackClient.sendCommand(CallbackClient.java:337)
	at py4j.CallbackClient.sendCommand(CallbackClient.java:316)
	at py4j.reflection.PythonProxyHandler.invoke(PythonProxyHandler.java:106)
	at com.sun.proxy.$Proxy16.call(Unknown Source)
	at org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:92)
	at org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)
	at org.apache.spark.streaming.api.python.PythonDStream$$anonfun$callForeachRDD$1.apply(PythonDStream.scala:179)
	at org.apache.spark.streaming.api.python.PythonDStream$$anonfun$callForeachRDD$1.apply(PythonDStream.scala:179)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1$$anonfun$apply$mcV$sp$1.apply$mcV$sp(ForEachDStream.scala:51)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1$$anonfun$apply$mcV$sp$1.apply(ForEachDStream.scala:51)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1$$anonfun$apply$mcV$sp$1.apply(ForEachDStream.scala:51)
	at org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:416)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1.apply$mcV$sp(ForEachDStream.scala:50)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1.apply(ForEachDStream.scala:50)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1.apply(ForEachDStream.scala:50)
	at scala.util.Try$.apply(Try.scala:192)
	at org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)
	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler$$anonfun$run$1.apply$mcV$sp(JobScheduler.scala:257)
	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler$$anonfun$run$1.apply(JobScheduler.scala:257)
	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler$$anonfun$run$1.apply(JobScheduler.scala:257)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)
	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.net.ConnectException: Connection refused (Connection refused)
	at java.net.PlainSocketImpl.socketConnect(Native Method)
	at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:350)
	at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:206)
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:188)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:589)
	at java.net.Socket.connect(Socket.java:538)
	at java.net.Socket.<init>(Socket.java:434)
	at java.net.Socket.<init>(Socket.java:244)
	at javax.net.DefaultSocketFactory.createSocket(SocketFactory.java:277)
	at py4j.CallbackConnection.start(CallbackConnection.java:206)
	at py4j.CallbackClient.getConnection(CallbackClient.java:199)
	at py4j.CallbackClient.getConnectionLock(CallbackClient.java:211)
	... 25 more
2018-04-26 01:59:58 ERROR JobScheduler:91 - Error running job streaming job 1524722394000 ms.1
py4j.Py4JException: Cannot obtain a new communication channel
	at py4j.CallbackClient.sendCommand(CallbackClient.java:340)
	at py4j.CallbackClient.sendCommand(CallbackClient.java:316)
	at py4j.reflection.PythonProxyHandler.invoke(PythonProxyHandler.java:106)
	at com.sun.proxy.$Proxy16.call(Unknown Source)
	at org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:92)
	at org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)
	at org.apache.spark.streaming.api.python.PythonDStream$$anonfun$callForeachRDD$1.apply(PythonDStream.scala:179)
	at org.apache.spark.streaming.api.python.PythonDStream$$anonfun$callForeachRDD$1.apply(PythonDStream.scala:179)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1$$anonfun$apply$mcV$sp$1.apply$mcV$sp(ForEachDStream.scala:51)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1$$anonfun$apply$mcV$sp$1.apply(ForEachDStream.scala:51)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1$$anonfun$apply$mcV$sp$1.apply(ForEachDStream.scala:51)
	at org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:416)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1.apply$mcV$sp(ForEachDStream.scala:50)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1.apply(ForEachDStream.scala:50)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1.apply(ForEachDStream.scala:50)
	at scala.util.Try$.apply(Try.scala:192)
	at org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)
	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler$$anonfun$run$1.apply$mcV$sp(JobScheduler.scala:257)
	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler$$anonfun$run$1.apply(JobScheduler.scala:257)
	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler$$anonfun$run$1.apply(JobScheduler.scala:257)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)
	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
2018-04-26 01:59:58 ERROR JobScheduler:91 - Error running job streaming job 1524722395000 ms.0
py4j.Py4JException: Cannot obtain a new communication channel
	at py4j.CallbackClient.sendCommand(CallbackClient.java:340)
	at py4j.CallbackClient.sendCommand(CallbackClient.java:316)
	at py4j.reflection.PythonProxyHandler.invoke(PythonProxyHandler.java:106)
	at com.sun.proxy.$Proxy16.call(Unknown Source)
	at org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:92)
	at org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)
	at org.apache.spark.streaming.api.python.PythonDStream$$anonfun$callForeachRDD$1.apply(PythonDStream.scala:179)
	at org.apache.spark.streaming.api.python.PythonDStream$$anonfun$callForeachRDD$1.apply(PythonDStream.scala:179)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1$$anonfun$apply$mcV$sp$1.apply$mcV$sp(ForEachDStream.scala:51)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1$$anonfun$apply$mcV$sp$1.apply(ForEachDStream.scala:51)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1$$anonfun$apply$mcV$sp$1.apply(ForEachDStream.scala:51)
	at org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:416)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1.apply$mcV$sp(ForEachDStream.scala:50)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1.apply(ForEachDStream.scala:50)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1.apply(ForEachDStream.scala:50)
	at scala.util.Try$.apply(Try.scala:192)
	at org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)
	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler$$anonfun$run$1.apply$mcV$sp(JobScheduler.scala:257)
	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler$$anonfun$run$1.apply(JobScheduler.scala:257)
	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler$$anonfun$run$1.apply(JobScheduler.scala:257)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)
	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
2018-04-26 01:59:58 ERROR JobScheduler:91 - Error running job streaming job 1524722395000 ms.1
py4j.Py4JException: Cannot obtain a new communication channel
	at py4j.CallbackClient.sendCommand(CallbackClient.java:340)
	at py4j.CallbackClient.sendCommand(CallbackClient.java:316)
	at py4j.reflection.PythonProxyHandler.invoke(PythonProxyHandler.java:106)
	at com.sun.proxy.$Proxy16.call(Unknown Source)
	at org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:92)
	at org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)
	at org.apache.spark.streaming.api.python.PythonDStream$$anonfun$callForeachRDD$1.apply(PythonDStream.scala:179)
	at org.apache.spark.streaming.api.python.PythonDStream$$anonfun$callForeachRDD$1.apply(PythonDStream.scala:179)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1$$anonfun$apply$mcV$sp$1.apply$mcV$sp(ForEachDStream.scala:51)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1$$anonfun$apply$mcV$sp$1.apply(ForEachDStream.scala:51)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1$$anonfun$apply$mcV$sp$1.apply(ForEachDStream.scala:51)
	at org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:416)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1.apply$mcV$sp(ForEachDStream.scala:50)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1.apply(ForEachDStream.scala:50)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1.apply(ForEachDStream.scala:50)
	at scala.util.Try$.apply(Try.scala:192)
	at org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)
	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler$$anonfun$run$1.apply$mcV$sp(JobScheduler.scala:257)
	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler$$anonfun$run$1.apply(JobScheduler.scala:257)
	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler$$anonfun$run$1.apply(JobScheduler.scala:257)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)
	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
2018-04-26 01:59:58 ERROR JobScheduler:91 - Error running job streaming job 1524722396000 ms.0
py4j.Py4JException: Cannot obtain a new communication channel
	at py4j.CallbackClient.sendCommand(CallbackClient.java:340)
	at py4j.CallbackClient.sendCommand(CallbackClient.java:316)
	at py4j.reflection.PythonProxyHandler.invoke(PythonProxyHandler.java:106)
	at com.sun.proxy.$Proxy16.call(Unknown Source)
	at org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:92)
	at org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)
	at org.apache.spark.streaming.api.python.PythonDStream$$anonfun$callForeachRDD$1.apply(PythonDStream.scala:179)
	at org.apache.spark.streaming.api.python.PythonDStream$$anonfun$callForeachRDD$1.apply(PythonDStream.scala:179)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1$$anonfun$apply$mcV$sp$1.apply$mcV$sp(ForEachDStream.scala:51)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1$$anonfun$apply$mcV$sp$1.apply(ForEachDStream.scala:51)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1$$anonfun$apply$mcV$sp$1.apply(ForEachDStream.scala:51)
	at org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:416)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1.apply$mcV$sp(ForEachDStream.scala:50)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1.apply(ForEachDStream.scala:50)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1.apply(ForEachDStream.scala:50)
	at scala.util.Try$.apply(Try.scala:192)
	at org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)
	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler$$anonfun$run$1.apply$mcV$sp(JobScheduler.scala:257)
	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler$$anonfun$run$1.apply(JobScheduler.scala:257)
	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler$$anonfun$run$1.apply(JobScheduler.scala:257)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)
	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
2018-04-26 01:59:58 ERROR JobScheduler:91 - Error running job streaming job 1524722396000 ms.1
py4j.Py4JException: Cannot obtain a new communication channel
	at py4j.CallbackClient.sendCommand(CallbackClient.java:340)
	at py4j.CallbackClient.sendCommand(CallbackClient.java:316)
	at py4j.reflection.PythonProxyHandler.invoke(PythonProxyHandler.java:106)
	at com.sun.proxy.$Proxy16.call(Unknown Source)
	at org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:92)
	at org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)
	at org.apache.spark.streaming.api.python.PythonDStream$$anonfun$callForeachRDD$1.apply(PythonDStream.scala:179)
	at org.apache.spark.streaming.api.python.PythonDStream$$anonfun$callForeachRDD$1.apply(PythonDStream.scala:179)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1$$anonfun$apply$mcV$sp$1.apply$mcV$sp(ForEachDStream.scala:51)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1$$anonfun$apply$mcV$sp$1.apply(ForEachDStream.scala:51)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1$$anonfun$apply$mcV$sp$1.apply(ForEachDStream.scala:51)
	at org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:416)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1.apply$mcV$sp(ForEachDStream.scala:50)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1.apply(ForEachDStream.scala:50)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1.apply(ForEachDStream.scala:50)
	at scala.util.Try$.apply(Try.scala:192)
	at org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)
	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler$$anonfun$run$1.apply$mcV$sp(JobScheduler.scala:257)
	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler$$anonfun$run$1.apply(JobScheduler.scala:257)
	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler$$anonfun$run$1.apply(JobScheduler.scala:257)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)
	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
2018-04-26 01:59:58 INFO  PythonRunner:54 - Times: total = 50, boot = 41, init = 9, finish = 0
2018-04-26 01:59:58 ERROR JobScheduler:91 - Error running job streaming job 1524722397000 ms.0
py4j.Py4JException: Cannot obtain a new communication channel
	at py4j.CallbackClient.sendCommand(CallbackClient.java:340)
	at py4j.CallbackClient.sendCommand(CallbackClient.java:316)
	at py4j.reflection.PythonProxyHandler.invoke(PythonProxyHandler.java:106)
	at com.sun.proxy.$Proxy16.call(Unknown Source)
	at org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:92)
	at org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)
	at org.apache.spark.streaming.api.python.PythonDStream$$anonfun$callForeachRDD$1.apply(PythonDStream.scala:179)
	at org.apache.spark.streaming.api.python.PythonDStream$$anonfun$callForeachRDD$1.apply(PythonDStream.scala:179)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1$$anonfun$apply$mcV$sp$1.apply$mcV$sp(ForEachDStream.scala:51)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1$$anonfun$apply$mcV$sp$1.apply(ForEachDStream.scala:51)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1$$anonfun$apply$mcV$sp$1.apply(ForEachDStream.scala:51)
	at org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:416)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1.apply$mcV$sp(ForEachDStream.scala:50)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1.apply(ForEachDStream.scala:50)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1.apply(ForEachDStream.scala:50)
	at scala.util.Try$.apply(Try.scala:192)
	at org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)
	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler$$anonfun$run$1.apply$mcV$sp(JobScheduler.scala:257)
	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler$$anonfun$run$1.apply(JobScheduler.scala:257)
	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler$$anonfun$run$1.apply(JobScheduler.scala:257)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)
	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
2018-04-26 01:59:58 ERROR JobScheduler:91 - Error running job streaming job 1524722397000 ms.1
py4j.Py4JException: Cannot obtain a new communication channel
	at py4j.CallbackClient.sendCommand(CallbackClient.java:340)
	at py4j.CallbackClient.sendCommand(CallbackClient.java:316)
	at py4j.reflection.PythonProxyHandler.invoke(PythonProxyHandler.java:106)
	at com.sun.proxy.$Proxy16.call(Unknown Source)
	at org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:92)
	at org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)
	at org.apache.spark.streaming.api.python.PythonDStream$$anonfun$callForeachRDD$1.apply(PythonDStream.scala:179)
	at org.apache.spark.streaming.api.python.PythonDStream$$anonfun$callForeachRDD$1.apply(PythonDStream.scala:179)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1$$anonfun$apply$mcV$sp$1.apply$mcV$sp(ForEachDStream.scala:51)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1$$anonfun$apply$mcV$sp$1.apply(ForEachDStream.scala:51)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1$$anonfun$apply$mcV$sp$1.apply(ForEachDStream.scala:51)
	at org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:416)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1.apply$mcV$sp(ForEachDStream.scala:50)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1.apply(ForEachDStream.scala:50)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1.apply(ForEachDStream.scala:50)
	at scala.util.Try$.apply(Try.scala:192)
	at org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)
	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler$$anonfun$run$1.apply$mcV$sp(JobScheduler.scala:257)
	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler$$anonfun$run$1.apply(JobScheduler.scala:257)
	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler$$anonfun$run$1.apply(JobScheduler.scala:257)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)
	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
2018-04-26 01:59:58 ERROR JobScheduler:91 - Error running job streaming job 1524722398000 ms.0
py4j.Py4JException: Cannot obtain a new communication channel
	at py4j.CallbackClient.sendCommand(CallbackClient.java:340)
	at py4j.CallbackClient.sendCommand(CallbackClient.java:316)
	at py4j.reflection.PythonProxyHandler.invoke(PythonProxyHandler.java:106)
	at com.sun.proxy.$Proxy16.call(Unknown Source)
	at org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:92)
	at org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)
	at org.apache.spark.streaming.api.python.PythonDStream$$anonfun$callForeachRDD$1.apply(PythonDStream.scala:179)
	at org.apache.spark.streaming.api.python.PythonDStream$$anonfun$callForeachRDD$1.apply(PythonDStream.scala:179)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1$$anonfun$apply$mcV$sp$1.apply$mcV$sp(ForEachDStream.scala:51)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1$$anonfun$apply$mcV$sp$1.apply(ForEachDStream.scala:51)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1$$anonfun$apply$mcV$sp$1.apply(ForEachDStream.scala:51)
	at org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:416)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1.apply$mcV$sp(ForEachDStream.scala:50)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1.apply(ForEachDStream.scala:50)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1.apply(ForEachDStream.scala:50)
	at scala.util.Try$.apply(Try.scala:192)
	at org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)
	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler$$anonfun$run$1.apply$mcV$sp(JobScheduler.scala:257)
	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler$$anonfun$run$1.apply(JobScheduler.scala:257)
	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler$$anonfun$run$1.apply(JobScheduler.scala:257)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)
	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
2018-04-26 01:59:58 INFO  PythonRunner:54 - Times: total = 49, boot = 30, init = 19, finish = 0
2018-04-26 01:59:58 ERROR JobScheduler:91 - Error running job streaming job 1524722398000 ms.1
py4j.Py4JException: Cannot obtain a new communication channel
	at py4j.CallbackClient.sendCommand(CallbackClient.java:340)
	at py4j.CallbackClient.sendCommand(CallbackClient.java:316)
	at py4j.reflection.PythonProxyHandler.invoke(PythonProxyHandler.java:106)
	at com.sun.proxy.$Proxy16.call(Unknown Source)
	at org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:92)
	at org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)
	at org.apache.spark.streaming.api.python.PythonDStream$$anonfun$callForeachRDD$1.apply(PythonDStream.scala:179)
	at org.apache.spark.streaming.api.python.PythonDStream$$anonfun$callForeachRDD$1.apply(PythonDStream.scala:179)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1$$anonfun$apply$mcV$sp$1.apply$mcV$sp(ForEachDStream.scala:51)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1$$anonfun$apply$mcV$sp$1.apply(ForEachDStream.scala:51)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1$$anonfun$apply$mcV$sp$1.apply(ForEachDStream.scala:51)
	at org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:416)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1.apply$mcV$sp(ForEachDStream.scala:50)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1.apply(ForEachDStream.scala:50)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1.apply(ForEachDStream.scala:50)
	at scala.util.Try$.apply(Try.scala:192)
	at org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)
	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler$$anonfun$run$1.apply$mcV$sp(JobScheduler.scala:257)
	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler$$anonfun$run$1.apply(JobScheduler.scala:257)
	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler$$anonfun$run$1.apply(JobScheduler.scala:257)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)
	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
2018-04-26 01:59:58 INFO  Executor:54 - Finished task 4.0 in stage 20.0 (TID 89). 1267 bytes result sent to driver
2018-04-26 01:59:58 ERROR PythonDStream$$anon$1:91 - Cannot connect to Python process. It's probably dead. Stopping StreamingContext.
py4j.Py4JException: Error while obtaining a new communication channel
	at py4j.CallbackClient.getConnectionLock(CallbackClient.java:218)
	at py4j.CallbackClient.sendCommand(CallbackClient.java:337)
	at py4j.CallbackClient.sendCommand(CallbackClient.java:316)
	at py4j.reflection.PythonProxyHandler.invoke(PythonProxyHandler.java:106)
	at com.sun.proxy.$Proxy16.call(Unknown Source)
	at org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:92)
	at org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)
	at org.apache.spark.streaming.api.python.PythonDStream$$anonfun$callForeachRDD$1.apply(PythonDStream.scala:179)
	at org.apache.spark.streaming.api.python.PythonDStream$$anonfun$callForeachRDD$1.apply(PythonDStream.scala:179)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1$$anonfun$apply$mcV$sp$1.apply$mcV$sp(ForEachDStream.scala:51)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1$$anonfun$apply$mcV$sp$1.apply(ForEachDStream.scala:51)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1$$anonfun$apply$mcV$sp$1.apply(ForEachDStream.scala:51)
	at org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:416)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1.apply$mcV$sp(ForEachDStream.scala:50)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1.apply(ForEachDStream.scala:50)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1.apply(ForEachDStream.scala:50)
	at scala.util.Try$.apply(Try.scala:192)
	at org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)
	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler$$anonfun$run$1.apply$mcV$sp(JobScheduler.scala:257)
	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler$$anonfun$run$1.apply(JobScheduler.scala:257)
	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler$$anonfun$run$1.apply(JobScheduler.scala:257)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)
	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.net.ConnectException: Connection refused (Connection refused)
	at java.net.PlainSocketImpl.socketConnect(Native Method)
	at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:350)
	at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:206)
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:188)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:589)
	at java.net.Socket.connect(Socket.java:538)
	at java.net.Socket.<init>(Socket.java:434)
	at java.net.Socket.<init>(Socket.java:244)
	at javax.net.DefaultSocketFactory.createSocket(SocketFactory.java:277)
	at py4j.CallbackConnection.start(CallbackConnection.java:206)
	at py4j.CallbackClient.getConnection(CallbackClient.java:199)
	at py4j.CallbackClient.getConnectionLock(CallbackClient.java:211)
	... 25 more
2018-04-26 01:59:58 ERROR PythonDStream$$anon$1:91 - Cannot connect to Python process. It's probably dead. Stopping StreamingContext.
py4j.Py4JException: Error while obtaining a new communication channel
	at py4j.CallbackClient.getConnectionLock(CallbackClient.java:218)
	at py4j.CallbackClient.sendCommand(CallbackClient.java:337)
	at py4j.CallbackClient.sendCommand(CallbackClient.java:316)
	at py4j.reflection.PythonProxyHandler.invoke(PythonProxyHandler.java:106)
	at com.sun.proxy.$Proxy16.call(Unknown Source)
	at org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:92)
	at org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)
	at org.apache.spark.streaming.api.python.PythonDStream$$anonfun$callForeachRDD$1.apply(PythonDStream.scala:179)
	at org.apache.spark.streaming.api.python.PythonDStream$$anonfun$callForeachRDD$1.apply(PythonDStream.scala:179)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1$$anonfun$apply$mcV$sp$1.apply$mcV$sp(ForEachDStream.scala:51)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1$$anonfun$apply$mcV$sp$1.apply(ForEachDStream.scala:51)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1$$anonfun$apply$mcV$sp$1.apply(ForEachDStream.scala:51)
	at org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:416)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1.apply$mcV$sp(ForEachDStream.scala:50)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1.apply(ForEachDStream.scala:50)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1.apply(ForEachDStream.scala:50)
	at scala.util.Try$.apply(Try.scala:192)
	at org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)
	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler$$anonfun$run$1.apply$mcV$sp(JobScheduler.scala:257)
	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler$$anonfun$run$1.apply(JobScheduler.scala:257)
	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler$$anonfun$run$1.apply(JobScheduler.scala:257)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)
	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.net.ConnectException: Connection refused (Connection refused)
	at java.net.PlainSocketImpl.socketConnect(Native Method)
	at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:350)
	at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:206)
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:188)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:589)
	at java.net.Socket.connect(Socket.java:538)
	at java.net.Socket.<init>(Socket.java:434)
	at java.net.Socket.<init>(Socket.java:244)
	at javax.net.DefaultSocketFactory.createSocket(SocketFactory.java:277)
	at py4j.CallbackConnection.start(CallbackConnection.java:206)
	at py4j.CallbackClient.getConnection(CallbackClient.java:199)
	at py4j.CallbackClient.getConnectionLock(CallbackClient.java:211)
	... 25 more
2018-04-26 01:59:58 ERROR PythonDStream$$anon$1:91 - Cannot connect to Python process. It's probably dead. Stopping StreamingContext.
py4j.Py4JException: Cannot obtain a new communication channel
	at py4j.CallbackClient.sendCommand(CallbackClient.java:340)
	at py4j.CallbackClient.sendCommand(CallbackClient.java:316)
	at py4j.reflection.PythonProxyHandler.invoke(PythonProxyHandler.java:106)
	at com.sun.proxy.$Proxy16.call(Unknown Source)
	at org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:92)
	at org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)
	at org.apache.spark.streaming.api.python.PythonDStream$$anonfun$callForeachRDD$1.apply(PythonDStream.scala:179)
	at org.apache.spark.streaming.api.python.PythonDStream$$anonfun$callForeachRDD$1.apply(PythonDStream.scala:179)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1$$anonfun$apply$mcV$sp$1.apply$mcV$sp(ForEachDStream.scala:51)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1$$anonfun$apply$mcV$sp$1.apply(ForEachDStream.scala:51)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1$$anonfun$apply$mcV$sp$1.apply(ForEachDStream.scala:51)
	at org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:416)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1.apply$mcV$sp(ForEachDStream.scala:50)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1.apply(ForEachDStream.scala:50)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1.apply(ForEachDStream.scala:50)
	at scala.util.Try$.apply(Try.scala:192)
	at org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)
	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler$$anonfun$run$1.apply$mcV$sp(JobScheduler.scala:257)
	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler$$anonfun$run$1.apply(JobScheduler.scala:257)
	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler$$anonfun$run$1.apply(JobScheduler.scala:257)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)
	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
2018-04-26 01:59:58 ERROR PythonDStream$$anon$1:91 - Cannot connect to Python process. It's probably dead. Stopping StreamingContext.
py4j.Py4JException: Cannot obtain a new communication channel
	at py4j.CallbackClient.sendCommand(CallbackClient.java:340)
	at py4j.CallbackClient.sendCommand(CallbackClient.java:316)
	at py4j.reflection.PythonProxyHandler.invoke(PythonProxyHandler.java:106)
	at com.sun.proxy.$Proxy16.call(Unknown Source)
	at org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:92)
	at org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)
	at org.apache.spark.streaming.api.python.PythonDStream$$anonfun$callForeachRDD$1.apply(PythonDStream.scala:179)
	at org.apache.spark.streaming.api.python.PythonDStream$$anonfun$callForeachRDD$1.apply(PythonDStream.scala:179)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1$$anonfun$apply$mcV$sp$1.apply$mcV$sp(ForEachDStream.scala:51)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1$$anonfun$apply$mcV$sp$1.apply(ForEachDStream.scala:51)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1$$anonfun$apply$mcV$sp$1.apply(ForEachDStream.scala:51)
	at org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:416)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1.apply$mcV$sp(ForEachDStream.scala:50)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1.apply(ForEachDStream.scala:50)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1.apply(ForEachDStream.scala:50)
	at scala.util.Try$.apply(Try.scala:192)
	at org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)
	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler$$anonfun$run$1.apply$mcV$sp(JobScheduler.scala:257)
	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler$$anonfun$run$1.apply(JobScheduler.scala:257)
	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler$$anonfun$run$1.apply(JobScheduler.scala:257)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)
	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
2018-04-26 01:59:58 ERROR PythonDStream$$anon$1:91 - Cannot connect to Python process. It's probably dead. Stopping StreamingContext.
py4j.Py4JException: Cannot obtain a new communication channel
	at py4j.CallbackClient.sendCommand(CallbackClient.java:340)
	at py4j.CallbackClient.sendCommand(CallbackClient.java:316)
	at py4j.reflection.PythonProxyHandler.invoke(PythonProxyHandler.java:106)
	at com.sun.proxy.$Proxy16.call(Unknown Source)
	at org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:92)
	at org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)
	at org.apache.spark.streaming.api.python.PythonDStream$$anonfun$callForeachRDD$1.apply(PythonDStream.scala:179)
	at org.apache.spark.streaming.api.python.PythonDStream$$anonfun$callForeachRDD$1.apply(PythonDStream.scala:179)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1$$anonfun$apply$mcV$sp$1.apply$mcV$sp(ForEachDStream.scala:51)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1$$anonfun$apply$mcV$sp$1.apply(ForEachDStream.scala:51)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1$$anonfun$apply$mcV$sp$1.apply(ForEachDStream.scala:51)
	at org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:416)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1.apply$mcV$sp(ForEachDStream.scala:50)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1.apply(ForEachDStream.scala:50)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1.apply(ForEachDStream.scala:50)
	at scala.util.Try$.apply(Try.scala:192)
	at org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)
	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler$$anonfun$run$1.apply$mcV$sp(JobScheduler.scala:257)
	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler$$anonfun$run$1.apply(JobScheduler.scala:257)
	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler$$anonfun$run$1.apply(JobScheduler.scala:257)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)
	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
2018-04-26 01:59:58 ERROR PythonDStream$$anon$1:91 - Cannot connect to Python process. It's probably dead. Stopping StreamingContext.
py4j.Py4JException: Cannot obtain a new communication channel
	at py4j.CallbackClient.sendCommand(CallbackClient.java:340)
	at py4j.CallbackClient.sendCommand(CallbackClient.java:316)
	at py4j.reflection.PythonProxyHandler.invoke(PythonProxyHandler.java:106)
	at com.sun.proxy.$Proxy16.call(Unknown Source)
	at org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:92)
	at org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)
	at org.apache.spark.streaming.api.python.PythonDStream$$anonfun$callForeachRDD$1.apply(PythonDStream.scala:179)
	at org.apache.spark.streaming.api.python.PythonDStream$$anonfun$callForeachRDD$1.apply(PythonDStream.scala:179)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1$$anonfun$apply$mcV$sp$1.apply$mcV$sp(ForEachDStream.scala:51)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1$$anonfun$apply$mcV$sp$1.apply(ForEachDStream.scala:51)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1$$anonfun$apply$mcV$sp$1.apply(ForEachDStream.scala:51)
	at org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:416)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1.apply$mcV$sp(ForEachDStream.scala:50)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1.apply(ForEachDStream.scala:50)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1.apply(ForEachDStream.scala:50)
	at scala.util.Try$.apply(Try.scala:192)
	at org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)
	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler$$anonfun$run$1.apply$mcV$sp(JobScheduler.scala:257)
	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler$$anonfun$run$1.apply(JobScheduler.scala:257)
	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler$$anonfun$run$1.apply(JobScheduler.scala:257)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)
	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
2018-04-26 01:59:58 ERROR PythonDStream$$anon$1:91 - Cannot connect to Python process. It's probably dead. Stopping StreamingContext.
py4j.Py4JException: Cannot obtain a new communication channel
	at py4j.CallbackClient.sendCommand(CallbackClient.java:340)
	at py4j.CallbackClient.sendCommand(CallbackClient.java:316)
	at py4j.reflection.PythonProxyHandler.invoke(PythonProxyHandler.java:106)
	at com.sun.proxy.$Proxy16.call(Unknown Source)
	at org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:92)
	at org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)
	at org.apache.spark.streaming.api.python.PythonDStream$$anonfun$callForeachRDD$1.apply(PythonDStream.scala:179)
	at org.apache.spark.streaming.api.python.PythonDStream$$anonfun$callForeachRDD$1.apply(PythonDStream.scala:179)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1$$anonfun$apply$mcV$sp$1.apply$mcV$sp(ForEachDStream.scala:51)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1$$anonfun$apply$mcV$sp$1.apply(ForEachDStream.scala:51)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1$$anonfun$apply$mcV$sp$1.apply(ForEachDStream.scala:51)
	at org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:416)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1.apply$mcV$sp(ForEachDStream.scala:50)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1.apply(ForEachDStream.scala:50)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1.apply(ForEachDStream.scala:50)
	at scala.util.Try$.apply(Try.scala:192)
	at org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)
	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler$$anonfun$run$1.apply$mcV$sp(JobScheduler.scala:257)
	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler$$anonfun$run$1.apply(JobScheduler.scala:257)
	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler$$anonfun$run$1.apply(JobScheduler.scala:257)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)
	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
2018-04-26 01:59:58 ERROR PythonDStream$$anon$1:91 - Cannot connect to Python process. It's probably dead. Stopping StreamingContext.
py4j.Py4JException: Cannot obtain a new communication channel
	at py4j.CallbackClient.sendCommand(CallbackClient.java:340)
	at py4j.CallbackClient.sendCommand(CallbackClient.java:316)
	at py4j.reflection.PythonProxyHandler.invoke(PythonProxyHandler.java:106)
	at com.sun.proxy.$Proxy16.call(Unknown Source)
	at org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:92)
	at org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)
	at org.apache.spark.streaming.api.python.PythonDStream$$anonfun$callForeachRDD$1.apply(PythonDStream.scala:179)
	at org.apache.spark.streaming.api.python.PythonDStream$$anonfun$callForeachRDD$1.apply(PythonDStream.scala:179)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1$$anonfun$apply$mcV$sp$1.apply$mcV$sp(ForEachDStream.scala:51)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1$$anonfun$apply$mcV$sp$1.apply(ForEachDStream.scala:51)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1$$anonfun$apply$mcV$sp$1.apply(ForEachDStream.scala:51)
	at org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:416)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1.apply$mcV$sp(ForEachDStream.scala:50)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1.apply(ForEachDStream.scala:50)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1.apply(ForEachDStream.scala:50)
	at scala.util.Try$.apply(Try.scala:192)
	at org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)
	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler$$anonfun$run$1.apply$mcV$sp(JobScheduler.scala:257)
	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler$$anonfun$run$1.apply(JobScheduler.scala:257)
	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler$$anonfun$run$1.apply(JobScheduler.scala:257)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)
	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
2018-04-26 01:59:58 ERROR PythonDStream$$anon$1:91 - Cannot connect to Python process. It's probably dead. Stopping StreamingContext.
py4j.Py4JException: Cannot obtain a new communication channel
	at py4j.CallbackClient.sendCommand(CallbackClient.java:340)
	at py4j.CallbackClient.sendCommand(CallbackClient.java:316)
	at py4j.reflection.PythonProxyHandler.invoke(PythonProxyHandler.java:106)
	at com.sun.proxy.$Proxy16.call(Unknown Source)
	at org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:92)
	at org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)
	at org.apache.spark.streaming.api.python.PythonDStream$$anonfun$callForeachRDD$1.apply(PythonDStream.scala:179)
	at org.apache.spark.streaming.api.python.PythonDStream$$anonfun$callForeachRDD$1.apply(PythonDStream.scala:179)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1$$anonfun$apply$mcV$sp$1.apply$mcV$sp(ForEachDStream.scala:51)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1$$anonfun$apply$mcV$sp$1.apply(ForEachDStream.scala:51)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1$$anonfun$apply$mcV$sp$1.apply(ForEachDStream.scala:51)
	at org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:416)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1.apply$mcV$sp(ForEachDStream.scala:50)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1.apply(ForEachDStream.scala:50)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1.apply(ForEachDStream.scala:50)
	at scala.util.Try$.apply(Try.scala:192)
	at org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)
	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler$$anonfun$run$1.apply$mcV$sp(JobScheduler.scala:257)
	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler$$anonfun$run$1.apply(JobScheduler.scala:257)
	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler$$anonfun$run$1.apply(JobScheduler.scala:257)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)
	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
2018-04-26 01:59:58 ERROR PythonDStream$$anon$1:91 - Cannot connect to Python process. It's probably dead. Stopping StreamingContext.
py4j.Py4JException: Cannot obtain a new communication channel
	at py4j.CallbackClient.sendCommand(CallbackClient.java:340)
	at py4j.CallbackClient.sendCommand(CallbackClient.java:316)
	at py4j.reflection.PythonProxyHandler.invoke(PythonProxyHandler.java:106)
	at com.sun.proxy.$Proxy16.call(Unknown Source)
	at org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:92)
	at org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)
	at org.apache.spark.streaming.api.python.PythonDStream$$anonfun$callForeachRDD$1.apply(PythonDStream.scala:179)
	at org.apache.spark.streaming.api.python.PythonDStream$$anonfun$callForeachRDD$1.apply(PythonDStream.scala:179)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1$$anonfun$apply$mcV$sp$1.apply$mcV$sp(ForEachDStream.scala:51)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1$$anonfun$apply$mcV$sp$1.apply(ForEachDStream.scala:51)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1$$anonfun$apply$mcV$sp$1.apply(ForEachDStream.scala:51)
	at org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:416)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1.apply$mcV$sp(ForEachDStream.scala:50)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1.apply(ForEachDStream.scala:50)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1.apply(ForEachDStream.scala:50)
	at scala.util.Try$.apply(Try.scala:192)
	at org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)
	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler$$anonfun$run$1.apply$mcV$sp(JobScheduler.scala:257)
	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler$$anonfun$run$1.apply(JobScheduler.scala:257)
	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler$$anonfun$run$1.apply(JobScheduler.scala:257)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)
	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
2018-04-26 01:59:58 INFO  TaskSetManager:54 - Finished task 4.0 in stage 20.0 (TID 89) in 88 ms on localhost (executor driver) (5/5)
2018-04-26 01:59:58 INFO  TaskSchedulerImpl:54 - Removed TaskSet 20.0, whose tasks have all completed, from pool 
2018-04-26 01:59:58 ERROR DAGScheduler:91 - Failed to update accumulators for task 4
java.net.SocketException: Broken pipe (Write failed)
	at java.net.SocketOutputStream.socketWrite0(Native Method)
	at java.net.SocketOutputStream.socketWrite(SocketOutputStream.java:111)
	at java.net.SocketOutputStream.write(SocketOutputStream.java:155)
	at java.io.BufferedOutputStream.flushBuffer(BufferedOutputStream.java:82)
	at java.io.BufferedOutputStream.flush(BufferedOutputStream.java:140)
	at java.io.DataOutputStream.flush(DataOutputStream.java:123)
	at org.apache.spark.api.python.PythonAccumulatorV2.merge(PythonRDD.scala:615)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$updateAccumulators$1.apply(DAGScheduler.scala:1132)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$updateAccumulators$1.apply(DAGScheduler.scala:1124)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)
	at org.apache.spark.scheduler.DAGScheduler.updateAccumulators(DAGScheduler.scala:1124)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskCompletion(DAGScheduler.scala:1207)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1817)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1769)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1758)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)
2018-04-26 01:59:58 INFO  DAGScheduler:54 - ResultStage 20 (call at /home/rjha/spark-2.3.0-bin-hadoop2.7/python/lib/py4j-0.10.6-src.zip/py4j/java_gateway.py:2257) finished in 0.638 s
2018-04-26 01:59:58 INFO  DAGScheduler:54 - Job 22 finished: call at /home/rjha/spark-2.3.0-bin-hadoop2.7/python/lib/py4j-0.10.6-src.zip/py4j/java_gateway.py:2257, took 0.643291 s
2018-04-26 01:59:58 ERROR PythonDStream$$anon$1:91 - Cannot connect to Python process. It's probably dead. Stopping StreamingContext.
py4j.Py4JException: Cannot obtain a new communication channel
	at py4j.CallbackClient.sendCommand(CallbackClient.java:340)
	at py4j.CallbackClient.sendCommand(CallbackClient.java:316)
	at py4j.reflection.PythonProxyHandler.invoke(PythonProxyHandler.java:106)
	at com.sun.proxy.$Proxy16.call(Unknown Source)
	at org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:92)
	at org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)
	at org.apache.spark.streaming.api.python.PythonDStream$$anonfun$callForeachRDD$1.apply(PythonDStream.scala:179)
	at org.apache.spark.streaming.api.python.PythonDStream$$anonfun$callForeachRDD$1.apply(PythonDStream.scala:179)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1$$anonfun$apply$mcV$sp$1.apply$mcV$sp(ForEachDStream.scala:51)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1$$anonfun$apply$mcV$sp$1.apply(ForEachDStream.scala:51)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1$$anonfun$apply$mcV$sp$1.apply(ForEachDStream.scala:51)
	at org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:416)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1.apply$mcV$sp(ForEachDStream.scala:50)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1.apply(ForEachDStream.scala:50)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1.apply(ForEachDStream.scala:50)
	at scala.util.Try$.apply(Try.scala:192)
	at org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)
	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler$$anonfun$run$1.apply$mcV$sp(JobScheduler.scala:257)
	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler$$anonfun$run$1.apply(JobScheduler.scala:257)
	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler$$anonfun$run$1.apply(JobScheduler.scala:257)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)
	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
2018-04-26 01:59:59 ERROR JobScheduler:91 - Error generating jobs for time 1524722399000 ms
py4j.Py4JException: Cannot obtain a new communication channel
	at py4j.CallbackClient.sendCommand(CallbackClient.java:340)
	at py4j.CallbackClient.sendCommand(CallbackClient.java:316)
	at py4j.reflection.PythonProxyHandler.invoke(PythonProxyHandler.java:106)
	at com.sun.proxy.$Proxy16.call(Unknown Source)
	at org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:92)
	at org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)
	at org.apache.spark.streaming.api.python.PythonTransformedDStream.compute(PythonDStream.scala:246)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1$$anonfun$1$$anonfun$apply$7.apply(DStream.scala:342)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1$$anonfun$1$$anonfun$apply$7.apply(DStream.scala:342)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1$$anonfun$1.apply(DStream.scala:341)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1$$anonfun$1.apply(DStream.scala:341)
	at org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:416)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1.apply(DStream.scala:336)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1.apply(DStream.scala:334)
	at scala.Option.orElse(Option.scala:289)
	at org.apache.spark.streaming.dstream.DStream.getOrCompute(DStream.scala:331)
	at org.apache.spark.streaming.dstream.ForEachDStream.generateJob(ForEachDStream.scala:48)
	at org.apache.spark.streaming.DStreamGraph$$anonfun$1.apply(DStreamGraph.scala:122)
	at org.apache.spark.streaming.DStreamGraph$$anonfun$1.apply(DStreamGraph.scala:121)
	at scala.collection.TraversableLike$$anonfun$flatMap$1.apply(TraversableLike.scala:241)
	at scala.collection.TraversableLike$$anonfun$flatMap$1.apply(TraversableLike.scala:241)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)
	at scala.collection.TraversableLike$class.flatMap(TraversableLike.scala:241)
	at scala.collection.AbstractTraversable.flatMap(Traversable.scala:104)
	at org.apache.spark.streaming.DStreamGraph.generateJobs(DStreamGraph.scala:121)
	at org.apache.spark.streaming.scheduler.JobGenerator$$anonfun$3.apply(JobGenerator.scala:249)
	at org.apache.spark.streaming.scheduler.JobGenerator$$anonfun$3.apply(JobGenerator.scala:247)
	at scala.util.Try$.apply(Try.scala:192)
	at org.apache.spark.streaming.scheduler.JobGenerator.generateJobs(JobGenerator.scala:247)
	at org.apache.spark.streaming.scheduler.JobGenerator.org$apache$spark$streaming$scheduler$JobGenerator$$processEvent(JobGenerator.scala:183)
	at org.apache.spark.streaming.scheduler.JobGenerator$$anon$1.onReceive(JobGenerator.scala:89)
	at org.apache.spark.streaming.scheduler.JobGenerator$$anon$1.onReceive(JobGenerator.scala:88)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)
2018-04-26 01:59:59 ERROR PythonDStream$$anon$1:91 - Cannot connect to Python process. It's probably dead. Stopping StreamingContext.
py4j.Py4JException: Cannot obtain a new communication channel
	at py4j.CallbackClient.sendCommand(CallbackClient.java:340)
	at py4j.CallbackClient.sendCommand(CallbackClient.java:316)
	at py4j.reflection.PythonProxyHandler.invoke(PythonProxyHandler.java:106)
	at com.sun.proxy.$Proxy16.call(Unknown Source)
	at org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:92)
	at org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)
	at org.apache.spark.streaming.api.python.PythonTransformedDStream.compute(PythonDStream.scala:246)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1$$anonfun$1$$anonfun$apply$7.apply(DStream.scala:342)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1$$anonfun$1$$anonfun$apply$7.apply(DStream.scala:342)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1$$anonfun$1.apply(DStream.scala:341)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1$$anonfun$1.apply(DStream.scala:341)
	at org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:416)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1.apply(DStream.scala:336)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1.apply(DStream.scala:334)
	at scala.Option.orElse(Option.scala:289)
	at org.apache.spark.streaming.dstream.DStream.getOrCompute(DStream.scala:331)
	at org.apache.spark.streaming.dstream.ForEachDStream.generateJob(ForEachDStream.scala:48)
	at org.apache.spark.streaming.DStreamGraph$$anonfun$1.apply(DStreamGraph.scala:122)
	at org.apache.spark.streaming.DStreamGraph$$anonfun$1.apply(DStreamGraph.scala:121)
	at scala.collection.TraversableLike$$anonfun$flatMap$1.apply(TraversableLike.scala:241)
	at scala.collection.TraversableLike$$anonfun$flatMap$1.apply(TraversableLike.scala:241)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)
	at scala.collection.TraversableLike$class.flatMap(TraversableLike.scala:241)
	at scala.collection.AbstractTraversable.flatMap(Traversable.scala:104)
	at org.apache.spark.streaming.DStreamGraph.generateJobs(DStreamGraph.scala:121)
	at org.apache.spark.streaming.scheduler.JobGenerator$$anonfun$3.apply(JobGenerator.scala:249)
	at org.apache.spark.streaming.scheduler.JobGenerator$$anonfun$3.apply(JobGenerator.scala:247)
	at scala.util.Try$.apply(Try.scala:192)
	at org.apache.spark.streaming.scheduler.JobGenerator.generateJobs(JobGenerator.scala:247)
	at org.apache.spark.streaming.scheduler.JobGenerator.org$apache$spark$streaming$scheduler$JobGenerator$$processEvent(JobGenerator.scala:183)
	at org.apache.spark.streaming.scheduler.JobGenerator$$anon$1.onReceive(JobGenerator.scala:89)
	at org.apache.spark.streaming.scheduler.JobGenerator$$anon$1.onReceive(JobGenerator.scala:88)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)
2018-04-26 01:59:59 ERROR PythonDStream$$anon$1:91 - Cannot connect to Python process. It's probably dead. Stopping StreamingContext.
py4j.Py4JException: Cannot obtain a new communication channel
	at py4j.CallbackClient.sendCommand(CallbackClient.java:340)
	at py4j.CallbackClient.sendCommand(CallbackClient.java:316)
	at py4j.reflection.PythonProxyHandler.invoke(PythonProxyHandler.java:106)
	at com.sun.proxy.$Proxy16.call(Unknown Source)
	at org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:92)
	at org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)
	at org.apache.spark.streaming.api.python.PythonTransformedDStream.compute(PythonDStream.scala:246)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1$$anonfun$1$$anonfun$apply$7.apply(DStream.scala:342)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1$$anonfun$1$$anonfun$apply$7.apply(DStream.scala:342)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1$$anonfun$1.apply(DStream.scala:341)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1$$anonfun$1.apply(DStream.scala:341)
	at org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:416)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1.apply(DStream.scala:336)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1.apply(DStream.scala:334)
	at scala.Option.orElse(Option.scala:289)
	at org.apache.spark.streaming.dstream.DStream.getOrCompute(DStream.scala:331)
	at org.apache.spark.streaming.dstream.ForEachDStream.generateJob(ForEachDStream.scala:48)
	at org.apache.spark.streaming.DStreamGraph$$anonfun$1.apply(DStreamGraph.scala:122)
	at org.apache.spark.streaming.DStreamGraph$$anonfun$1.apply(DStreamGraph.scala:121)
	at scala.collection.TraversableLike$$anonfun$flatMap$1.apply(TraversableLike.scala:241)
	at scala.collection.TraversableLike$$anonfun$flatMap$1.apply(TraversableLike.scala:241)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)
	at scala.collection.TraversableLike$class.flatMap(TraversableLike.scala:241)
	at scala.collection.AbstractTraversable.flatMap(Traversable.scala:104)
	at org.apache.spark.streaming.DStreamGraph.generateJobs(DStreamGraph.scala:121)
	at org.apache.spark.streaming.scheduler.JobGenerator$$anonfun$3.apply(JobGenerator.scala:249)
	at org.apache.spark.streaming.scheduler.JobGenerator$$anonfun$3.apply(JobGenerator.scala:247)
	at scala.util.Try$.apply(Try.scala:192)
	at org.apache.spark.streaming.scheduler.JobGenerator.generateJobs(JobGenerator.scala:247)
	at org.apache.spark.streaming.scheduler.JobGenerator.org$apache$spark$streaming$scheduler$JobGenerator$$processEvent(JobGenerator.scala:183)
	at org.apache.spark.streaming.scheduler.JobGenerator$$anon$1.onReceive(JobGenerator.scala:89)
	at org.apache.spark.streaming.scheduler.JobGenerator$$anon$1.onReceive(JobGenerator.scala:88)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)
2018-04-26 01:59:59 INFO  RecurringTimer:54 - Stopped timer for BlockGenerator after time 1524722399200
2018-04-26 01:59:59 INFO  BlockGenerator:54 - Waiting for block pushing thread to terminate
2018-04-26 01:59:59 INFO  BlockGenerator:54 - Pushing out the last 0 blocks
2018-04-26 01:59:59 INFO  BlockGenerator:54 - Stopped block pushing thread
2018-04-26 01:59:59 INFO  BlockGenerator:54 - Stopped BlockGenerator
2018-04-26 01:59:59 INFO  ReceiverSupervisorImpl:54 - Stopped receiver without error
2018-04-26 01:59:59 INFO  Executor:54 - Finished task 0.0 in stage 0.0 (TID 0). 794 bytes result sent to driver
2018-04-26 01:59:59 INFO  TaskSetManager:54 - Finished task 0.0 in stage 0.0 (TID 0) in 18326 ms on localhost (executor driver) (1/1)
2018-04-26 01:59:59 INFO  TaskSchedulerImpl:54 - Removed TaskSet 0.0, whose tasks have all completed, from pool 
2018-04-26 01:59:59 INFO  DAGScheduler:54 - ResultStage 0 (start at NativeMethodAccessorImpl.java:0) finished in 18.764 s
2018-04-26 01:59:59 INFO  ReceiverTracker:54 - All of the receivers have deregistered successfully
2018-04-26 01:59:59 INFO  ReceiverTracker:54 - ReceiverTracker stopped
2018-04-26 01:59:59 INFO  JobGenerator:54 - Stopping JobGenerator immediately
2018-04-26 01:59:59 INFO  RecurringTimer:54 - Stopped timer for JobGenerator after time 1524722399000
2018-04-26 01:59:59 INFO  JobGenerator:54 - Stopped JobGenerator
2018-04-26 01:59:59 INFO  JobScheduler:54 - Stopped JobScheduler
2018-04-26 01:59:59 INFO  ContextHandler:910 - Stopped o.s.j.s.ServletContextHandler@59831beb{/streaming,null,UNAVAILABLE,@Spark}
2018-04-26 01:59:59 INFO  ContextHandler:910 - Stopped o.s.j.s.ServletContextHandler@72bcd4e4{/streaming/batch,null,UNAVAILABLE,@Spark}
2018-04-26 01:59:59 INFO  ContextHandler:910 - Stopped o.s.j.s.ServletContextHandler@1edf4cc7{/static/streaming,null,UNAVAILABLE,@Spark}
2018-04-26 01:59:59 INFO  StreamingContext:54 - StreamingContext stopped successfully
2018-04-26 01:59:59 WARN  StreamingContext:66 - StreamingContext has already been stopped
2018-04-26 01:59:59 WARN  StreamingContext:66 - StreamingContext has already been stopped
2018-04-26 01:59:59 WARN  StreamingContext:66 - StreamingContext has already been stopped
2018-04-26 01:59:59 WARN  StreamingContext:66 - StreamingContext has already been stopped
2018-04-26 01:59:59 WARN  StreamingContext:66 - StreamingContext has already been stopped
2018-04-26 01:59:59 WARN  StreamingContext:66 - StreamingContext has already been stopped
2018-04-26 01:59:59 WARN  StreamingContext:66 - StreamingContext has already been stopped
2018-04-26 01:59:59 WARN  StreamingContext:66 - StreamingContext has already been stopped
2018-04-26 01:59:59 WARN  StreamingContext:66 - StreamingContext has already been stopped
2018-04-26 01:59:59 WARN  StreamingContext:66 - StreamingContext has already been stopped
2018-04-26 01:59:59 WARN  StreamingContext:66 - StreamingContext has already been stopped
2018-04-26 01:59:59 WARN  StreamingContext:66 - StreamingContext has already been stopped
2018-04-26 01:59:59 WARN  StreamingContext:66 - StreamingContext has already been stopped
2018-04-26 01:59:59 WARN  StreamingContext:66 - StreamingContext has already been stopped
2018-04-26 01:59:59 INFO  SparkContext:54 - Invoking stop() from shutdown hook
2018-04-26 01:59:59 INFO  AbstractConnector:318 - Stopped Spark@65c4d5c6{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2018-04-26 01:59:59 INFO  SparkUI:54 - Stopped Spark web UI at http://10.0.2.15:4040
2018-04-26 01:59:59 INFO  MapOutputTrackerMasterEndpoint:54 - MapOutputTrackerMasterEndpoint stopped!
2018-04-26 01:59:59 INFO  MemoryStore:54 - MemoryStore cleared
2018-04-26 01:59:59 INFO  BlockManager:54 - BlockManager stopped
2018-04-26 01:59:59 INFO  BlockManagerMaster:54 - BlockManagerMaster stopped
2018-04-26 01:59:59 INFO  OutputCommitCoordinator$OutputCommitCoordinatorEndpoint:54 - OutputCommitCoordinator stopped!
2018-04-26 01:59:59 INFO  SparkContext:54 - Successfully stopped SparkContext
2018-04-26 01:59:59 INFO  ShutdownHookManager:54 - Shutdown hook called
2018-04-26 01:59:59 INFO  ShutdownHookManager:54 - Deleting directory /tmp/spark-d125ef3c-c1e8-4c0b-99d6-f5ed7731f62b
2018-04-26 01:59:59 INFO  ShutdownHookManager:54 - Deleting directory /tmp/spark-c4f521ee-56a9-44dc-ac5a-9bda398e4540
2018-04-26 01:59:59 INFO  ShutdownHookManager:54 - Deleting directory /tmp/spark-d125ef3c-c1e8-4c0b-99d6-f5ed7731f62b/pyspark-0f58794c-d241-4700-acd3-2fd29f9ce403
