2018-04-26 00:34:23 WARN  Utils:66 - Your hostname, rjha-VirtualBox resolves to a loopback address: 127.0.1.1; using 10.0.2.15 instead (on interface enp0s3)
2018-04-26 00:34:23 WARN  Utils:66 - Set SPARK_LOCAL_IP if you need to bind to another address
2018-04-26 00:34:24 WARN  NativeCodeLoader:62 - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2018-04-26 00:34:25 INFO  SparkContext:54 - Running Spark version 2.3.0
2018-04-26 00:34:25 INFO  SparkContext:54 - Submitted application: PythonStreamingNetworkWordCount
2018-04-26 00:34:25 INFO  SecurityManager:54 - Changing view acls to: rjha
2018-04-26 00:34:25 INFO  SecurityManager:54 - Changing modify acls to: rjha
2018-04-26 00:34:25 INFO  SecurityManager:54 - Changing view acls groups to: 
2018-04-26 00:34:25 INFO  SecurityManager:54 - Changing modify acls groups to: 
2018-04-26 00:34:25 INFO  SecurityManager:54 - SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(rjha); groups with view permissions: Set(); users  with modify permissions: Set(rjha); groups with modify permissions: Set()
2018-04-26 00:34:26 INFO  Utils:54 - Successfully started service 'sparkDriver' on port 43243.
2018-04-26 00:34:26 INFO  SparkEnv:54 - Registering MapOutputTracker
2018-04-26 00:34:26 INFO  SparkEnv:54 - Registering BlockManagerMaster
2018-04-26 00:34:26 INFO  BlockManagerMasterEndpoint:54 - Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
2018-04-26 00:34:26 INFO  BlockManagerMasterEndpoint:54 - BlockManagerMasterEndpoint up
2018-04-26 00:34:26 INFO  DiskBlockManager:54 - Created local directory at /tmp/blockmgr-a843fa64-24ce-4f2d-b44c-a5bd16a03ef1
2018-04-26 00:34:26 INFO  MemoryStore:54 - MemoryStore started with capacity 413.9 MB
2018-04-26 00:34:26 INFO  SparkEnv:54 - Registering OutputCommitCoordinator
2018-04-26 00:34:27 INFO  log:192 - Logging initialized @5449ms
2018-04-26 00:34:27 INFO  Server:346 - jetty-9.3.z-SNAPSHOT
2018-04-26 00:34:27 INFO  Server:414 - Started @5576ms
2018-04-26 00:34:27 INFO  AbstractConnector:278 - Started ServerConnector@1a4f0cc6{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2018-04-26 00:34:27 INFO  Utils:54 - Successfully started service 'SparkUI' on port 4040.
2018-04-26 00:34:27 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@385d9011{/jobs,null,AVAILABLE,@Spark}
2018-04-26 00:34:27 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@63758db4{/jobs/json,null,AVAILABLE,@Spark}
2018-04-26 00:34:27 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@49c2b473{/jobs/job,null,AVAILABLE,@Spark}
2018-04-26 00:34:27 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@3f973b82{/jobs/job/json,null,AVAILABLE,@Spark}
2018-04-26 00:34:27 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@704019e9{/stages,null,AVAILABLE,@Spark}
2018-04-26 00:34:27 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@2c7d5abe{/stages/json,null,AVAILABLE,@Spark}
2018-04-26 00:34:27 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@847db6e{/stages/stage,null,AVAILABLE,@Spark}
2018-04-26 00:34:27 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@389b787c{/stages/stage/json,null,AVAILABLE,@Spark}
2018-04-26 00:34:27 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@eb148aa{/stages/pool,null,AVAILABLE,@Spark}
2018-04-26 00:34:27 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@5536d761{/stages/pool/json,null,AVAILABLE,@Spark}
2018-04-26 00:34:27 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@bbd0a4{/storage,null,AVAILABLE,@Spark}
2018-04-26 00:34:27 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@63001057{/storage/json,null,AVAILABLE,@Spark}
2018-04-26 00:34:27 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@67f0e085{/storage/rdd,null,AVAILABLE,@Spark}
2018-04-26 00:34:27 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@49eaec5{/storage/rdd/json,null,AVAILABLE,@Spark}
2018-04-26 00:34:27 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@5a7e6002{/environment,null,AVAILABLE,@Spark}
2018-04-26 00:34:27 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@391f51b2{/environment/json,null,AVAILABLE,@Spark}
2018-04-26 00:34:27 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@598b6adc{/executors,null,AVAILABLE,@Spark}
2018-04-26 00:34:27 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@28920fae{/executors/json,null,AVAILABLE,@Spark}
2018-04-26 00:34:27 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@1c514ce{/executors/threadDump,null,AVAILABLE,@Spark}
2018-04-26 00:34:27 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@42945785{/executors/threadDump/json,null,AVAILABLE,@Spark}
2018-04-26 00:34:27 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@404b0881{/static,null,AVAILABLE,@Spark}
2018-04-26 00:34:27 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@c7b3e58{/,null,AVAILABLE,@Spark}
2018-04-26 00:34:27 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@6831a8b5{/api,null,AVAILABLE,@Spark}
2018-04-26 00:34:27 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@15c5327d{/jobs/job/kill,null,AVAILABLE,@Spark}
2018-04-26 00:34:27 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@3716f898{/stages/stage/kill,null,AVAILABLE,@Spark}
2018-04-26 00:34:27 INFO  SparkUI:54 - Bound SparkUI to 0.0.0.0, and started at http://10.0.2.15:4040
2018-04-26 00:34:28 INFO  SparkContext:54 - Added file file:/home/rjha/stream-benchmarking-scratch/streaming_test.py at file:/home/rjha/stream-benchmarking-scratch/streaming_test.py with timestamp 1524717268157
2018-04-26 00:34:28 INFO  Utils:54 - Copying /home/rjha/stream-benchmarking-scratch/streaming_test.py to /tmp/spark-10173200-7f67-4568-b40d-c272989e46f1/userFiles-72d55d03-2b39-4bc5-b7a5-e2cb50b206be/streaming_test.py
2018-04-26 00:34:28 INFO  Executor:54 - Starting executor ID driver on host localhost
2018-04-26 00:34:28 INFO  Utils:54 - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 46027.
2018-04-26 00:34:28 INFO  NettyBlockTransferService:54 - Server created on 10.0.2.15:46027
2018-04-26 00:34:28 INFO  BlockManager:54 - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
2018-04-26 00:34:28 INFO  BlockManagerMaster:54 - Registering BlockManager BlockManagerId(driver, 10.0.2.15, 46027, None)
2018-04-26 00:34:28 INFO  BlockManagerMasterEndpoint:54 - Registering block manager 10.0.2.15:46027 with 413.9 MB RAM, BlockManagerId(driver, 10.0.2.15, 46027, None)
2018-04-26 00:34:28 INFO  BlockManagerMaster:54 - Registered BlockManager BlockManagerId(driver, 10.0.2.15, 46027, None)
2018-04-26 00:34:28 INFO  BlockManager:54 - Initialized BlockManager: BlockManagerId(driver, 10.0.2.15, 46027, None)
2018-04-26 00:34:29 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@3b7d2c71{/metrics/json,null,AVAILABLE,@Spark}
2018-04-26 00:34:30 INFO  ReceiverTracker:54 - Starting 1 receivers
2018-04-26 00:34:30 INFO  ReceiverTracker:54 - ReceiverTracker started
2018-04-26 00:34:30 INFO  SocketInputDStream:54 - Slide time = 1000 ms
2018-04-26 00:34:30 INFO  SocketInputDStream:54 - Storage level = Serialized 1x Replicated
2018-04-26 00:34:30 INFO  SocketInputDStream:54 - Checkpoint interval = null
2018-04-26 00:34:30 INFO  SocketInputDStream:54 - Remember interval = 1000 ms
2018-04-26 00:34:30 INFO  SocketInputDStream:54 - Initialized and validated org.apache.spark.streaming.dstream.SocketInputDStream@1360df8d
2018-04-26 00:34:30 INFO  PythonTransformedDStream:54 - Slide time = 1000 ms
2018-04-26 00:34:30 INFO  PythonTransformedDStream:54 - Storage level = Serialized 1x Replicated
2018-04-26 00:34:30 INFO  PythonTransformedDStream:54 - Checkpoint interval = null
2018-04-26 00:34:30 INFO  PythonTransformedDStream:54 - Remember interval = 1000 ms
2018-04-26 00:34:30 INFO  PythonTransformedDStream:54 - Initialized and validated org.apache.spark.streaming.api.python.PythonTransformedDStream@6dc5e006
2018-04-26 00:34:30 INFO  ForEachDStream:54 - Slide time = 1000 ms
2018-04-26 00:34:30 INFO  ForEachDStream:54 - Storage level = Serialized 1x Replicated
2018-04-26 00:34:30 INFO  ForEachDStream:54 - Checkpoint interval = null
2018-04-26 00:34:30 INFO  ForEachDStream:54 - Remember interval = 1000 ms
2018-04-26 00:34:30 INFO  ForEachDStream:54 - Initialized and validated org.apache.spark.streaming.dstream.ForEachDStream@4ab72849
2018-04-26 00:34:30 INFO  SocketInputDStream:54 - Slide time = 1000 ms
2018-04-26 00:34:30 INFO  SocketInputDStream:54 - Storage level = Serialized 1x Replicated
2018-04-26 00:34:30 INFO  SocketInputDStream:54 - Checkpoint interval = null
2018-04-26 00:34:30 INFO  SocketInputDStream:54 - Remember interval = 1000 ms
2018-04-26 00:34:30 INFO  SocketInputDStream:54 - Initialized and validated org.apache.spark.streaming.dstream.SocketInputDStream@1360df8d
2018-04-26 00:34:30 INFO  PythonTransformedDStream:54 - Slide time = 1000 ms
2018-04-26 00:34:30 INFO  PythonTransformedDStream:54 - Storage level = Serialized 1x Replicated
2018-04-26 00:34:30 INFO  PythonTransformedDStream:54 - Checkpoint interval = null
2018-04-26 00:34:30 INFO  PythonTransformedDStream:54 - Remember interval = 1000 ms
2018-04-26 00:34:30 INFO  PythonTransformedDStream:54 - Initialized and validated org.apache.spark.streaming.api.python.PythonTransformedDStream@6dc5e006
2018-04-26 00:34:30 INFO  ForEachDStream:54 - Slide time = 1000 ms
2018-04-26 00:34:30 INFO  ForEachDStream:54 - Storage level = Serialized 1x Replicated
2018-04-26 00:34:30 INFO  ForEachDStream:54 - Checkpoint interval = null
2018-04-26 00:34:30 INFO  ForEachDStream:54 - Remember interval = 1000 ms
2018-04-26 00:34:30 INFO  ForEachDStream:54 - Initialized and validated org.apache.spark.streaming.dstream.ForEachDStream@c27c525
2018-04-26 00:34:31 INFO  ReceiverTracker:54 - Receiver 0 started
2018-04-26 00:34:31 INFO  DAGScheduler:54 - Got job 0 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions
2018-04-26 00:34:31 INFO  DAGScheduler:54 - Final stage: ResultStage 0 (start at NativeMethodAccessorImpl.java:0)
2018-04-26 00:34:31 INFO  DAGScheduler:54 - Parents of final stage: List()
2018-04-26 00:34:31 INFO  RecurringTimer:54 - Started timer for JobGenerator at time 1524717271000
2018-04-26 00:34:31 INFO  JobGenerator:54 - Started JobGenerator at 1524717271000 ms
2018-04-26 00:34:31 INFO  JobScheduler:54 - Started JobScheduler
2018-04-26 00:34:31 INFO  DAGScheduler:54 - Missing parents: List()
2018-04-26 00:34:31 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@20e0d880{/streaming,null,AVAILABLE,@Spark}
2018-04-26 00:34:31 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@32c008d{/streaming/json,null,AVAILABLE,@Spark}
2018-04-26 00:34:31 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@4a528609{/streaming/batch,null,AVAILABLE,@Spark}
2018-04-26 00:34:31 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@7777d67e{/streaming/batch/json,null,AVAILABLE,@Spark}
2018-04-26 00:34:31 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@1895b2a5{/static/streaming,null,AVAILABLE,@Spark}
2018-04-26 00:34:31 INFO  StreamingContext:54 - StreamingContext started
2018-04-26 00:34:31 INFO  DAGScheduler:54 - Submitting ResultStage 0 (Receiver 0 ParallelCollectionRDD[0] at makeRDD at ReceiverTracker.scala:613), which has no missing parents
2018-04-26 00:34:31 INFO  JobScheduler:54 - Added jobs for time 1524717271000 ms
2018-04-26 00:34:31 INFO  JobScheduler:54 - Starting job streaming job 1524717271000 ms.0 from job set of time 1524717271000 ms
2018-04-26 00:34:31 INFO  JobScheduler:54 - Finished job streaming job 1524717271000 ms.0 from job set of time 1524717271000 ms
2018-04-26 00:34:31 ERROR JobScheduler:91 - Error running job streaming job 1524717271000 ms.0
org.apache.spark.SparkException: An exception was raised by Python:
Traceback (most recent call last):
  File "/home/rjha/spark-2.3.0-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/streaming/util.py", line 65, in call
    r = self.func(t, *rdds)
  File "/home/rjha/spark-2.3.0-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/streaming/dstream.py", line 159, in <lambda>
    func = lambda t, rdd: old_func(rdd)
  File "/home/rjha/stream-benchmarking-scratch/streaming_test.py", line 36, in <lambda>
    w_2_ts.foreachRDD(lambda rdd: send_rdd_to_db(rdd, out_port)) # storing
  File "/home/rjha/stream-benchmarking-scratch/streaming_test.py", line 18, in send_rdd_to_db
    conn = sock.create_connection(('localhost', out_port))
AttributeError: '_socketobject' object has no attribute 'create_connection'

	at org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)
	at org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)
	at org.apache.spark.streaming.api.python.PythonDStream$$anonfun$callForeachRDD$1.apply(PythonDStream.scala:179)
	at org.apache.spark.streaming.api.python.PythonDStream$$anonfun$callForeachRDD$1.apply(PythonDStream.scala:179)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1$$anonfun$apply$mcV$sp$1.apply$mcV$sp(ForEachDStream.scala:51)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1$$anonfun$apply$mcV$sp$1.apply(ForEachDStream.scala:51)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1$$anonfun$apply$mcV$sp$1.apply(ForEachDStream.scala:51)
	at org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:416)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1.apply$mcV$sp(ForEachDStream.scala:50)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1.apply(ForEachDStream.scala:50)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1.apply(ForEachDStream.scala:50)
	at scala.util.Try$.apply(Try.scala:192)
	at org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)
	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler$$anonfun$run$1.apply$mcV$sp(JobScheduler.scala:257)
	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler$$anonfun$run$1.apply(JobScheduler.scala:257)
	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler$$anonfun$run$1.apply(JobScheduler.scala:257)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)
	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Traceback (most recent call last):
  File "/home/rjha/stream-benchmarking-scratch/streaming_test.py", line 44, in <module>
    ssc.awaitTermination()
  File "/home/rjha/spark-2.3.0-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/streaming/context.py", line 206, in awaitTermination
  File "/home/rjha/spark-2.3.0-bin-hadoop2.7/python/lib/py4j-0.10.6-src.zip/py4j/java_gateway.py", line 1160, in __call__
  File "/home/rjha/spark-2.3.0-bin-hadoop2.7/python/lib/py4j-0.10.6-src.zip/py4j/protocol.py", line 320, in get_return_value
py4j.protocol.Py4JJavaError: An error occurred while calling o22.awaitTermination.
: org.apache.spark.SparkException: An exception was raised by Python:
Traceback (most recent call last):
  File "/home/rjha/spark-2.3.0-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/streaming/util.py", line 65, in call
    r = self.func(t, *rdds)
  File "/home/rjha/spark-2.3.0-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/streaming/dstream.py", line 159, in <lambda>
    func = lambda t, rdd: old_func(rdd)
  File "/home/rjha/stream-benchmarking-scratch/streaming_test.py", line 36, in <lambda>
    w_2_ts.foreachRDD(lambda rdd: send_rdd_to_db(rdd, out_port)) # storing
  File "/home/rjha/stream-benchmarking-scratch/streaming_test.py", line 18, in send_rdd_to_db
    conn = sock.create_connection(('localhost', out_port))
AttributeError: '_socketobject' object has no attribute 'create_connection'

	at org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)
	at org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)
	at org.apache.spark.streaming.api.python.PythonDStream$$anonfun$callForeachRDD$1.apply(PythonDStream.scala:179)
	at org.apache.spark.streaming.api.python.PythonDStream$$anonfun$callForeachRDD$1.apply(PythonDStream.scala:179)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1$$anonfun$apply$mcV$sp$1.apply$mcV$sp(ForEachDStream.scala:51)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1$$anonfun$apply$mcV$sp$1.apply(ForEachDStream.scala:51)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1$$anonfun$apply$mcV$sp$1.apply(ForEachDStream.scala:51)
	at org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:416)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1.apply$mcV$sp(ForEachDStream.scala:50)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1.apply(ForEachDStream.scala:50)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1.apply(ForEachDStream.scala:50)
	at scala.util.Try$.apply(Try.scala:192)
	at org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)
	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler$$anonfun$run$1.apply$mcV$sp(JobScheduler.scala:257)
	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler$$anonfun$run$1.apply(JobScheduler.scala:257)
	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler$$anonfun$run$1.apply(JobScheduler.scala:257)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)
	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

2018-04-26 00:34:31 INFO  JobScheduler:54 - Starting job streaming job 1524717271000 ms.1 from job set of time 1524717271000 ms
Exception in thread Thread-3 (most likely raised during interpreter shutdown):
Traceback (most recent call last):
  File "/usr/lib/python2.7/threading.py", line 801, in __bootstrap_inner
  File "/home/rjha/spark-2.3.0-bin-hadoop2.7/python/lib/py4j-0.10.6-src.zip/py4j/java_gateway.py", line 2223, in run
<type 'exceptions.AttributeError'>: 'NoneType' object has no attribute 'timeout'
2018-04-26 00:34:31 INFO  JobScheduler:54 - Finished job streaming job 1524717271000 ms.1 from job set of time 1524717271000 ms
2018-04-26 00:34:31 ERROR JobScheduler:91 - Error running job streaming job 1524717271000 ms.1
py4j.Py4JException: Error while sending a command.
	at py4j.CallbackClient.sendCommand(CallbackClient.java:357)
	at py4j.CallbackClient.sendCommand(CallbackClient.java:316)
	at py4j.reflection.PythonProxyHandler.invoke(PythonProxyHandler.java:106)
	at com.sun.proxy.$Proxy16.call(Unknown Source)
	at org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:92)
	at org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)
	at org.apache.spark.streaming.api.python.PythonDStream$$anonfun$callForeachRDD$1.apply(PythonDStream.scala:179)
	at org.apache.spark.streaming.api.python.PythonDStream$$anonfun$callForeachRDD$1.apply(PythonDStream.scala:179)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1$$anonfun$apply$mcV$sp$1.apply$mcV$sp(ForEachDStream.scala:51)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1$$anonfun$apply$mcV$sp$1.apply(ForEachDStream.scala:51)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1$$anonfun$apply$mcV$sp$1.apply(ForEachDStream.scala:51)
	at org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:416)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1.apply$mcV$sp(ForEachDStream.scala:50)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1.apply(ForEachDStream.scala:50)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1.apply(ForEachDStream.scala:50)
	at scala.util.Try$.apply(Try.scala:192)
	at org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)
	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler$$anonfun$run$1.apply$mcV$sp(JobScheduler.scala:257)
	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler$$anonfun$run$1.apply(JobScheduler.scala:257)
	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler$$anonfun$run$1.apply(JobScheduler.scala:257)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)
	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: py4j.Py4JNetworkException
	at py4j.CallbackConnection.sendCommand(CallbackConnection.java:138)
	at py4j.CallbackClient.sendCommand(CallbackClient.java:344)
	... 24 more
2018-04-26 00:34:31 INFO  StreamingContext:54 - Invoking stop(stopGracefully=false) from shutdown hook
2018-04-26 00:34:31 INFO  ReceiverTracker:54 - Sent stop signal to all 1 receivers
2018-04-26 00:34:32 ERROR JobScheduler:91 - Error generating jobs for time 1524717272000 ms
py4j.Py4JException: Cannot obtain a new communication channel
	at py4j.CallbackClient.sendCommand(CallbackClient.java:340)
	at py4j.CallbackClient.sendCommand(CallbackClient.java:316)
	at py4j.reflection.PythonProxyHandler.invoke(PythonProxyHandler.java:106)
	at com.sun.proxy.$Proxy16.call(Unknown Source)
	at org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:92)
	at org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)
	at org.apache.spark.streaming.api.python.PythonTransformedDStream.compute(PythonDStream.scala:246)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1$$anonfun$1$$anonfun$apply$7.apply(DStream.scala:342)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1$$anonfun$1$$anonfun$apply$7.apply(DStream.scala:342)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1$$anonfun$1.apply(DStream.scala:341)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1$$anonfun$1.apply(DStream.scala:341)
	at org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:416)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1.apply(DStream.scala:336)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1.apply(DStream.scala:334)
	at scala.Option.orElse(Option.scala:289)
	at org.apache.spark.streaming.dstream.DStream.getOrCompute(DStream.scala:331)
	at org.apache.spark.streaming.dstream.ForEachDStream.generateJob(ForEachDStream.scala:48)
	at org.apache.spark.streaming.DStreamGraph$$anonfun$1.apply(DStreamGraph.scala:122)
	at org.apache.spark.streaming.DStreamGraph$$anonfun$1.apply(DStreamGraph.scala:121)
	at scala.collection.TraversableLike$$anonfun$flatMap$1.apply(TraversableLike.scala:241)
	at scala.collection.TraversableLike$$anonfun$flatMap$1.apply(TraversableLike.scala:241)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)
	at scala.collection.TraversableLike$class.flatMap(TraversableLike.scala:241)
	at scala.collection.AbstractTraversable.flatMap(Traversable.scala:104)
	at org.apache.spark.streaming.DStreamGraph.generateJobs(DStreamGraph.scala:121)
	at org.apache.spark.streaming.scheduler.JobGenerator$$anonfun$3.apply(JobGenerator.scala:249)
	at org.apache.spark.streaming.scheduler.JobGenerator$$anonfun$3.apply(JobGenerator.scala:247)
	at scala.util.Try$.apply(Try.scala:192)
	at org.apache.spark.streaming.scheduler.JobGenerator.generateJobs(JobGenerator.scala:247)
	at org.apache.spark.streaming.scheduler.JobGenerator.org$apache$spark$streaming$scheduler$JobGenerator$$processEvent(JobGenerator.scala:183)
	at org.apache.spark.streaming.scheduler.JobGenerator$$anon$1.onReceive(JobGenerator.scala:89)
	at org.apache.spark.streaming.scheduler.JobGenerator$$anon$1.onReceive(JobGenerator.scala:88)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)
2018-04-26 00:34:32 ERROR PythonDStream$$anon$1:91 - Cannot connect to Python process. It's probably dead. Stopping StreamingContext.
py4j.Py4JException: Cannot obtain a new communication channel
	at py4j.CallbackClient.sendCommand(CallbackClient.java:340)
	at py4j.CallbackClient.sendCommand(CallbackClient.java:316)
	at py4j.reflection.PythonProxyHandler.invoke(PythonProxyHandler.java:106)
	at com.sun.proxy.$Proxy16.call(Unknown Source)
	at org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:92)
	at org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)
	at org.apache.spark.streaming.api.python.PythonTransformedDStream.compute(PythonDStream.scala:246)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1$$anonfun$1$$anonfun$apply$7.apply(DStream.scala:342)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1$$anonfun$1$$anonfun$apply$7.apply(DStream.scala:342)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1$$anonfun$1.apply(DStream.scala:341)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1$$anonfun$1.apply(DStream.scala:341)
	at org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:416)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1.apply(DStream.scala:336)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1.apply(DStream.scala:334)
	at scala.Option.orElse(Option.scala:289)
	at org.apache.spark.streaming.dstream.DStream.getOrCompute(DStream.scala:331)
	at org.apache.spark.streaming.dstream.ForEachDStream.generateJob(ForEachDStream.scala:48)
	at org.apache.spark.streaming.DStreamGraph$$anonfun$1.apply(DStreamGraph.scala:122)
	at org.apache.spark.streaming.DStreamGraph$$anonfun$1.apply(DStreamGraph.scala:121)
	at scala.collection.TraversableLike$$anonfun$flatMap$1.apply(TraversableLike.scala:241)
	at scala.collection.TraversableLike$$anonfun$flatMap$1.apply(TraversableLike.scala:241)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)
	at scala.collection.TraversableLike$class.flatMap(TraversableLike.scala:241)
	at scala.collection.AbstractTraversable.flatMap(Traversable.scala:104)
	at org.apache.spark.streaming.DStreamGraph.generateJobs(DStreamGraph.scala:121)
	at org.apache.spark.streaming.scheduler.JobGenerator$$anonfun$3.apply(JobGenerator.scala:249)
	at org.apache.spark.streaming.scheduler.JobGenerator$$anonfun$3.apply(JobGenerator.scala:247)
	at scala.util.Try$.apply(Try.scala:192)
	at org.apache.spark.streaming.scheduler.JobGenerator.generateJobs(JobGenerator.scala:247)
	at org.apache.spark.streaming.scheduler.JobGenerator.org$apache$spark$streaming$scheduler$JobGenerator$$processEvent(JobGenerator.scala:183)
	at org.apache.spark.streaming.scheduler.JobGenerator$$anon$1.onReceive(JobGenerator.scala:89)
	at org.apache.spark.streaming.scheduler.JobGenerator$$anon$1.onReceive(JobGenerator.scala:88)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)
2018-04-26 00:34:32 ERROR PythonDStream$$anon$1:91 - Cannot connect to Python process. It's probably dead. Stopping StreamingContext.
py4j.Py4JException: Cannot obtain a new communication channel
	at py4j.CallbackClient.sendCommand(CallbackClient.java:340)
	at py4j.CallbackClient.sendCommand(CallbackClient.java:316)
	at py4j.reflection.PythonProxyHandler.invoke(PythonProxyHandler.java:106)
	at com.sun.proxy.$Proxy16.call(Unknown Source)
	at org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:92)
	at org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)
	at org.apache.spark.streaming.api.python.PythonTransformedDStream.compute(PythonDStream.scala:246)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1$$anonfun$1$$anonfun$apply$7.apply(DStream.scala:342)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1$$anonfun$1$$anonfun$apply$7.apply(DStream.scala:342)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1$$anonfun$1.apply(DStream.scala:341)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1$$anonfun$1.apply(DStream.scala:341)
	at org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:416)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1.apply(DStream.scala:336)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1.apply(DStream.scala:334)
	at scala.Option.orElse(Option.scala:289)
	at org.apache.spark.streaming.dstream.DStream.getOrCompute(DStream.scala:331)
	at org.apache.spark.streaming.dstream.ForEachDStream.generateJob(ForEachDStream.scala:48)
	at org.apache.spark.streaming.DStreamGraph$$anonfun$1.apply(DStreamGraph.scala:122)
	at org.apache.spark.streaming.DStreamGraph$$anonfun$1.apply(DStreamGraph.scala:121)
	at scala.collection.TraversableLike$$anonfun$flatMap$1.apply(TraversableLike.scala:241)
	at scala.collection.TraversableLike$$anonfun$flatMap$1.apply(TraversableLike.scala:241)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)
	at scala.collection.TraversableLike$class.flatMap(TraversableLike.scala:241)
	at scala.collection.AbstractTraversable.flatMap(Traversable.scala:104)
	at org.apache.spark.streaming.DStreamGraph.generateJobs(DStreamGraph.scala:121)
	at org.apache.spark.streaming.scheduler.JobGenerator$$anonfun$3.apply(JobGenerator.scala:249)
	at org.apache.spark.streaming.scheduler.JobGenerator$$anonfun$3.apply(JobGenerator.scala:247)
	at scala.util.Try$.apply(Try.scala:192)
	at org.apache.spark.streaming.scheduler.JobGenerator.generateJobs(JobGenerator.scala:247)
	at org.apache.spark.streaming.scheduler.JobGenerator.org$apache$spark$streaming$scheduler$JobGenerator$$processEvent(JobGenerator.scala:183)
	at org.apache.spark.streaming.scheduler.JobGenerator$$anon$1.onReceive(JobGenerator.scala:89)
	at org.apache.spark.streaming.scheduler.JobGenerator$$anon$1.onReceive(JobGenerator.scala:88)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)
2018-04-26 00:34:32 INFO  MemoryStore:54 - Block broadcast_0 stored as values in memory (estimated size 68.7 KB, free 413.9 MB)
2018-04-26 00:34:32 INFO  MemoryStore:54 - Block broadcast_0_piece0 stored as bytes in memory (estimated size 23.9 KB, free 413.8 MB)
2018-04-26 00:34:32 INFO  BlockManagerInfo:54 - Added broadcast_0_piece0 in memory on 10.0.2.15:46027 (size: 23.9 KB, free: 413.9 MB)
2018-04-26 00:34:32 INFO  SparkContext:54 - Created broadcast 0 from broadcast at DAGScheduler.scala:1039
2018-04-26 00:34:32 INFO  DAGScheduler:54 - Submitting 1 missing tasks from ResultStage 0 (Receiver 0 ParallelCollectionRDD[0] at makeRDD at ReceiverTracker.scala:613) (first 15 tasks are for partitions Vector(0))
2018-04-26 00:34:32 INFO  TaskSchedulerImpl:54 - Adding task set 0.0 with 1 tasks
2018-04-26 00:34:32 INFO  TaskSetManager:54 - Starting task 0.0 in stage 0.0 (TID 0, localhost, executor driver, partition 0, PROCESS_LOCAL, 8442 bytes)
2018-04-26 00:34:32 INFO  Executor:54 - Running task 0.0 in stage 0.0 (TID 0)
2018-04-26 00:34:32 INFO  Executor:54 - Fetching file:/home/rjha/stream-benchmarking-scratch/streaming_test.py with timestamp 1524717268157
2018-04-26 00:34:32 INFO  Utils:54 - /home/rjha/stream-benchmarking-scratch/streaming_test.py has been previously copied to /tmp/spark-10173200-7f67-4568-b40d-c272989e46f1/userFiles-72d55d03-2b39-4bc5-b7a5-e2cb50b206be/streaming_test.py
2018-04-26 00:34:32 INFO  RecurringTimer:54 - Started timer for BlockGenerator at time 1524717273000
2018-04-26 00:34:32 INFO  BlockGenerator:54 - Started BlockGenerator
2018-04-26 00:34:32 INFO  ReceiverSupervisorImpl:54 - Stopping receiver with message: Registered unsuccessfully because Driver refused to start receiver 0: 
2018-04-26 00:34:32 WARN  ReceiverSupervisorImpl:66 - Skip stopping receiver because it has not yet stared
2018-04-26 00:34:32 INFO  BlockGenerator:54 - Stopping BlockGenerator
2018-04-26 00:34:32 INFO  BlockGenerator:54 - Started block pushing thread
2018-04-26 00:34:33 ERROR JobScheduler:91 - Error generating jobs for time 1524717273000 ms
py4j.Py4JException: Cannot obtain a new communication channel
	at py4j.CallbackClient.sendCommand(CallbackClient.java:340)
	at py4j.CallbackClient.sendCommand(CallbackClient.java:316)
	at py4j.reflection.PythonProxyHandler.invoke(PythonProxyHandler.java:106)
	at com.sun.proxy.$Proxy16.call(Unknown Source)
	at org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:92)
	at org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)
	at org.apache.spark.streaming.api.python.PythonTransformedDStream.compute(PythonDStream.scala:246)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1$$anonfun$1$$anonfun$apply$7.apply(DStream.scala:342)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1$$anonfun$1$$anonfun$apply$7.apply(DStream.scala:342)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1$$anonfun$1.apply(DStream.scala:341)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1$$anonfun$1.apply(DStream.scala:341)
	at org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:416)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1.apply(DStream.scala:336)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1.apply(DStream.scala:334)
	at scala.Option.orElse(Option.scala:289)
	at org.apache.spark.streaming.dstream.DStream.getOrCompute(DStream.scala:331)
	at org.apache.spark.streaming.dstream.ForEachDStream.generateJob(ForEachDStream.scala:48)
	at org.apache.spark.streaming.DStreamGraph$$anonfun$1.apply(DStreamGraph.scala:122)
	at org.apache.spark.streaming.DStreamGraph$$anonfun$1.apply(DStreamGraph.scala:121)
	at scala.collection.TraversableLike$$anonfun$flatMap$1.apply(TraversableLike.scala:241)
	at scala.collection.TraversableLike$$anonfun$flatMap$1.apply(TraversableLike.scala:241)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)
	at scala.collection.TraversableLike$class.flatMap(TraversableLike.scala:241)
	at scala.collection.AbstractTraversable.flatMap(Traversable.scala:104)
	at org.apache.spark.streaming.DStreamGraph.generateJobs(DStreamGraph.scala:121)
	at org.apache.spark.streaming.scheduler.JobGenerator$$anonfun$3.apply(JobGenerator.scala:249)
	at org.apache.spark.streaming.scheduler.JobGenerator$$anonfun$3.apply(JobGenerator.scala:247)
	at scala.util.Try$.apply(Try.scala:192)
	at org.apache.spark.streaming.scheduler.JobGenerator.generateJobs(JobGenerator.scala:247)
	at org.apache.spark.streaming.scheduler.JobGenerator.org$apache$spark$streaming$scheduler$JobGenerator$$processEvent(JobGenerator.scala:183)
	at org.apache.spark.streaming.scheduler.JobGenerator$$anon$1.onReceive(JobGenerator.scala:89)
	at org.apache.spark.streaming.scheduler.JobGenerator$$anon$1.onReceive(JobGenerator.scala:88)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)
2018-04-26 00:34:33 ERROR PythonDStream$$anon$1:91 - Cannot connect to Python process. It's probably dead. Stopping StreamingContext.
py4j.Py4JException: Cannot obtain a new communication channel
	at py4j.CallbackClient.sendCommand(CallbackClient.java:340)
	at py4j.CallbackClient.sendCommand(CallbackClient.java:316)
	at py4j.reflection.PythonProxyHandler.invoke(PythonProxyHandler.java:106)
	at com.sun.proxy.$Proxy16.call(Unknown Source)
	at org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:92)
	at org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)
	at org.apache.spark.streaming.api.python.PythonTransformedDStream.compute(PythonDStream.scala:246)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1$$anonfun$1$$anonfun$apply$7.apply(DStream.scala:342)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1$$anonfun$1$$anonfun$apply$7.apply(DStream.scala:342)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1$$anonfun$1.apply(DStream.scala:341)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1$$anonfun$1.apply(DStream.scala:341)
	at org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:416)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1.apply(DStream.scala:336)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1.apply(DStream.scala:334)
	at scala.Option.orElse(Option.scala:289)
	at org.apache.spark.streaming.dstream.DStream.getOrCompute(DStream.scala:331)
	at org.apache.spark.streaming.dstream.ForEachDStream.generateJob(ForEachDStream.scala:48)
	at org.apache.spark.streaming.DStreamGraph$$anonfun$1.apply(DStreamGraph.scala:122)
	at org.apache.spark.streaming.DStreamGraph$$anonfun$1.apply(DStreamGraph.scala:121)
	at scala.collection.TraversableLike$$anonfun$flatMap$1.apply(TraversableLike.scala:241)
	at scala.collection.TraversableLike$$anonfun$flatMap$1.apply(TraversableLike.scala:241)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)
	at scala.collection.TraversableLike$class.flatMap(TraversableLike.scala:241)
	at scala.collection.AbstractTraversable.flatMap(Traversable.scala:104)
	at org.apache.spark.streaming.DStreamGraph.generateJobs(DStreamGraph.scala:121)
	at org.apache.spark.streaming.scheduler.JobGenerator$$anonfun$3.apply(JobGenerator.scala:249)
	at org.apache.spark.streaming.scheduler.JobGenerator$$anonfun$3.apply(JobGenerator.scala:247)
	at scala.util.Try$.apply(Try.scala:192)
	at org.apache.spark.streaming.scheduler.JobGenerator.generateJobs(JobGenerator.scala:247)
	at org.apache.spark.streaming.scheduler.JobGenerator.org$apache$spark$streaming$scheduler$JobGenerator$$processEvent(JobGenerator.scala:183)
	at org.apache.spark.streaming.scheduler.JobGenerator$$anon$1.onReceive(JobGenerator.scala:89)
	at org.apache.spark.streaming.scheduler.JobGenerator$$anon$1.onReceive(JobGenerator.scala:88)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)
2018-04-26 00:34:33 ERROR PythonDStream$$anon$1:91 - Cannot connect to Python process. It's probably dead. Stopping StreamingContext.
py4j.Py4JException: Cannot obtain a new communication channel
	at py4j.CallbackClient.sendCommand(CallbackClient.java:340)
	at py4j.CallbackClient.sendCommand(CallbackClient.java:316)
	at py4j.reflection.PythonProxyHandler.invoke(PythonProxyHandler.java:106)
	at com.sun.proxy.$Proxy16.call(Unknown Source)
	at org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:92)
	at org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)
	at org.apache.spark.streaming.api.python.PythonTransformedDStream.compute(PythonDStream.scala:246)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1$$anonfun$1$$anonfun$apply$7.apply(DStream.scala:342)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1$$anonfun$1$$anonfun$apply$7.apply(DStream.scala:342)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1$$anonfun$1.apply(DStream.scala:341)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1$$anonfun$1.apply(DStream.scala:341)
	at org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:416)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1.apply(DStream.scala:336)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1.apply(DStream.scala:334)
	at scala.Option.orElse(Option.scala:289)
	at org.apache.spark.streaming.dstream.DStream.getOrCompute(DStream.scala:331)
	at org.apache.spark.streaming.dstream.ForEachDStream.generateJob(ForEachDStream.scala:48)
	at org.apache.spark.streaming.DStreamGraph$$anonfun$1.apply(DStreamGraph.scala:122)
	at org.apache.spark.streaming.DStreamGraph$$anonfun$1.apply(DStreamGraph.scala:121)
	at scala.collection.TraversableLike$$anonfun$flatMap$1.apply(TraversableLike.scala:241)
	at scala.collection.TraversableLike$$anonfun$flatMap$1.apply(TraversableLike.scala:241)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)
	at scala.collection.TraversableLike$class.flatMap(TraversableLike.scala:241)
	at scala.collection.AbstractTraversable.flatMap(Traversable.scala:104)
	at org.apache.spark.streaming.DStreamGraph.generateJobs(DStreamGraph.scala:121)
	at org.apache.spark.streaming.scheduler.JobGenerator$$anonfun$3.apply(JobGenerator.scala:249)
	at org.apache.spark.streaming.scheduler.JobGenerator$$anonfun$3.apply(JobGenerator.scala:247)
	at scala.util.Try$.apply(Try.scala:192)
	at org.apache.spark.streaming.scheduler.JobGenerator.generateJobs(JobGenerator.scala:247)
	at org.apache.spark.streaming.scheduler.JobGenerator.org$apache$spark$streaming$scheduler$JobGenerator$$processEvent(JobGenerator.scala:183)
	at org.apache.spark.streaming.scheduler.JobGenerator$$anon$1.onReceive(JobGenerator.scala:89)
	at org.apache.spark.streaming.scheduler.JobGenerator$$anon$1.onReceive(JobGenerator.scala:88)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)
2018-04-26 00:34:33 INFO  RecurringTimer:54 - Stopped timer for BlockGenerator after time 1524717273200
2018-04-26 00:34:33 INFO  BlockGenerator:54 - Pushing out the last 0 blocks
2018-04-26 00:34:33 INFO  BlockGenerator:54 - Stopped block pushing thread
2018-04-26 00:34:33 INFO  BlockGenerator:54 - Waiting for block pushing thread to terminate
2018-04-26 00:34:33 INFO  BlockGenerator:54 - Stopped BlockGenerator
2018-04-26 00:34:33 INFO  ReceiverSupervisorImpl:54 - Waiting for receiver to be stopped
2018-04-26 00:34:33 INFO  ReceiverSupervisorImpl:54 - Stopped receiver without error
2018-04-26 00:34:33 INFO  Executor:54 - Finished task 0.0 in stage 0.0 (TID 0). 794 bytes result sent to driver
2018-04-26 00:34:33 INFO  TaskSetManager:54 - Finished task 0.0 in stage 0.0 (TID 0) in 953 ms on localhost (executor driver) (1/1)
2018-04-26 00:34:33 INFO  TaskSchedulerImpl:54 - Removed TaskSet 0.0, whose tasks have all completed, from pool 
2018-04-26 00:34:33 INFO  DAGScheduler:54 - ResultStage 0 (start at NativeMethodAccessorImpl.java:0) finished in 1.975 s
2018-04-26 00:34:33 WARN  ReceiverTracker:66 - Not all of the receivers have deregistered, ArrayBuffer(0)
2018-04-26 00:34:33 INFO  ReceiverTracker:54 - ReceiverTracker stopped
2018-04-26 00:34:33 INFO  JobGenerator:54 - Stopping JobGenerator immediately
2018-04-26 00:34:33 INFO  RecurringTimer:54 - Stopped timer for JobGenerator after time 1524717273000
2018-04-26 00:34:33 INFO  JobGenerator:54 - Stopped JobGenerator
2018-04-26 00:34:33 INFO  JobScheduler:54 - Stopped JobScheduler
2018-04-26 00:34:33 INFO  ContextHandler:910 - Stopped o.s.j.s.ServletContextHandler@20e0d880{/streaming,null,UNAVAILABLE,@Spark}
2018-04-26 00:34:33 INFO  ContextHandler:910 - Stopped o.s.j.s.ServletContextHandler@4a528609{/streaming/batch,null,UNAVAILABLE,@Spark}
2018-04-26 00:34:33 INFO  ContextHandler:910 - Stopped o.s.j.s.ServletContextHandler@1895b2a5{/static/streaming,null,UNAVAILABLE,@Spark}
2018-04-26 00:34:33 INFO  StreamingContext:54 - StreamingContext stopped successfully
2018-04-26 00:34:33 WARN  StreamingContext:66 - StreamingContext has already been stopped
2018-04-26 00:34:33 WARN  StreamingContext:66 - StreamingContext has already been stopped
2018-04-26 00:34:33 WARN  StreamingContext:66 - StreamingContext has already been stopped
2018-04-26 00:34:33 WARN  StreamingContext:66 - StreamingContext has already been stopped
2018-04-26 00:34:33 INFO  SparkContext:54 - Invoking stop() from shutdown hook
2018-04-26 00:34:33 INFO  AbstractConnector:318 - Stopped Spark@1a4f0cc6{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2018-04-26 00:34:33 INFO  SparkUI:54 - Stopped Spark web UI at http://10.0.2.15:4040
2018-04-26 00:34:33 INFO  MapOutputTrackerMasterEndpoint:54 - MapOutputTrackerMasterEndpoint stopped!
2018-04-26 00:34:33 INFO  MemoryStore:54 - MemoryStore cleared
2018-04-26 00:34:33 INFO  BlockManager:54 - BlockManager stopped
2018-04-26 00:34:33 INFO  BlockManagerMaster:54 - BlockManagerMaster stopped
2018-04-26 00:34:33 INFO  OutputCommitCoordinator$OutputCommitCoordinatorEndpoint:54 - OutputCommitCoordinator stopped!
2018-04-26 00:34:33 INFO  SparkContext:54 - Successfully stopped SparkContext
2018-04-26 00:34:33 INFO  ShutdownHookManager:54 - Shutdown hook called
2018-04-26 00:34:33 INFO  ShutdownHookManager:54 - Deleting directory /tmp/spark-3569e25a-46a7-43b6-be5d-6be778da4938
2018-04-26 00:34:33 INFO  ShutdownHookManager:54 - Deleting directory /tmp/spark-10173200-7f67-4568-b40d-c272989e46f1/pyspark-30a178a6-7187-4253-95a0-f3730d219fb6
2018-04-26 00:34:33 INFO  ShutdownHookManager:54 - Deleting directory /tmp/spark-10173200-7f67-4568-b40d-c272989e46f1
