1: testing the out-of-box streaming program
   listening: ./bin/spark-submit --master local[2] examples/src/main/python/streaming/network_wordcount.py localhost 9999
   writing data: nc -l -k 9999

2: testing a setup to test writing to ports
   listening: nc localhost 9999
   writing data: nc -l -k 9999

3: testing the writing to ports (old non-server model)
   listening: nc -l -k 9999
   writing: python write_to_port.py 9999

4: writing from python to spark streaming
   listening: ~/bin/spark-submit --master local[2] network_wordcount.py localhost 9998
   writing: python write_to_port.py 9998

5: writing from python to spark streaming with reasonable output
   listening: ~/bin/spark-submit --master local[2] network_wordcount.py localhost 9998 > output.txt
   writing: python write_to_port.py 9998
   NOTE: also a step of copying the output from spark streaming -- it's currently just output to the shell

6. mostly finalized framework
   listening: ~/spark-2.3.0-bin-hadoop2.7/bin/spark-submit --master local[2] streaming_test.py localhost 9998 4444 > output.txt
   writing: python write_to_port.py 9998
   reading: python read_from_spark.py 4444 int2.csv
   interval between ts2 and ts3: python resuls_proc.py int1.csv int2.csv
   looking at results: cat results.txt
   looking at results: sqlite3 results.db

7. post-organization
   listening: ~/spark-2.3.0-bin-hadoop2.7/bin/spark-submit --master local[2] usage.py localhost 9998 4444 baseline > results/output.txt
   writing: python write_to_port.py 9998
   reading: python read_from_spark.py 4444 results/int2.csv
   interval between ts2 and ts3: python results_proc.py results/int1.csv results/int2.csv
   looking at results: cat results/results.txt
   looking at results: sqlite3 results/results.db

--------------------------------------------------------------------------------

Instructions for running a benchmark:
  1. open the results database in sqlite3 and run "drop table results;"
  2. run read_from_spark.py (see above)
  3. run write_to_port.py (see above)
  4. run streaming_test.py (see above)
  5. after 2000 (or some other number) data points end each of the three
     terminals with ctrl+c, in reverse order to their starting
  6. find the average interval between the first two timestamps by running in
     sqlite: select avg(ts2 - ts1) from results;
  7. output the datapoints and the values of ts2 to the console by running in
     sqlite:
     a. .mode csv
     b. .output results/int1.csv
     c. select ind, ts2 from results;
     d. .output stdout
  8. find the average interval between the second two timestamps by running
     results_proc.py
